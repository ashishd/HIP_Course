{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "993686df-b188-4872-a319-b9829fca5383",
   "metadata": {},
   "source": [
    "# Using HIP on Setonix\n",
    "\n",
    "## Access to Setonix\n",
    "\n",
    "Firstly you need a username and password to access Setonix. Your **username** and **password** will be given to you prior to the beginning of this workshop. If you are using your regular Pawsey account then you can reset your password [here](https://support.pawsey.org.au/password-reset/).\n",
    "\n",
    "Access to Setonix is via Secure SHell (SSH). On Linux, Mac OS, and Windows 10 and higher, an SSH client is available from the command line or terminal application. Otherwise you need to use a client program like [Putty](https://www.putty.org/) or [MobaXterm](https://mobaxterm.mobatek.net/download-home-edition.html).\n",
    "\n",
    "### Access with SSH on the command line\n",
    "\n",
    "On the command line use the command **ssh** to access Setonix.\n",
    "\n",
    "```bash\n",
    "ssh -Y <username>@setonix.pawsey.org.au\n",
    "```\n",
    "\n",
    "#### Passwordless login with SSH\n",
    "\n",
    "In order to avoid specifying a username and password on each login you can generate a key and password combination on your computer using the following on the command line.\n",
    "\n",
    "```bash\n",
    "ssh-keygen -t rsa\n",
    "```\n",
    "\n",
    "Then copy the public key (the file that ends in \\*.pub) to your account on setonix and append it to the authorized keys in .ssh. On your machine run this command\n",
    "\n",
    "```bash\n",
    "scp -r <filename>.pub <username>@setonix.pawsey.org.au\n",
    "```\n",
    "\n",
    "Then login to Setonix and run this command\n",
    "\n",
    "```bash\n",
    "mkdir -p ${HOME}/.ssh\n",
    "cat <filename>.pub >> ${HOME}/.ssh/authorized_keys\n",
    "chmod -R 0400 ${HOME}/.ssh\n",
    "```\n",
    "\n",
    "Finally, if you are using MacOS or Linux you can add this line to ${HOME}/.ssh/config on your computer\n",
    "\n",
    "```text\n",
    "Host setonix\n",
    "    Hostname setonix.pawsey.org.au\n",
    "    IdentityFile <private_key_file>\n",
    "    User <username>\n",
    "    ForwardX11 yes\n",
    "    ForwardAgent yes\n",
    "    ServerAliveInterval 300\n",
    "    ServerAliveCountMax 2\n",
    "    TCPKeepAlive no\n",
    "```\n",
    "\n",
    "Then you can run \n",
    "\n",
    "```bash\n",
    "ssh setonix\n",
    "```\n",
    "\n",
    "without a password.\n",
    "\n",
    "### Access from Windows with the MobaXterm client\n",
    "\n",
    "If you have a OS that is older than Windows 10 and need a client in a hurry, just download **MobaXterm Home (Portable Edition)** from [this location](https://mobaxterm.mobatek.net/download-home-edition.html). Extract the Zip file and run the application. You might need to accept a firewall notification. \n",
    "\n",
    "Now go to **Settings -> SSH** and uncheck **\"Enable graphical SSH-browser\"** in the SSH-browser settings pane. Also enable **\"SSH keepalive\"** to keep SSH connections active.\n",
    "\n",
    "<figure style=\"margin-bottom 3em; margin-top: 2em; margin-left:auto; margin-right:auto; width:100%\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/MobaXTerm_Settings.svg\"> <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Figure: MobaXTerm settings.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Then close the settings and start a local terminal.\n",
    "\n",
    "\n",
    "\n",
    "## Hardware environment\n",
    "\n",
    "<figure style=\"margin: 1em; margin-left:auto; margin-right:auto; width:100%;\">\n",
    "    <img src=\"../images/MI250x.png\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">AMD Instinct<span>&trade;</span> MI250X compute architecture. Image credit: <a href=\"https://hc34.hotchips.org/\")>AMD Instinct<span>&trade;</span> MI200 Series Accelerator and Node Architectures | Hot Chips 34</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "### Setonix CPU specifications\n",
    "\n",
    "| Computer | CPU | Nominal clock frequency (GHz) | Cores | Hardware threads | L1 Cache (KB) | L2 Cache (KB) | L3 cache (MB) | FP SIMD width (bits) | Tflops (FP32 calculated) |\n",
    "|:----:|:----:|-----:| -----: | -----: | :----: | :----: | :----: | :----: | :----: |\n",
    "| Setonix |AMD EPYC 7A53 | 2.0 | 64 | 128 | 64x32 | 64x512 | 8x32 | 256 | ~2 |\n",
    "\n",
    "### Setonix GPU specifications\n",
    "\n",
    "| Card | Boost clock (GHz)| Compute Units | FP32 Processing Elements | FP64 Processing Elements (equivalent compute capacity) | L1 Cache (KB) | L2 Cache (KB) | device memory (GB) | Peak Tflops (FP32)| Peak Tflops (FP64)|\n",
    "|:----:|:-----| :----- | :----- | :---- | :---- | :---- | :---- | :---- | :---- |\n",
    "| AMD Radeon Instinct MI250x |1.7 | 220 | 14080 | 14080 | 220x16 | 16000 | 128 | 47.9 | 47.9 |\n",
    "\n",
    "\n",
    "## Job queues\n",
    "\n",
    "On Setonix the following queues are available for general use:\n",
    "\n",
    "|Queue| Max time limit| Processing elements (CPU) | Socket| Cores| processing elements per CPU core | Available memory (GB) | Number of HIP devices | Memory per HIP device (GB) |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| work | 24 hours | 256 | 2 | 64 | 2 | 230 | 0 | 0 |\n",
    "| long | 96 hours | 256 | 2 | 64 | 2 | 230 | 0 | 0 |\n",
    "| debug | 1 hour | 256 | 2 | 64 | 2 | 230 | 0 | 0 |\n",
    "| highmem | 24 hours | 256 | 2 | 64 | 2 | 980 | 0 | 0 |\n",
    "| copy | 24 hours | 32 | 1 | 64 | 2 | 118 | 0 | 0 |\n",
    "| gpu | 24 hours | 128 | 1 | 64 | 2 | 128 | 8 (4x2) | 64 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fca2784-aecf-4496-8c93-d546565bfde3",
   "metadata": {},
   "source": [
    "## Interactive jobs on GPU nodes\n",
    "\n",
    "```bash\n",
    "salloc --account ${PAWSEY_PROJECT} --ntasks 1 --mem 4GB --cpus-per-task 1 --time 1:00:00 --gpus-per-task 2 --partition gpu\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484c6de0-d2f6-4e65-b207-bf9f16686a04",
   "metadata": {},
   "source": [
    "## Building software for Setonix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00304c87-b992-4918-8d0b-83532c5b8f69",
   "metadata": {},
   "source": [
    "### Software modules\n",
    "\n",
    "There are three main programming environments available on Setonix. Each provides C/C++ and Fortran compilers that build software with knowledge of of the MPI libraries available on Setonix. The **PrgEnv-GNU** programming environment uses the GNU compilers, **PrgEnv-aocc** uses the AMD aocc optimising compiler to try and get the best performance from the AMD CPU's on Setonix, and the **PrgEnv-cray** compilers use the compilers from Cray. Use these commands to find which module to load.\n",
    "\n",
    "| Programming environment | command to use |\n",
    "| :--- | :--- |\n",
    "| AMD | ```module avail PrgEnv-aocc``` |\n",
    "| Cray | ```module avail PrgEnv-cray``` |\n",
    "| GNU | ```module avail PrgEnv-gnu``` |\n",
    "\n",
    "When compiling HIP sources you have the choice of either the the ROCM **hipcc** compiler wrapper or the Cray compiler wrapper **CC** from **PrgEnv-cray**. If you use the Cray compiler wrapper you need to swap to the module **PrgEnv-cray** as the GNU programming environment (**PrgEnv-gnu**) is loaded by default. \n",
    "\n",
    "```bash\n",
    "module swap PrgEnv-gnu PrgEnv-cray\n",
    "```\n",
    "\n",
    "Then the following compiler wrappers are available for use to compile source files:\n",
    "\n",
    "| Command | Explanation |\n",
    "| :--- | :--- |\n",
    "| cc | C compiler |\n",
    "| CC | C++ compiler |\n",
    "| ftn | FORTRAN compiler |\n",
    "\n",
    "In order to use the GPU-aware MPI library you also need to load the **craype-accel-amd-gfx90a** module, which works in all three programming environments. To see which version to load run this command.\n",
    "\n",
    "```bash\n",
    "module avail craype-accel-amd-gfx90a\n",
    "```\n",
    "\n",
    "Load the module **craype-accel-amd-gfx90a** then set the environment variable\n",
    "\n",
    "```bash\n",
    "export MPICH_GPU_SUPPORT_ENABLED=1\n",
    "```\n",
    "\n",
    "Finally, in order to have ROCM software (such as hipcc and rocgdb) and libraries available you need to have the **rocm** module loaded. To see which one to load run this command:\n",
    "\n",
    "```bash\n",
    "module avail rocm\n",
    "```\n",
    "\n",
    "The **rocm** module is independent of the programming environment module loaded. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275ea2ac-377e-4414-9d08-f52384bc08a3",
   "metadata": {},
   "source": [
    "### Compiling software with HIP and MPI support\n",
    "\n",
    "According to this [documentation](https://docs.amd.com/bundle/HIP-Programming-Guide-v5.0/page/Transitioning_from_CUDA_to_HIP.html) the AMD compiler wrapper **hipcc** can be use for compiling HIP source files and is the suggested linker for program objects. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb2395a-9b5f-43d5-8af0-848747e9d978",
   "metadata": {},
   "source": [
    "#### Compiling and linking with the **hipcc** compiler wrapper\n",
    "\n",
    "You can use these compiler flags to bring in the MPI headers and make sure **hipcc** compiles kernels for the MI250X GPU's on Setonix.\n",
    "\n",
    "| Function | flags |\n",
    "| :--- | :--- |\n",
    "| Compile | ```-I${MPICH_DIR}/include --offload-arch=gfx90a ``` |\n",
    "| Link | ```-L${MPICH_DIR}/lib -lmpi ${PE_MPICH_GTL_DIR_amd_gfx90a} ${PE_MPICH_GTL_LIBS_amd_gfx90a}``` |\n",
    "| Debug (compile and link) | ```-ggdb``` |\n",
    "| OpenMP (compile and link)| ```-fopenmp``` |\n",
    "\n",
    "If you want **hipcc** to behave like Cray **CC**, make sure the **PrgEnv-cray** and **craype-accel-amd-gfx90a** modules are also loaded. Then you can add the output of this command,\n",
    "\n",
    "```bash\n",
    "$(CC --cray-print-opts=cflags)\n",
    "```\n",
    "\n",
    "to the hipcc compile flags, and the output of this command,\n",
    "\n",
    "```bash\n",
    "$(CC --cray-print-opts=libs)\n",
    "```\n",
    "\n",
    "to the hipcc linker flags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fc42f2-3b77-4843-89f5-9d6da4a2caff",
   "metadata": {},
   "source": [
    "#### Compiling and linking with the Cray **CC** compiler wrapper \n",
    "\n",
    "If you are using the Cray compiler wrapper **CC** you can add these flags to compile and link HIP code for the MI250X GPU's on Setonix. You need to have the **rocm** module loaded.\n",
    "\n",
    "| Function | flags |\n",
    "| :--- | :--- |\n",
    "| Compile | ```-D__HIP_ROCclr__ -D__HIP_ARCH_GFX90A__=1 --offload-arch=gfx90a -x hip``` |\n",
    "| Link |  |\n",
    "| Debug (compile and link) | ```-g``` |\n",
    "| OpenMP (compile and link)| ```-fopenmp``` |\n",
    "\n",
    "#### Mixing hipcc and Cray compilation\n",
    "\n",
    "From this [documentation](https://docs.amd.com/bundle/HIP-Programming-Guide-v5.0/page/Transitioning_from_CUDA_to_HIP.html) it is important to note that all code links back to the same C++ standard libraries. The command ```hipconfig --cxx``` generates extra compile flags that might be useful for including in the build process with the Cray wrapper. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d77ef4f-17f2-463a-97ed-4d215df7ed77",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise: compile and run your first MPI-enabled HIP application\n",
    "\n",
    "In the files [hello_devices_mpi.cpp](hello_devices_mpi.cpp) and [hello_devices_mpi_onefile.cpp](hello_devices_mpi_onefile.cpp) are files to implement an MPI-enabled HIP application that reports on devices and fills a vector. The difference between the two is that for [hello_devices_mpi.cpp](hello_devices_mpi.cpp) has the kernel located in a separate file [kernels.hip.cpp](kernels.hip.cpp). Your task is to compile these files into two executables, **hello_devices_mpi.exe** and **hello_devices_mpi_onefile.exe**.\n",
    "\n",
    "### Compilation steps\n",
    "\n",
    "1. Log into **setonix.pawsey.org.au**.\n",
    "1. Use **cd** to change directory to your temporary file location in /scratch.\n",
    "1. Clone the course material from Github if don't already have it.\n",
    "    <br></br>\n",
    "    ```git clone git@github.com:pelagos-consulting/HIP_Course.git```\n",
    "    <br></br>\n",
    "1. Change directory to **course_material/L2_Using_HIP_On_Setonix**.\n",
    "1. Get an interactive job on the GPU queue of Setonix with this command:\n",
    "    <br></br>\n",
    "    ```salloc --account ${PAWSEY_PROJECT} --ntasks 1 --mem 4GB --cpus-per-task 1 --time 1:00:00 --gpus-per-task 1 --partition gpu```\n",
    "    <br></br>\n",
    "1. Load the **rocm** module\n",
    "    <br></br>\n",
    "    ```module load rocm```\n",
    "    <br></br>    \n",
    "1. Swap out the **PrgEnv-gnu** module for the **PrgEnv-cray** module\n",
    "    <br></br>\n",
    "    ```module swap PrgEnv-gnu PrgEnv-cray```\n",
    "    <br></br>\n",
    "1. Load the **craype-accel-amd-gfx90a** module\n",
    "    <br></br>\n",
    "    ```module load craype-accel-amd-gfx90a```\n",
    "    \n",
    "#### Compile the kernel and main program in separate files\n",
    "1. Compile the kernel file [kernels.hip.cpp](kernels.hip.cpp)\n",
    "    <br></br>\n",
    "    ```hipcc -c kernels.hip.cpp --offload-arch=gfx90a -o kernels.o```\n",
    "    <br></br>\n",
    "1. Use **CC** to compile the file [hello_devices_mpi.cpp](hello_devices_mpi.cpp). Make sure to include the location of the **hip_helper.hpp** library, located in ../include.\n",
    "    <br></br>\n",
    "    ```CC -c -D__HIP_ROCclr__ -D__HIP_ARCH_GFX90A__=1 --offload-arch=gfx90a -I../include -x hip hello_devices_mpi.cpp -o hello_devices_mpi.o```\n",
    "    <br></br>\n",
    "1. Use **hipcc** to link the object files together in a way that is aware of the MPI library.\n",
    "    <br></br>\n",
    "    ```hipcc kernels.o hello_devices_mpi.o -o hello_devices_mpi.exe -L${MPICH_DIR}/lib -lmpi ${PE_MPICH_GTL_DIR_amd_gfx90a} ${PE_MPICH_GTL_LIBS_amd_gfx90a}```\n",
    "<br></br>\n",
    "\n",
    "#### Compile the combined file in one go using **hipcc**\n",
    "\n",
    "```hipcc -I${MPICH_DIR}/include -I../include --offload-arch=gfx90a hello_devices_mpi_onefile.cpp -o hello_devices_mpi_onefile_hipcc.exe -L${MPICH_DIR}/lib -lmpi ${PE_MPICH_GTL_DIR_amd_gfx90a} ${PE_MPICH_GTL_LIBS_amd_gfx90a}```\n",
    "<br></br>\n",
    "\n",
    "#### Compile the combined file in one go using **CC**\n",
    "\n",
    "```CC -D__HIP_ROCclr__ -D__HIP_ARCH_GFX90A__=1 --offload-arch=gfx90a -I../include -x hip hello_devices_mpi_onefile.cpp -o hello_devices_mpi_onefile_CC.exe```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff480a8-30d6-480a-9f35-068ab0551d43",
   "metadata": {},
   "source": [
    "### Run the compiled code\n",
    "\n",
    "Using **srun** we can run the executable files. If we don't use **srun** then it will pick up all the GPU's on a node. \n",
    "\n",
    "1. ```srun ./hello_devices_mpi.exe```\n",
    "1. ```srun ./hello_devices_mpi_onefile_hipcc.exe```\n",
    "1. ```srun ./hello_devices_mpi_onefile_CC.exe```\n",
    "\n",
    "### Bonus task: Try running these programs with and without **srun** to see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ea2932-843d-443c-8214-0006bc5feef2",
   "metadata": {},
   "source": [
    "### The answer\n",
    "\n",
    "If you get stuck, the example [Makefile](Makefile) contains the above compilation steps. Assuming you loaded the right modules defined above, the make command is run as follows:\n",
    "\n",
    "```make clean; make```\n",
    "\n",
    "The script **run_compile.sh** contains the necessary commands to load the appropriate modules and run the **make** command.\n",
    "\n",
    "```bash\n",
    "   chmod 700 run_compile.sh\n",
    "   ./run_compile.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f47a4e-d43e-4a7f-9c28-c4720064a08d",
   "metadata": {},
   "source": [
    "## Batch jobs on GPU nodes\n",
    "\n",
    "Pawsey has extensive documentation available for running jobs, at this [site](https://support.pawsey.org.au/documentation/display/US/Running+Jobs+in+Setonix). The suggested job script below will allocate an MPI task for each compute device on a node of Setonix. Then it will allocate 8 OpenMP threads to each MPI task. We can use the helper program [hello_jobstep]( https://code.ornl.gov/olcf/hello_jobstep) Written by Thomas Papatheodore from ORNL to show the node hostname\n",
    "\n",
    "```bash\n",
    "#!/bin/bash -l\n",
    "\n",
    "#SBATCH --account=pawsey0007       # your account\n",
    "#SBATCH --partition=gpu            # Using the gpu partition\n",
    "#SBATCH --ntasks=6                 # Total number of tasks\n",
    "#SBATCH --ntasks-per-node=8        # Set this for 1 mpi task per compute device\n",
    "#SBATCH --cpus-per-task=16          # How many OpenMP threads per MPI task \n",
    "#SBATCH --gpus-per-task=1          # How many HIP compute devices to allocate to a task\n",
    "#SBATCH --mem=4000M                #Indicate the amount of memory per node when asking for share resources\n",
    "#SBATCH --time=01:00:00\n",
    "\n",
    "module swap PrgEnv-gnu PrgEnv-cray\n",
    "module load craype-accel-amd-gfx90a\n",
    "module load rocm\n",
    "\n",
    "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK   #To define the number of OpenMP threads available per MPI task, in this case it will be 8\n",
    "export OMP_PLACES=cores     #To bind threads to cores\n",
    "export OMP_PROC_BIND=close  #To bind (fix) threads (allocating them as close as possible). This option works together with the \"places\" indicated above, then: allocates threads in closest cores.\n",
    " \n",
    "# Temporal workaround for avoiding Slingshot issues on shared nodes:\n",
    "export FI_CXI_DEFAULT_VNI=$(od -vAn -N4 -tu < /dev/urandom)\n",
    "\n",
    "# Bind options to srun plane=4 is the number of NUMA domains per processor (4)\n",
    "export BIND_OPTIONS=\"--cpu-bind=ldoms -m plane=4 --gpu-bind=closest\"\n",
    "\n",
    "# Run a job with task placement and $BIND_OPTIONS\n",
    "srun -N $SLURM_JOB_NUM_NODES -n $SLURM_NTASKS -c $OMP_NUM_THREADS $BIND_OPTIONS  ./hello_jobstep.exe\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f930c64e-9224-48ae-9c57-c8053782e820",
   "metadata": {},
   "source": [
    "<address>\n",
    "Written by Dr. Toby Potter of <a href=\"https://www.pelagos-consulting.com\">Pelagos Consulting and Education</a> for the Pawsey Supercomputing Centre\n",
    "</address>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
