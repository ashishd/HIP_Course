{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da6d105-2df5-40a6-9baa-02497fcef0d8",
   "metadata": {},
   "source": [
    "# Measuring performance in HIP applications\n",
    "\n",
    "An understanding of how well HIP applications perform is a vital part of the development process. Two main techniques, **profiling** and **tracing** collect information about how well an application is performing. **Profiling** is the statistical collection of the cumulative time that threads spend in each program component. **Tracing** is a collection of both **when** and **for how long** threads spend in each application component. Since HIP applications use either an AMD or a CUDA backend, the profiling tools from each platform are available for use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4bd9a5-220f-4ebc-99b5-fe753e6eca84",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Event based timing\n",
    "\n",
    "Events in HIP are used with streams to check the progress of work that has been submitted and establish dependencies between workflows. They can also be used to time the execution of work, such as kernels and memory copies. Here is how they fit into the picture of a HIP application.\n",
    "\n",
    "<figure style=\"margin-left:auto; margin-right:auto; width:70%;\">\n",
    "    <img style=\"vertical-\n",
    "                align:middle\" src=\"../images/hip_components.svg\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Components of a HIP application. Events are associated with streams, and provide a way to time the duration of work in a stream. </figcaption>\n",
    "</figure>\n",
    "\n",
    "## Example application\n",
    "\n",
    "The code [mat_mult_profiling.cpp](mat_mult_profiling.cpp) contains a complete example where events are used to time the execution of the host to device memory copy as well as the timing of the matrix multiplication kernel. The data type **hipEvent_t** stores event data. \n",
    "\n",
    "### Source code changes\n",
    "\n",
    "In [mat_mult_profiling.cpp](mat_mult_profiling.cpp) we use the function **hipEventCreate** to create two events **t1** and **t2**, as seen in line 111.\n",
    "\n",
    "```C++\n",
    "    // mat_mult_profiling.cpp:111\n",
    "\n",
    "    // Create events for the memory copies and kernel runs\n",
    "    hipEvent_t t1=0, t2=0;\n",
    "    // Create the events\n",
    "    H_ERRCHK(hipEventCreate(&t1));\n",
    "    H_ERRCHK(hipEventCreate(&t2));\n",
    "```\n",
    "\n",
    "Now we wish to use these events to time the upload of host matrices **A_h** and **B_h** to the compute device. The HIP function **hipEventRecord** inserts the event into the \"flow\" of a stream. We haven't talked in depth about HIP streams yet and at this stage we can think of a stream as a queue to which work is submitted. Since we are not using a particular stream we are using the default stream (denoted by 0). We insert event `t1` into the default stream, perform the memory copies, then insert `t2` after the copy is launched.\n",
    "\n",
    "```C++\n",
    "    // Record the start event into the default stream\n",
    "    H_ERRCHK(hipEventRecord(t1,0));\n",
    "    \n",
    "    // Peform the memory copies\n",
    "    H_ERRCHK(hipMemcpy(A_d, A_h, nbytes_A, hipMemcpyHostToDevice));\n",
    "    H_ERRCHK(hipMemcpy(B_d, B_h, nbytes_B, hipMemcpyHostToDevice));\n",
    "    \n",
    "    // Record the stop event into the default stream\n",
    "    H_ERRCHK(hipEventRecord(t2,0));\n",
    "```\n",
    "\n",
    "The function **hipEventSynchronize** waits until events reach a complete status. Then we can use the function **hipEventElapsedTime** to get the time elapsed between the two events. The helper function **h_get_event_time_ms** takes care of calling these functions, prints performance measurement information, and returns the number of milliseconds between the two events.\n",
    "\n",
    "```C++\n",
    "    // Total number of Bytes copied\n",
    "    size_t total_bytes = nbytes_A + nbytes_B;\n",
    "\n",
    "    // Get the elapsed time in milliseconds\n",
    "    float elapsed_ms = h_get_event_time_ms(t1, t2, \"memcpy\", &total_bytes);\n",
    "```\n",
    "\n",
    "The source code of **h_get_event_time_ms** is in <a href=\"../common/hip_helper.cpp\">hip_helper.cpp</a> and reproduced below:\n",
    "\n",
    "```C++\n",
    "// Get how much time elapsed between two events that were recorded\n",
    "float h_get_event_time_ms(\n",
    "        // Assumes start and stop events have been recorded\n",
    "        // with the hipEventRecord() function\n",
    "        hipEvent_t t1,\n",
    "        hipEvent_t t2,\n",
    "        const char* message, \n",
    "        size_t* nbytes) {\n",
    "    \n",
    "    // Make sure the stop and start events have finished\n",
    "    H_ERRCHK(hipEventSynchronize(t2));\n",
    "    H_ERRCHK(hipEventSynchronize(t1));\n",
    "\n",
    "    // Elapsed time in milliseconds\n",
    "    float elapsed_ms=0;\n",
    "\n",
    "    // Convert the time into milliseconds\n",
    "    H_ERRCHK(hipEventElapsedTime(&elapsed_ms, t1, t2));\n",
    "        \n",
    "    // Print the timing message if necessary\n",
    "    if ((message != NULL) && (strlen(message)>0)) {\n",
    "        std::printf(\"Time for event \\\"%s\\\": %.3f ms\", message, elapsed_ms);\n",
    "        \n",
    "        // Print transfer rate if nbytes is not NULL\n",
    "        if (nbytes != NULL) {\n",
    "            double io_rate_MBs = h_get_io_rate_MBs(\n",
    "                elapsed_ms, \n",
    "                *nbytes\n",
    "            );\n",
    "            std::printf(\" (%.2f MB/s)\", io_rate_MBs);\n",
    "        }\n",
    "        std::printf(\"\\n\");\n",
    "    }\n",
    "    \n",
    "    return elapsed_ms;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db80de96-12f4-474c-b8e7-fcb4e0d50a24",
   "metadata": {},
   "source": [
    "We can reuse the events to time the execution of the kernel. \n",
    "\n",
    "```C++\n",
    "    // Record the start event into the default stream\n",
    "    H_ERRCHK(hipEventRecord(t1,0));\n",
    "\n",
    "    // Launch the kernel using hipLaunchKernelGGL method\n",
    "    hipLaunchKernelGGL(mat_mult, \n",
    "            grid_nblocks, \n",
    "            block_size, sharedMemBytes, 0, \n",
    "            A_d, B_d, C_d,\n",
    "            N1_A,\n",
    "            N0_C,\n",
    "            N1_C\n",
    "    );\n",
    "\n",
    "    // Record the stop event into the default stream \n",
    "    H_ERRCHK(hipEventRecord(t2,0));\n",
    "\n",
    "    // Get the elapsed time in milliseconds\n",
    "    elapsed_ms = h_get_event_time_ms(t1, t2, \"mat_mult kernel\", NULL);\n",
    "```\n",
    "\n",
    "When we are finished with an event we can destroy them with the **hipEventDestroy** function. \n",
    "\n",
    "```C++\n",
    "    // Destroy events\n",
    "    H_ERRCHK(hipEventDestroy(t1));\n",
    "    H_ERRCHK(hipEventDestroy(t2));\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bd3ff9-409d-4f1a-a692-24e512e58bcb",
   "metadata": {},
   "source": [
    "In this manner we instrument the uploads, downloads, and kernel execution in the source file [mat_mult_profiling.cpp](mat_mult_profiling.cpp). Now we run the instrumented code and view the timing results. Change directory to **course_material/L5_Profiling** and run the following code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259bedb4-78c2-40fd-bd37-fb830077dbb9",
   "metadata": {},
   "source": [
    "## Import the environment\n",
    "\n",
    "The command below brings the `run` and `build` commands within reach of the Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5d0b201-baf0-4ff5-9333-63fb1f95eed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PATH'] = f\"{os.environ['PATH']}:../install/bin\"\n",
    "\n",
    "# At a Bash terminal you need to do this instead\n",
    "# source ../env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b389e5f0-8814-4f0b-a1e9-dc42501f3043",
   "metadata": {},
   "source": [
    "## Build and run the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63a7ca7f-acee-40b7-9ec5-30bd9d0ba996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target hip_helper\u001b[0m\n",
      "[ 66%] Built target hip_helper\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target mat_mult_profiling.exe\u001b[0m\n",
      "[100%] Built target mat_mult_profiling.exe\n",
      "\u001b[36mInstall the project...\u001b[0m\n",
      "-- Install configuration: \"RELEASE\"\n",
      "Device id: 0\n",
      "\tname:                                    NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "\tglobal memory size:                      6219 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                1536 \n",
      "\tmaximum shared memory size per block:    49 KB\n",
      "\tmaximum shared memory size per SM or CU: 0 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,64)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65535,65535)\n",
      "Time for event \"memcpy\": 0.132 ms (12039.76 MB/s)\n",
      "Time for event \"mat_mult kernel\": 0.523 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n"
     ]
    }
   ],
   "source": [
    "!build mat_mult_profiling.exe; run mat_mult_profiling.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec2892-86fe-46df-9e43-8446ebb8f977",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Performance measurement with AMD tools\n",
    "\n",
    "AMD has a number of tools available to help with collection of performance data. The low-level AMD profiler tool **ROC-profiler** (rocprof) has the ability to collect traces and information from hardware performance counters. Tools like [Omnitrace](https://github.com/AMDResearch/omnitrace) expand on the information collected by `rocprof` to include CPU resources and system metrics like GPU temperature and power usage. Tools like [Omniperf](https://github.com/AMDResearch/omniperf) use information from rocprof to help understand **how well** an application is performing in relation to peak performance, using reports such as roofline analysis and making information collected by rocprof understandable through graphical interfaces.\n",
    "\n",
    "### HIP application traces with rocprof\n",
    "\n",
    "Collection of HIP application traces with **rocprof** is accomplished with both the **--hip-trace** and **--hsa-trace** flags. Tracing with **rocprof** only seems to work with the AMD HIP backend at present. Here is what a typical profling command looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a09e5d94-80c2-4f99-99c5-afc97c285458",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RPL: on '240515_142039' from '/opt/rocm-6.0.2' in '/home/toby/Pelagos/Projects/HIP_Course/course_material/L5_Profiling'\n",
      "RPL: profiling '\"mat_mult_profiling.exe\"'\n",
      "RPL: input file ''\n",
      "RPL: output dir '/tmp/rpl_data_240515_142039_44532'\n",
      "RPL: result dir '/tmp/rpl_data_240515_142039_44532/input_results_240515_142039'\n",
      "ROCtracer (44556):\n",
      "Device id: 0\n",
      "\tname:                                    NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "\tglobal memory size:                      6219 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                1536 \n",
      "\tmaximum shared memory size per block:    49 KB\n",
      "\tmaximum shared memory size per SM or CU: 0 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,64)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65535,65535)\n",
      "Time for event \"memcpy\": 0.144 ms (11021.75 MB/s)\n",
      "Time for event \"mat_mult kernel\": 0.521 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n"
     ]
    }
   ],
   "source": [
    "!run rocprof --hip-trace --hsa-trace --sys-trace -o rocprof_trace/result.csv mat_mult_profiling.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b616c5e-6096-427a-af49-74a933c7e64b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Inside the **rocprof_trace** folder you will find the following files:\n",
    "\n",
    "| file | purpose |\n",
    "| --- | --- |\n",
    "| result.sysinfo.txt | System information on available devices |\n",
    "| result.copy_stats.csv | Statistics on all IO calls |\n",
    "| result.hip_stats.csv | Statistics on non-IO HIP function calls |\n",
    "| result.hsa_stats.csv | Statistics on HSA function calls |\n",
    "| result.stats.csv | Statistics on all kernel calls |\n",
    "| result.db | SQLITE3 database of profiling information |\n",
    "| result.json | Trace information in JSON format |\n",
    "| result.csv | Information on kernels such as **mat_mult** |\n",
    "\n",
    "We can load the trace file **rocprof_trace/result.json** using a web browser. In a web browser you can go to this site for a user interface on viewing trace information for offline use.\n",
    "\n",
    "[https://ui.perfetto.dev/](https://ui.perfetto.dev/)\n",
    "\n",
    "Download the trace file **result.json** to your computer and open it with the Perfetto UI in your web browser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3383f3-4c27-4daf-b506-68486ce77bd1",
   "metadata": {},
   "source": [
    "<figure style=\"margin-left:0; margin-right:auto; width:100%;\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/Perfetto_UI.png\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Viewing rocprof application traces with Perfetto UI.</figcaption>\n",
    "</figure>\n",
    "\n",
    "If you zoom (using the `wasd` keys) in you can see calls in GPU threads, COPY threads and HOST threads on the CPU. Notice how the **hipEventRecord** function is executed before and after the **hipMemcpy** calls and the **mat_mult** kernel execution. If you click on the **mat_mult** function you can see how long the kernel took to execute.\n",
    "\n",
    "<figure style=\"margin-left:0; margin-right:auto; width:100%;\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/Perfetto_UI_kernel.png\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Determining the time for a kernel call</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb51c18-aeb9-47f5-b5c2-57d8d7ee99cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hardware performance counters with rocprof\n",
    "\n",
    "Hardware performance counters are devices in a processor that measure events, such as the number of wavefronts executed, or the number of times a cache is missed. Rocprof can collect performance counters on kernels. The type of performance counter information that can be captured is obtained with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bedf65ea-5b6d-4262-b447-d4839f33216f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RPL: on '240507_135625' from '/opt/rocm-6.0.2' in '/home/toby/Pelagos/Projects/HIP_Course/course_material/L5_Profiling'\n",
      "Derived metrics:\n",
      "aqlprofile API table load failed: HSA_STATUS_ERROR: A generic error has occurred.\n",
      "/opt/rocm-6.0.2/bin/rocprof: line 466: 12882 Aborted                 (core dumped) /opt/rocm-6.0.2/lib/rocprofiler/rocprof-ctrl\n"
     ]
    }
   ],
   "source": [
    "!rocprof --list-derived"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce2b350-4a1c-4408-b807-9b79e5f911c6",
   "metadata": {},
   "source": [
    "We can specify the counters to collect in a file such as [rocprof_counters.txt](rocprof_counters.txt). Here we specify some commonly used metrics for collection. Each **pmc** line is a unique experiment involving an individual run of the code. In this example we collect stats for the **mat_mult** kernel for the first 64 work-items on GPU 0.\n",
    "\n",
    "```txt\n",
    "# Cache hits and Cache misses\n",
    "pmc: TCC_HIT_sum, TCC_MISS_sum\n",
    "\n",
    "# Total video memory fetched and written\n",
    "pmc: FETCH_SIZE, WRITE_SIZE\n",
    "\n",
    "# Percentage of time the GPU was busy, total wavefronts executed\n",
    "pmc: GPUBusy, Wavefronts\n",
    "\n",
    "# Average number of vector and scalar instructions executed per work-item\n",
    "pmc: VALUInsts, SALUInsts\n",
    "\n",
    "# Average number of vector and scalar fetch instructions per work-item\n",
    "pmc: VFetchInsts, SFetchInsts\n",
    "\n",
    "# Average number of vector write instructions per work-item\n",
    "pmc: VWriteInsts\n",
    "\n",
    "# Average number of shared and global memory read or write instructions per work item\n",
    "pmc: LDSInsts, GDSInsts\n",
    "\n",
    "# Percentage of active vector ALU threads in a wave, percentage of GPU time vector and scalar instructions are processed\n",
    "pmc: VALUUtilization, VALUBusy, SALUBusy, \n",
    "\n",
    "# Percentage of fetch, write, atomic, and other instructions that hit the L2 cache\n",
    "pmc: L2CacheHit\n",
    "\n",
    "# Percentage of time the memory unit is active (including stalled), and just stalled, percentage of time the write unit is stalled\n",
    "pmc: MemUnitBusy, MemUnitStalled, WriteUnitStalled\n",
    "\n",
    "# Percentage of time ALU's are stalled by shared memory access, percentage of GPU time local memory is stalled by bank conflicts\n",
    "pmc: ALUStalledByLDS, LDSBankConflict\n",
    "\n",
    "# Dispatches range, which work-items to profile\n",
    "range: 0 : 64\n",
    "# Which GPU's to profile\n",
    "gpu: 0\n",
    "# Names of kernels to profile\n",
    "kernel: mat_mult\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee575685-ab1f-40bf-aa3f-50224668d1aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "Then we can use rocprof to collect the data for these counters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e6071ab-48a0-4529-bc46-d7ea4d554965",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RPL: on '230510_161224' from '/opt/rocm-5.4.3' in '/home/toby/Pelagos/Projects/HIP_Course/course_material/L5_Profiling'\n",
      "RPL: profiling '\"./mat_mult_profiling.exe\"'\n",
      "RPL: input file 'rocprof_counters.txt'\n",
      "RPL: output dir '/tmp/rpl_data_230510_161224_23735'\n",
      "RPL: result dir '/tmp/rpl_data_230510_161224_23735/input0_results_230510_161224'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_230510_161224_23735/input0.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  2 metrics\n",
      "    TCC_HIT_sum, TCC_MISS_sum\n",
      "Device id: 0\n",
      "\tname:                                    \n",
      "\tglobal memory size:                      536 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum pitch size for memory copies:    536 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,2147483647,2147483647)\n",
      "Time for event \"memcpy\": 0.111 ms (14280.50 MB/s)\n",
      "Time for event \"mat_mult kernel\": 3.202 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 0 contexts collected, output directory /tmp/rpl_data_230510_161224_23735/input0_results_230510_161224\n",
      "RPL: result dir '/tmp/rpl_data_230510_161224_23735/input10_results_230510_161224'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_230510_161224_23735/input10.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  2 metrics\n",
      "    ALUStalledByLDS, LDSBankConflict\n",
      "Device id: 0\n",
      "\tname:                                    \n",
      "\tglobal memory size:                      536 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum pitch size for memory copies:    536 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,2147483647,2147483647)\n",
      "Time for event \"memcpy\": 0.106 ms (15010.18 MB/s)\n",
      "Time for event \"mat_mult kernel\": 3.045 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 0 contexts collected, output directory /tmp/rpl_data_230510_161224_23735/input10_results_230510_161224\n",
      "RPL: result dir '/tmp/rpl_data_230510_161224_23735/input1_results_230510_161224'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_230510_161224_23735/input1.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  2 metrics\n",
      "    FETCH_SIZE, WRITE_SIZE\n",
      "Device id: 0\n",
      "\tname:                                    \n",
      "\tglobal memory size:                      536 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum pitch size for memory copies:    536 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,2147483647,2147483647)\n",
      "Time for event \"memcpy\": 0.105 ms (15107.35 MB/s)\n",
      "Time for event \"mat_mult kernel\": 3.098 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 0 contexts collected, output directory /tmp/rpl_data_230510_161224_23735/input1_results_230510_161224\n",
      "RPL: result dir '/tmp/rpl_data_230510_161224_23735/input2_results_230510_161224'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_230510_161224_23735/input2.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  2 metrics\n",
      "    GPUBusy, Wavefronts\n",
      "Device id: 0\n",
      "\tname:                                    \n",
      "\tglobal memory size:                      536 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum pitch size for memory copies:    536 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,2147483647,2147483647)\n",
      "Time for event \"memcpy\": 0.105 ms (15084.55 MB/s)\n",
      "Time for event \"mat_mult kernel\": 3.621 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 0 contexts collected, output directory /tmp/rpl_data_230510_161224_23735/input2_results_230510_161224\n",
      "RPL: result dir '/tmp/rpl_data_230510_161224_23735/input3_results_230510_161224'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_230510_161224_23735/input3.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  2 metrics\n",
      "    VALUInsts, SALUInsts\n",
      "Device id: 0\n",
      "\tname:                                    \n",
      "\tglobal memory size:                      536 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum pitch size for memory copies:    536 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,2147483647,2147483647)\n",
      "Time for event \"memcpy\": 0.410 ms (3875.76 MB/s)\n",
      "Time for event \"mat_mult kernel\": 2.495 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 0 contexts collected, output directory /tmp/rpl_data_230510_161224_23735/input3_results_230510_161224\n",
      "RPL: result dir '/tmp/rpl_data_230510_161224_23735/input4_results_230510_161224'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_230510_161224_23735/input4.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  2 metrics\n",
      "    VFetchInsts, SFetchInsts\n",
      "Device id: 0\n",
      "\tname:                                    \n",
      "\tglobal memory size:                      536 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum pitch size for memory copies:    536 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,2147483647,2147483647)\n",
      "Time for event \"memcpy\": 0.105 ms (15072.96 MB/s)\n",
      "Time for event \"mat_mult kernel\": 3.448 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 0 contexts collected, output directory /tmp/rpl_data_230510_161224_23735/input4_results_230510_161224\n",
      "RPL: result dir '/tmp/rpl_data_230510_161224_23735/input5_results_230510_161224'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_230510_161224_23735/input5.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  1 metrics\n",
      "    VWriteInsts\n",
      "Device id: 0\n",
      "\tname:                                    \n",
      "\tglobal memory size:                      536 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum pitch size for memory copies:    536 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,2147483647,2147483647)\n",
      "Time for event \"memcpy\": 0.105 ms (15095.87 MB/s)\n",
      "Time for event \"mat_mult kernel\": 2.832 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 0 contexts collected, output directory /tmp/rpl_data_230510_161224_23735/input5_results_230510_161224\n",
      "RPL: result dir '/tmp/rpl_data_230510_161224_23735/input6_results_230510_161224'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_230510_161224_23735/input6.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  2 metrics\n",
      "    LDSInsts, GDSInsts\n",
      "Device id: 0\n",
      "\tname:                                    \n",
      "\tglobal memory size:                      536 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum pitch size for memory copies:    536 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,2147483647,2147483647)\n",
      "Time for event \"memcpy\": 0.429 ms (3704.43 MB/s)\n",
      "Time for event \"mat_mult kernel\": 1.943 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 0 contexts collected, output directory /tmp/rpl_data_230510_161224_23735/input6_results_230510_161224\n",
      "RPL: result dir '/tmp/rpl_data_230510_161224_23735/input7_results_230510_161224'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_230510_161224_23735/input7.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  3 metrics\n",
      "    VALUUtilization, VALUBusy, SALUBusy\n",
      "Device id: 0\n",
      "\tname:                                    \n",
      "\tglobal memory size:                      536 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum pitch size for memory copies:    536 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,2147483647,2147483647)\n",
      "Time for event \"memcpy\": 0.403 ms (3946.15 MB/s)\n",
      "Time for event \"mat_mult kernel\": 2.876 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 0 contexts collected, output directory /tmp/rpl_data_230510_161224_23735/input7_results_230510_161224\n",
      "RPL: result dir '/tmp/rpl_data_230510_161224_23735/input8_results_230510_161224'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_230510_161224_23735/input8.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  1 metrics\n",
      "    L2CacheHit\n",
      "Device id: 0\n",
      "\tname:                                    \n",
      "\tglobal memory size:                      536 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum pitch size for memory copies:    536 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,2147483647,2147483647)\n",
      "Time for event \"memcpy\": 0.352 ms (4510.31 MB/s)\n",
      "Time for event \"mat_mult kernel\": 1.755 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 0 contexts collected, output directory /tmp/rpl_data_230510_161224_23735/input8_results_230510_161224\n",
      "RPL: result dir '/tmp/rpl_data_230510_161224_23735/input9_results_230510_161224'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_230510_161224_23735/input9.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  3 metrics\n",
      "    MemUnitBusy, MemUnitStalled, WriteUnitStalled\n",
      "Device id: 0\n",
      "\tname:                                    \n",
      "\tglobal memory size:                      536 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum pitch size for memory copies:    536 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,2147483647,2147483647)\n",
      "Time for event \"memcpy\": 0.512 ms (3103.53 MB/s)\n",
      "Time for event \"mat_mult kernel\": 3.065 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 0 contexts collected, output directory /tmp/rpl_data_230510_161224_23735/input9_results_230510_161224\n",
      "File 'rocprof_counters/result.csv' is generating\n"
     ]
    }
   ],
   "source": [
    "!rocprof -i rocprof_counters.txt -o rocprof_counters/result.csv ./mat_mult_profiling.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7440b698-587e-42ec-a944-382dc9d2c481",
   "metadata": {
    "tags": []
   },
   "source": [
    "If your chosen performance counters are supported, then the file [rocprof_counters/result.csv](rocprof_counters/result.csv) should contain a count for every time the counter was triggered. The file [rocprof_counters/example.csv](rocprof_counters/example.csv) is an example file collected with rocprof on **mat_mult_profiling.exe**. This [page](https://docs.amd.com/bundle/ROCProfiler-User-Guide-v5.1/page/rocprof_Command_Line_Tool.html) has information on what the keys in the CSV file mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3090705-8e92-4834-b46b-af432243382c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Rocprof under a job manager\n",
    "\n",
    "Rocprof runs fine under a job manager like SLURM, you just need to make an output file for each process launched. For example on SLURM the `$SLURM_JOBID` and `$SLURM_PROCID` environment variables are helpful in separating the output. Put the rocprof commands in a script called **profile.sh**.\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "rocprof -i rocprof_counters.txt -o rocprof_counters/result-$SLURM_JOBID-$SLURM_PROCID.csv ./mat_mult_profiling_mpi.exe\n",
    "```\n",
    "\n",
    "Then you can run the script from **srun** like this so it picks up the environment variable **$SLURM_PROCID** from within the script.\n",
    "\n",
    "```bash\n",
    "srun -N $SLURM_JOB_NUM_NODES -n $SLURM_NTASKS -c $OMP_NUM_THREADS ./profile.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadc4531-2f2a-4437-a5d5-7ef16e670537",
   "metadata": {},
   "source": [
    "A complete example for using rocprof with an MPI-enabled application is in **course_material/L2_Using_HIP_On_Setonix/rocprof_mpi**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c822fc0-0f3b-4f05-b259-8bc690a7d250",
   "metadata": {},
   "source": [
    "### Rocprofiler API\n",
    "\n",
    "If you'd like to instrument code with profiling calls the **[rocprofiler API](https://github.com/ROCm-Developer-Tools/rocprofiler/blob/amd-master/doc/rocprofiler_spec.md)** is available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c16d9e3-0dc9-478f-8d84-3f71f67579c9",
   "metadata": {},
   "source": [
    "### Tracing with Omnitrace\n",
    "\n",
    "[Omnitrace](https://github.com/AMDResearch/omnitrace) is an AMD research project to collect performance information on a program at runtime. It supports programs written in C, C++, Fortran and Python, as well as compute frameworks like OpenCL and HIP. Load the modules for Omnitrace, (you will find these commands in either the welcome letter or in Lesson 2). Now compile the software with `make`.\n",
    "\n",
    "```bash\n",
    "cd course_material/L5_Profiling\n",
    "make\n",
    "```\n",
    "\n",
    "Then we can use Omnitrace to make a trace of **mat_mult_profiling.exe**.\n",
    "\n",
    "```bash\n",
    "omnitrace-instrument -- ./mat_mult_profiling.exe\n",
    "```\n",
    "\n",
    "Or we can have omnitrace **instrument** the application for profiling. This is useful if we want to run an application with MPI support.\n",
    "\n",
    "```bash\n",
    "omnitrace-instrument -v -1 -o mat_mult_profiling.inst.exe -- ./mat_mult_profiling.exe\n",
    "omnitrace-run -- ./mat_mult_profiling.inst.exe\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bee8406-d101-4c4c-884e-a6a86cfc872f",
   "metadata": {},
   "source": [
    "If you look in the subfolders \n",
    "\n",
    "* **omnitrace-mat_mult_profiling-output**\n",
    "* **omnitrace-mat_mult_profiling.inst-output**, \n",
    "\n",
    "either in **course_material/L5_Profiling** or in the example folder **course_material/L5_Profiling/omnitrace_example** there are subfolders with dates on them. In those subfolders are `*.proto` files for use with perfetto. Download the **.proto** file to your computer and open it with [ui.perfetto.dev](https://ui.perfetto.dev) in a similar way to the json trace files from rocprof. You should see when and for how long functions are executed on the host and for how long kernels are executed on the device, along with a more detailed set of metrics such as CPU frequency and power consumption.\n",
    "\n",
    "<figure style=\"margin-left:0; margin-right:auto; width:100%;\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/omnitrace.png\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Examining the output from Omnitrace using <a href=\"https://ui.perfetto.dev\">ui.perfetto.dev</a></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a0a9d0-0406-47a1-bf01-b1eb087a7c75",
   "metadata": {},
   "source": [
    "### Performance measurement with Omniperf\n",
    "\n",
    "The AMD research tool Omniperf [Omniperf](https://github.com/AMDResearch/omniperf) is a powerful tool for measuring the performance of applications on AMD Instinct GPU's like the MI250X on Setonix. It can perform feats like [Roofline Analysis](https://en.wikipedia.org/wiki/Roofline_model). Load the Omniperf modules, using the module load commands from either the welcome letter or from Lesson 2. Then use Omniperf like this to make an analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4b44b0-9b5a-4fbc-8702-c3ea664a4b2c",
   "metadata": {},
   "source": [
    "```bash\n",
    "omniperf profile -n mat_mult -- ./mat_mult_profiling.exe -o mat_mult.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732da7f1-ddba-434a-ae93-d945401af617",
   "metadata": {},
   "source": [
    "The resulting hardware collection information is in a directory called **workloads/mat_mult**. You can view the output in text format using the command\n",
    "\n",
    "```bash\n",
    "omniperf analyze -p workloads/mat_mult/mi200 &> analysis.txt\n",
    "```\n",
    "\n",
    "or, if you have Omniperf installed to your laptop you can see the results from your web browser. Download the **workloads** directory to your computer and run this command. \n",
    "\n",
    "```bash\n",
    "omniperf analyze -p workloads/mat_mult/mi200 --gui\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcdbb79-6c0e-4133-98be-d54c7631dfba",
   "metadata": {},
   "source": [
    "Then you should be able to go to the location [http://127.0.0.1:8050](http://127.0.0.1:8050) and view the profiling information collected. An example data collection is in **course_material/L5_Profiling/omniperf_example**.\n",
    "\n",
    "#### Roofline models with Omniperf\n",
    "\n",
    "The **[Arithmetic intensity](https://en.wikipedia.org/wiki/Roofline_model#Arithmetic_intensity)** of an algorithm is the ratio of floating point operations (FLOPS) computed per byte transferred. It helps us gauge if an algorithm is likely to be constrained by either the bandwidth or floating point performance of a compute resource. In matrix multiplication the input matrix **A** is of size ($N_{0,C}, N_{1,A}$) and **B** is of size ($N_{1,A}, N_{1,C}$). Every element of matrix **C** requires $N_{1,A}$ loads from A, $N_{1,A}$ loads from B, and 1 store to **C**. It also requires $N_{1,A}$ multiplications and $N_{1,A}$ additions. The arithmetic intensity of matrix multiplication is then\n",
    "\n",
    "$$ a = \\frac{2N1_A}{(2N1_A+1)b} $$\n",
    "\n",
    "where **b** is the number of bytes stored per element. When $N1_A$ is large the theoretical arithmetic intensity for matrix multiplication is\n",
    "\n",
    "$$ a \\approx \\frac{1}{b}. $$\n",
    "\n",
    "If a processor has a peak floating point performance of $\\textbf{F}_{P}$ FLOP/second, and a particular cache can feed that processor at a peak bandwidth of $\\textbf{B}_{P}$ bytes/second, then we can calculate a floating point limit that is dependent on memory bandwidth.\n",
    "\n",
    "$$F_{B} = a  \\frac{\\mbox{FLOP}}{\\mbox{byte}} B_{P}\\frac{\\mbox{byte}}{\\mbox{second}} = a B_{P} \\frac{\\mbox{FLOP}}{\\mbox{second}}$$ \n",
    "\n",
    "The actual attainable floating point performance will be either $F_{B}$ or $F_{P}$, whatever is lower. If we set $F_{B} = F_{P}$ then we can solve for the crossover point in arithmetic intensity.\n",
    "\n",
    "$$a_{0}=\\frac{F_{P}}{B_{P}}$$\n",
    "\n",
    "Therefore the limits (or roofline) on performance is as follows:  \n",
    "\n",
    "$$\n",
    "F = \\left \\{\n",
    "\\begin{array}{rl}\n",
    "aB_{P} & \\mbox{if} \\space a<\\frac{F_{P}}{B_{P}},\\\\\n",
    "F_{P}& \\mbox{otherwise}\n",
    "\\end{array}\n",
    "\\right .\n",
    "$$\n",
    "\n",
    "For example, a single compute device in a AMD Mi250x GPU processor has a peak 32-bit floating point processing rate of $F_{P} = 23.95$ TFLOPS and a peak memory bandwidth of $F_{B}=1.6$ TB/s from global memory. Problems will be constrained by memory bandwidth up to an arithmetic intensity of \n",
    "\n",
    "$$a_{0}=\\frac{23.95}{1.6} \\approx 15$$\n",
    "\n",
    "Shown below is a roofline plot of **mat_mult_profiling.cpp**, showing the various rooflines for the L1, L2, and global memory (HBM) caches. The crossover point for 32-bit floating point arithmetic and global memory is correctly situated around **a=15**. Performance in loading memory cache appears to be close to optimal at the theoretical arithmetic intensity of a=0.25, however significant gains in performance look possible if we can improve loads from main memory.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3038dfaa-3008-4326-bd52-cbf8e5fd9baa",
   "metadata": {},
   "source": [
    "<figure style=\"margin-left:0; margin-right:auto; width:70%;\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/roofline_plot.png\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Roofline model created from mat_mult_profiling.exe</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302f458b-1bd1-4fb9-8856-b2d94677ded9",
   "metadata": {},
   "source": [
    "If you just want **pdf** versions of the roofline models then run this command\n",
    "\n",
    "```bash\n",
    "omniperf profile -n mat_mult --roof-only -- ./mat_mult_profiling.exe\n",
    "```\n",
    "\n",
    "The pdf files will be available in **workloads/mat_mult/mi200**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b2aaf2-ef12-44af-b96a-1e2a35a75082",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Performance measurement with NVIDIA tools\n",
    "\n",
    "HIP applications that use the CUDA backend (when CUDA is available and the environment variable `HIP_PLATFORM` is set to `nvidia`) have access to the NVIDIA performance measurement tools such as [NVIDIA Nsight Systems](https://developer.nvidia.com/nsight-systems) and [NVIDIA Nsight Compute](https://developer.nvidia.com/nsight-compute). Here we briefly cover how to use these tools.\n",
    "\n",
    "### Tracing with Nsight Systems\n",
    "\n",
    "The command line application **nsys** can collect traces on **mat_mult_profiling.exe**. Make sure you have re-compiled **mat_mult_profiling.exe** with the `HIP_PLATFORM` environment variable set to `nvidia`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8e3e699-6e16-431b-94f8-39c07100ac87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: CPU IP/backtrace sampling not supported, disabling.\n",
      "Try the 'nsys status --environment' command to learn more.\n",
      "\n",
      "WARNING: CPU context switch tracing not supported, disabling.\n",
      "Try the 'nsys status --environment' command to learn more.\n",
      "\n",
      "Device id: 0\n",
      "\tname:                                    NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "\tglobal memory size:                      6226 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    49 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,64)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65535,65535)\n",
      "Time for event \"memcpy\": 0.152 ms (10464.39 MB/s)\n",
      "Time for event \"mat_mult kernel\": 0.452 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "Generating '/tmp/nsys-report-ce04.qdstrm'\n",
      "Failed to create '/home/toby/Pelagos/Projects/HIP_Course/course_material/L5_Profiling/nsys_trace/results.nsys-rep': File exists.\n",
      "Use `--force-overwrite true` to overwrite existing files.\n",
      "[1/1] [========================100%] nsys-report-077a.nsys-rep\n",
      "Generated:\n",
      "    /tmp/nsys-report-077a.nsys-rep\n"
     ]
    }
   ],
   "source": [
    "!nsys profile -o nsys_trace/results ./mat_mult_profiling.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451fd761-d14f-407b-afd4-319424719b78",
   "metadata": {},
   "source": [
    "Then you can use this command under Linux to view the application trace \n",
    "\n",
    "```bash\n",
    "nsys-ui nsys_trace/results.nsys-rep\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51171e76-80dc-4c03-a1b8-0fe557832350",
   "metadata": {},
   "source": [
    "It's important to note that when using the NVIDIA backend it is important to note that HIP is a **thin layer** over CUDA. NVIDIA performance tools will report usage for the underlying CUDA functions instead of the HIP labelled functions. For example, when using Nsight Systems it will report a call to **cudaDeviceSynchronize** instead of **hipDeviceSynchronize**. One has to make the mental mapping between HIP and CUDA API calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5b9bfd-94ac-4158-bb20-300b732adf93",
   "metadata": {},
   "source": [
    "### Hardware collection with Nsight compute\n",
    "\n",
    "Nsight compute has the ability to collect hardware performance counters, however this ability needs either administrator access or access granted to performance counters at the OS level. If this access is possible then the following command will collect hardware performance counters on **mat_mult_profiling.exe**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "220a1c36-8bb4-44ff-b508-1437fb28234f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==PROF== Connected to process 9050 (/home/toby/Pelagos/Projects/HIP_Course/course_material/L5_Profiling/mat_mult_profiling.exe)\n",
      "Device id: 0\n",
      "\tname:                                    NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "\tglobal memory size:                      6226 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    49 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,64)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65535,65535)\n",
      "Time for event \"memcpy\": 0.140 ms (11341.40 MB/s)\n",
      "==PROF== Profiling \"mat_mult\" - 0: 0%....50%....100% - 9 passes\n",
      "Time for event \"mat_mult kernel\": 200.655 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "==PROF== Disconnected from process 9050\n",
      "==PROF== Report: /home/toby/Pelagos/Projects/HIP_Course/course_material/L5_Profiling/ncu_counters/results.ncu-rep\n"
     ]
    }
   ],
   "source": [
    "!ncu -f -o ncu_counters/results ./mat_mult_profiling.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cd0f55-b0c0-48d1-98bd-b7be9ef65991",
   "metadata": {},
   "source": [
    "Then you can run the command:\n",
    "\n",
    "```bash\n",
    "ncu-ui\n",
    "```\n",
    "\n",
    "To view the hardware performance counter information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ded4596-2705-4315-800b-624f8e062d9c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This chapter covers how to measure performance in HIP applications. HIP events are tools within the HIP framework to measure the execution time of kernels or memory copies. External tools such as `rocprof` can trace applications to collect information on **when** and for **how long** compute resources are used, as well as collecting low level information from hardware performance counters. Higher level tools like **Omnitrace** and **Omniperf** collect additional information and make the information obtained through rocprof more accessible through GUI-based reporting tools. Since HIP is a cross-platform environment, we conclude the chapter by walking through some performance monitoring tools that NVIDIA backends can make use of."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c38c04c-7c8d-4646-a382-0b02fb91a42e",
   "metadata": {},
   "source": [
    "<address>\n",
    "Written by Dr. Toby Potter of <a href=\"https://www.pelagos-consulting.com\">Pelagos Consulting and Education</a> for the <a href=\"https://pawsey.org.au\">Pawsey Supercomputing Centre</a>.<br>\n",
    "</address>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
