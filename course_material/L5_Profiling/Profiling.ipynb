{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da6d105-2df5-40a6-9baa-02497fcef0d8",
   "metadata": {},
   "source": [
    "# Measuring performance in HIP applications\n",
    "\n",
    "An understanding of how well HIP applications perform is a vital part of the development process. Two main techniques, **profiling** and **tracing** collect information about how well an application is performing. **Profiling** is the statistical collection of the cumulative time that threads spend in each program component. **Tracing** is a collection of both **when** and **for how long** threads spend in each application component. Since HIP applications use either an AMD or a CUDA backend, the profiling tools from each platform are available for use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4bd9a5-220f-4ebc-99b5-fe753e6eca84",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Event based timing\n",
    "\n",
    "Events in HIP are used with streams to check the progress of work that has been submitted and establish dependencies between workflows. They can also be used to time the execution of work, such as kernels and memory copies. Here is how they fit into the picture of a HIP application.\n",
    "\n",
    "<figure style=\"margin-left:auto; margin-right:auto; width:70%;\">\n",
    "    <img style=\"vertical-\n",
    "                align:middle\" src=\"../images/hip_components.svg\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Components of a HIP application. Events are associated with streams, and provide a way to time the duration of work in a stream. </figcaption>\n",
    "</figure>\n",
    "\n",
    "## Example application\n",
    "\n",
    "The code [mat_mult_profiling.cpp](mat_mult_profiling.cpp) contains a complete example where events are used to time the execution of the host to device memory copy as well as the timing of the matrix multiplication kernel. The data type **hipEvent_t** stores event data. \n",
    "\n",
    "### Source code changes\n",
    "\n",
    "In [mat_mult_profiling.cpp](mat_mult_profiling.cpp) we use the function **hipEventCreate** to create two events **t1** and **t2**, as seen in line 111.\n",
    "\n",
    "```C++\n",
    "    // mat_mult_profiling.cpp:111\n",
    "\n",
    "    // Create events for the memory copies and kernel runs\n",
    "    hipEvent_t t1=0, t2=0;\n",
    "    // Create the events\n",
    "    H_ERRCHK(hipEventCreate(&t1));\n",
    "    H_ERRCHK(hipEventCreate(&t2));\n",
    "```\n",
    "\n",
    "Now we wish to use these events to time the upload of host matrices **A_h** and **B_h** to the compute device. The HIP function **hipEventRecord** inserts the event into the \"flow\" of a stream. We haven't talked in depth about HIP streams yet and at this stage we can think of a stream as a queue to which work is submitted. Since we are not using a particular stream we are using the default stream (denoted by 0). We insert event `t1` into the default stream, perform the memory copies, then insert `t2` after the copy is launched.\n",
    "\n",
    "```C++\n",
    "    // Record the start event into the default stream\n",
    "    H_ERRCHK(hipEventRecord(t1,0));\n",
    "    \n",
    "    // Peform the memory copies\n",
    "    H_ERRCHK(hipMemcpy(A_d, A_h, nbytes_A, hipMemcpyHostToDevice));\n",
    "    H_ERRCHK(hipMemcpy(B_d, B_h, nbytes_B, hipMemcpyHostToDevice));\n",
    "    \n",
    "    // Record the stop event into the default stream\n",
    "    H_ERRCHK(hipEventRecord(t2,0));\n",
    "```\n",
    "\n",
    "The function **hipEventSynchronize** waits until events reach a complete status. Then we can use the function **hipEventElapsedTime** to get the time elapsed between the two events. The helper function **h_get_event_time_ms** takes care of calling these functions, prints performance measurement information, and returns the number of milliseconds between the two events.\n",
    "\n",
    "```C++\n",
    "    // Total number of Bytes copied\n",
    "    size_t total_bytes = nbytes_A + nbytes_B;\n",
    "\n",
    "    // Get the elapsed time in milliseconds\n",
    "    float elapsed_ms = h_get_event_time_ms(t1, t2, \"memcpy\", &total_bytes);\n",
    "```\n",
    "\n",
    "The source code of **h_get_event_time_ms** is in <a href=\"../common/hip_helper.cpp\">hip_helper.cpp</a> and reproduced below:\n",
    "\n",
    "```C++\n",
    "// Get how much time elapsed between two events that were recorded\n",
    "float h_get_event_time_ms(\n",
    "        // Assumes start and stop events have been recorded\n",
    "        // with the hipEventRecord() function\n",
    "        hipEvent_t t1,\n",
    "        hipEvent_t t2,\n",
    "        const char* message, \n",
    "        size_t* nbytes) {\n",
    "    \n",
    "    // Make sure the stop and start events have finished\n",
    "    H_ERRCHK(hipEventSynchronize(t2));\n",
    "    H_ERRCHK(hipEventSynchronize(t1));\n",
    "\n",
    "    // Elapsed time in milliseconds\n",
    "    float elapsed_ms=0;\n",
    "\n",
    "    // Convert the time into milliseconds\n",
    "    H_ERRCHK(hipEventElapsedTime(&elapsed_ms, t1, t2));\n",
    "        \n",
    "    // Print the timing message if necessary\n",
    "    if ((message != NULL) && (strlen(message)>0)) {\n",
    "        std::printf(\"Time for event \\\"%s\\\": %.3f ms\", message, elapsed_ms);\n",
    "        \n",
    "        // Print transfer rate if nbytes is not NULL\n",
    "        if (nbytes != NULL) {\n",
    "            double io_rate_MBs = h_get_io_rate_MBs(\n",
    "                elapsed_ms, \n",
    "                *nbytes\n",
    "            );\n",
    "            std::printf(\" (%.2f MB/s)\", io_rate_MBs);\n",
    "        }\n",
    "        std::printf(\"\\n\");\n",
    "    }\n",
    "    \n",
    "    return elapsed_ms;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db80de96-12f4-474c-b8e7-fcb4e0d50a24",
   "metadata": {},
   "source": [
    "We can reuse the events to time the execution of the kernel. \n",
    "\n",
    "```C++\n",
    "    // Record the start event into the default stream\n",
    "    H_ERRCHK(hipEventRecord(t1,0));\n",
    "\n",
    "    // Launch the kernel using hipLaunchKernelGGL method\n",
    "    hipLaunchKernelGGL(mat_mult, \n",
    "            grid_nblocks, \n",
    "            block_size, sharedMemBytes, 0, \n",
    "            A_d, B_d, C_d,\n",
    "            N1_A,\n",
    "            N0_C,\n",
    "            N1_C\n",
    "    );\n",
    "\n",
    "    // Record the stop event into the default stream \n",
    "    H_ERRCHK(hipEventRecord(t2,0));\n",
    "\n",
    "    // Get the elapsed time in milliseconds\n",
    "    elapsed_ms = h_get_event_time_ms(t1, t2, \"mat_mult kernel\", NULL);\n",
    "```\n",
    "\n",
    "When we are finished with an event we can destroy them with the **hipEventDestroy** function. \n",
    "\n",
    "```C++\n",
    "    // Destroy events\n",
    "    H_ERRCHK(hipEventDestroy(t1));\n",
    "    H_ERRCHK(hipEventDestroy(t2));\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bd3ff9-409d-4f1a-a692-24e512e58bcb",
   "metadata": {},
   "source": [
    "In this manner we instrument the uploads, downloads, and kernel execution in the source file [mat_mult_profiling.cpp](mat_mult_profiling.cpp). Now we run the instrumented code and view the timing results. Change directory to **course_material/L5_Profiling** and run the following code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259bedb4-78c2-40fd-bd37-fb830077dbb9",
   "metadata": {},
   "source": [
    "## Import the environment\n",
    "\n",
    "The command below brings the `run` and `build` commands within reach of the Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5d0b201-baf0-4ff5-9333-63fb1f95eed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PATH'] = f\"{os.environ['PATH']}:../install/bin\"\n",
    "\n",
    "# At a Bash terminal you need to do this instead\n",
    "# source ../env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b389e5f0-8814-4f0b-a1e9-dc42501f3043",
   "metadata": {},
   "source": [
    "## Build and run the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63a7ca7f-acee-40b7-9ec5-30bd9d0ba996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 66%] Built target hip_helper\n",
      "[100%] Built target mat_mult_profiling.exe\n",
      "\u001b[36mInstall the project...\u001b[0m\n",
      "-- Install configuration: \"RELEASE\"\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon VII\n",
      "\tglobal memory size:                      17163 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2560 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65536,65536)\n",
      "Time for event \"memcpy\": 1.189 ms (1336.85 MB/s)\n",
      "Time for event \"mat_mult kernel\": 0.821 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n"
     ]
    }
   ],
   "source": [
    "!build mat_mult_profiling.exe; run mat_mult_profiling.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec2892-86fe-46df-9e43-8446ebb8f977",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Performance measurement with AMD tools\n",
    "\n",
    "AMD has a number of tools available to help with collection of performance data. The low-level AMD profiler tool **ROC-profiler** (rocprof) has the ability to collect traces and information from hardware performance counters. Tools like [Omnitrace](https://github.com/AMDResearch/omnitrace) expand on the information collected by `rocprof` to include CPU resources and system metrics like GPU temperature and power usage. Tools like [Omniperf](https://github.com/AMDResearch/omniperf) use information from rocprof to help understand **how well** an application is performing in relation to peak performance, using reports such as roofline analysis and making information collected by rocprof understandable through graphical interfaces.\n",
    "\n",
    "### HIP application traces with rocprof\n",
    "\n",
    "Collection of HIP application traces with **rocprof** is accomplished with both the **--hip-trace** and **--hsa-trace** flags. Tracing with **rocprof** only seems to work with the AMD HIP backend at present. Here is what a typical profling command looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a09e5d94-80c2-4f99-99c5-afc97c285458",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘rocprof_trace’: File exists\n",
      "RPL: on '240517_162558' from '/opt/rocm-6.0.2' in '/nethome/tpotter/Pelagos/Projects/HIP_Course/course_material/L5_Profiling'\n",
      "RPL: profiling '\"mat_mult_profiling.exe\"'\n",
      "RPL: input file ''\n",
      "RPL: output dir '/tmp/rpl_data_240517_162558_3228'\n",
      "RPL: result dir '/tmp/rpl_data_240517_162558_3228/input_results_240517_162558'\n",
      "ROCtracer (3252):\n",
      "ROCProfiler: input from \"/tmp/rpl_data_240517_162558_3228/input.xml\"\n",
      "  0 metrics\n",
      "    HSA-trace(*)\n",
      "    HSA-activity-trace()\n",
      "    HIP-trace(*)\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon VII\n",
      "\tglobal memory size:                      17163 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2560 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65536,65536)\n",
      "Time for event \"memcpy\": 1.133 ms (1402.94 MB/s)\n",
      "Time for event \"mat_mult kernel\": 0.840 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 1 contexts collected, output directory /tmp/rpl_data_240517_162558_3228/input_results_240517_162558\n",
      "hsa_copy_deps: 1\n",
      "scan hsa API data 563:564                                                                                                    hsa_copy_deps: 0\n",
      "scan hip API data 45:46                                                                                                    File 'rocprof_trace/result.csv' is generating\n",
      "File 'rocprof_trace/result.stats.csv' is generating\n",
      "dump json 0:1                                                                                                    \n",
      "File 'rocprof_trace/result.json' is generating\n",
      "File 'rocprof_trace/result.hsa_stats.csv' is generating\n",
      "dump json 564:565                                                                                                    \n",
      "File 'rocprof_trace/result.json' is generating\n",
      "File 'rocprof_trace/result.copy_stats.csv' is generating\n",
      "dump json 2:3                                                                                                    \n",
      "File 'rocprof_trace/result.json' is generating\n",
      "File 'rocprof_trace/result.hip_stats.csv' is generating\n",
      "dump json 45:46                                                                                                    \n",
      "File 'rocprof_trace/result.json' is generating\n"
     ]
    }
   ],
   "source": [
    "!mkdir rocprof_trace; run rocprof --hip-trace --hsa-trace --sys-trace -o rocprof_trace/result.csv mat_mult_profiling.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2add0e6-e446-4a60-b757-9d9190b6d1c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "Inside the **rocprof_trace** folder you will find the following files:\n",
    "\n",
    "| file | purpose |\n",
    "| --- | --- |\n",
    "| result.sysinfo.txt | System information on available devices |\n",
    "| result.copy_stats.csv | Statistics on all IO calls |\n",
    "| result.hip_stats.csv | Statistics on non-IO HIP function calls |\n",
    "| result.hsa_stats.csv | Statistics on HSA function calls |\n",
    "| result.stats.csv | Statistics on all kernel calls |\n",
    "| result.db | SQLITE3 database of profiling information |\n",
    "| result.json | Trace information in JSON format |\n",
    "| result.csv | Information on kernels such as **mat_mult** |\n",
    "\n",
    "From the [Rocprof Documentation](https://rocmdocs.amd.com/projects/rocprofiler/en/latest/rocprofv1.html#introduction) some useful metrics in the files have the following codes\n",
    "\n",
    "|Metric| Units | Explanation |\n",
    "| :---- | :---- | :--- |\n",
    "| grd | integer | Number of threads scheduled (size of the grid) |\n",
    "| wgr | integer | Number of threads in each block (workgroup) |\n",
    "| lds | Bytes | Number of Local Data Share (LDS) bytes used by the block (workgroup) |\n",
    "| scr | Bytes | Number of scratch memory bytes used by the block |\n",
    "| sgpr | integer | Number of Scalar General Purpose Registers (SGPR's) used by each kernel |\n",
    "| vgpr | integer | Number of Vector General Purpose Registers (VGPR's) used by each kernel |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fb9c76-2da4-47a7-9098-c4ee926e5419",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Visualising traces with Perfetto\n",
    "\n",
    "We can load the trace file **rocprof_trace/result.json** using a web browser. In a web browser you can go to this site for a user interface on viewing trace information for offline use.\n",
    "\n",
    "[https://ui.perfetto.dev/](https://ui.perfetto.dev/)\n",
    "\n",
    "Download the trace file **result.json** to your computer and open it with the Perfetto UI in your web browser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3383f3-4c27-4daf-b506-68486ce77bd1",
   "metadata": {},
   "source": [
    "If you zoom (using the `wasd` keys) in you can see calls in GPU threads, COPY threads and HOST threads on the CPU. Notice how the **hipEventRecord** function is executed before and after the **hipMemcpy** calls and the **mat_mult** kernel execution. If you click on the **mat_mult** function you can see how long the kernel took to execute as well as some of the Metrics defined in the table above.\n",
    "\n",
    "<figure style=\"margin-left:0; margin-right:auto; width:100%;\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/Perfetto_UI_kernel.png\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Determining the time for a kernel call</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb51c18-aeb9-47f5-b5c2-57d8d7ee99cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hardware performance counters with rocprof\n",
    "\n",
    "Hardware performance counters are devices in a processor that measure events, such as the number of wavefronts executed, or the number of times a cache is missed. Rocprof can collect performance counters on kernels. \n",
    "\n",
    "#### Basic performance counters\n",
    "\n",
    "The fundamental hardware performance counters collected with Rocprof can be obtained with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bedf65ea-5b6d-4262-b447-d4839f33216f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RPL: on '240517_162602' from '/opt/rocm-6.0.2' in '/nethome/tpotter/Pelagos/Projects/HIP_Course/course_material/L5_Profiling'\n",
      "Basic HW counters:\n",
      "\n",
      "  gpu-agent1 : TCC_EA1_WRREQ[0-15] : Number of transactions (either 32-byte or 64-byte) going over the TC_EA_wrreq interface. Atomics may travel over the same interface and are generally classified as write requests. This does not include probe commands.\n",
      "      block TCC has 4 counters\n",
      "\n",
      "  gpu-agent1 : TCC_EA1_WRREQ_64B[0-15] : Number of 64-byte transactions going (64-byte write or CMPSWAP) over the TC_EA_wrreq interface.\n",
      "      block TCC has 4 counters\n",
      "\n",
      "  gpu-agent1 : TCC_EA1_WRREQ_STALL[0-15] : Number of cycles a write request was stalled.\n",
      "      block TCC has 4 counters\n",
      "\n",
      "  gpu-agent1 : TCC_EA1_RDREQ[0-15] : Number of TCC/EA read requests (either 32-byte or 64-byte)\n",
      "      block TCC has 4 counters\n",
      "\n",
      "  gpu-agent1 : TCC_EA1_RDREQ_32B[0-15] : Number of 32-byte TCC/EA read requests\n",
      "      block TCC has 4 counters\n",
      "\n",
      "  gpu-agent1 : GRBM_COUNT : Tie High - Count Number of Clocks\n",
      "      block GRBM has 2 counters\n",
      "\n",
      "  gpu-agent1 : GRBM_GUI_ACTIVE : The GUI is Active\n",
      "      block GRBM has 2 counters\n",
      "\n",
      "  gpu-agent1 : SQ_WAVES : Count number of waves sent to SQs. (per-simd, emulated, global)\n",
      "      block SQ has 8 counters\n",
      "\n",
      "  gpu-agent1 : SQ_INSTS_VALU : Number of VALU instructions issued. (per-simd, emulated)\n",
      "      block SQ has 8 counters\n",
      "\n",
      "  gpu-agent1 : SQ_INSTS_VMEM_WR : Number of VMEM write instructions issued (including FLAT). (per-simd, emulated)\n",
      "      block SQ has 8 counters\n",
      "\n",
      "  gpu-agent1 : SQ_INSTS_VMEM_RD : Number of VMEM read instructions issued (including FLAT). (per-simd, emulated)\n",
      "      block SQ has 8 counters\n",
      "\n",
      "  gpu-agent1 : SQ_INSTS_SALU : Number of SALU instructions issued. (per-simd, emulated)\n",
      "      block SQ has 8 counters\n",
      "\n",
      "  gpu-agent1 : SQ_INSTS_SMEM : Number of SMEM instructions issued. (per-simd, emulated)\n",
      "      block SQ has 8 counters\n",
      "\n",
      "  gpu-agent1 : SQ_INSTS_FLAT : Number of FLAT instructions issued. (per-simd, emulated)\n",
      "      block SQ has 8 counters\n",
      "\n",
      "  gpu-agent1 : SQ_INSTS_FLAT_LDS_ONLY : Number of FLAT instructions issued that read/wrote only from/to LDS (only works if EARLY_TA_DONE is enabled). (per-simd, emulated)\n",
      "      block SQ has 8 counters\n",
      "\n",
      "  gpu-agent1 : SQ_INSTS_LDS : Number of LDS instructions issued (including FLAT). (per-simd, emulated)\n",
      "      block SQ has 8 counters\n",
      "\n",
      "  gpu-agent1 : SQ_INSTS_GDS : Number of GDS instructions issued. (per-simd, emulated)\n",
      "      block SQ has 8 counters\n",
      "\n",
      "  gpu-agent1 : SQ_WAIT_INST_LDS : Number of wave-cycles spent waiting for LDS instruction issue. In units of 4 cycles. (per-simd, nondeterministic)\n",
      "      block SQ has 8 counters\n",
      "\n",
      "  gpu-agent1 : SQ_ACTIVE_INST_VALU : regspec 71? Number of cycles the SQ instruction arbiter is working on a VALU instruction. (per-simd, nondeterministic). Units in quad-cycles(4 cycles)\n",
      "      block SQ has 8 counters\n",
      "\n",
      "  gpu-agent1 : SQ_INST_CYCLES_SALU : Number of cycles needed to execute non-memory read scalar operations. (per-simd, emulated)\n",
      "      block SQ has 8 counters\n",
      "\n",
      "  gpu-agent1 : SQ_THREAD_CYCLES_VALU : Number of thread-cycles used to execute VALU operations (similar to INST_CYCLES_VALU but multiplied by # of active threads). (per-simd)\n",
      "      block SQ has 8 counters\n",
      "\n",
      "  gpu-agent1 : SQ_LDS_BANK_CONFLICT : Number of cycles LDS is stalled by bank conflicts. (emulated)\n",
      "      block SQ has 8 counters\n",
      "\n",
      "  gpu-agent1 : TA_TA_BUSY[0-15] : TA block is busy. Perf_Windowing not supported for this counter.\n",
      "      block TA has 2 counters\n",
      "\n",
      "  gpu-agent1 : TA_FLAT_READ_WAVEFRONTS[0-15] : Number of flat opcode reads processed by the TA.\n",
      "      block TA has 2 counters\n",
      "\n",
      "  gpu-agent1 : TA_FLAT_WRITE_WAVEFRONTS[0-15] : Number of flat opcode writes processed by the TA.\n",
      "      block TA has 2 counters\n",
      "\n",
      "  gpu-agent1 : TCC_HIT[0-15] : Number of cache hits.\n",
      "      block TCC has 4 counters\n",
      "\n",
      "  gpu-agent1 : TCC_MISS[0-15] : Number of cache misses. UC reads count as misses.\n",
      "      block TCC has 4 counters\n",
      "\n",
      "  gpu-agent1 : TCC_EA_WRREQ[0-15] : Number of transactions (either 32-byte or 64-byte) going over the TC_EA_wrreq interface. Atomics may travel over the same interface and are generally classified as write requests. This does not include probe commands.\n",
      "      block TCC has 4 counters\n",
      "\n",
      "  gpu-agent1 : TCC_EA_WRREQ_64B[0-15] : Number of 64-byte transactions going (64-byte write or CMPSWAP) over the TC_EA_wrreq interface.\n",
      "      block TCC has 4 counters\n",
      "\n",
      "  gpu-agent1 : TCC_EA_WRREQ_STALL[0-15] : Number of cycles a write request was stalled.\n",
      "      block TCC has 4 counters\n",
      "\n",
      "  gpu-agent1 : TCC_EA_RDREQ[0-15] : Number of TCC/EA read requests (either 32-byte or 64-byte)\n",
      "      block TCC has 4 counters\n",
      "\n",
      "  gpu-agent1 : TCC_EA_RDREQ_32B[0-15] : Number of 32-byte TCC/EA read requests\n",
      "      block TCC has 4 counters\n",
      "\n",
      "  gpu-agent1 : TCP_TCP_TA_DATA_STALL_CYCLES[0-15] : TCP stalls TA data interface. Now Windowed.\n",
      "      block TCP has 4 counters\n",
      "\n",
      "ROCPRofiler: 0 contexts collected\n"
     ]
    }
   ],
   "source": [
    "!run rocprof --list-basic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1b5b5f-3187-44b3-bc9a-a3b4e12f61db",
   "metadata": {},
   "source": [
    "#### Derived performance counters\n",
    "\n",
    "Using this command we can show all of the available derived performance counters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddfcd8c9-a167-44ee-b562-3e3fbb809113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RPL: on '240517_162604' from '/opt/rocm-6.0.2' in '/nethome/tpotter/Pelagos/Projects/HIP_Course/course_material/L5_Profiling'\n",
      "Derived metrics:\n",
      "\n",
      "  gpu-agent1 : TCC_EA1_RDREQ_32B_sum : Number of 32-byte TCC/EA read requests. Sum over TCC EA1s.\n",
      "      TCC_EA1_RDREQ_32B_sum = sum(TCC_EA1_RDREQ_32B,16)\n",
      "\n",
      "  gpu-agent1 : TCC_EA1_RDREQ_sum : Number of TCC/EA read requests (either 32-byte or 64-byte). Sum over TCC EA1s.\n",
      "      TCC_EA1_RDREQ_sum = sum(TCC_EA1_RDREQ,16)\n",
      "\n",
      "  gpu-agent1 : TCC_EA1_WRREQ_sum : Number of transactions (either 32-byte or 64-byte) going over the TC_EA_wrreq interface. Sum over TCC EA1s.\n",
      "      TCC_EA1_WRREQ_sum = sum(TCC_EA1_WRREQ,16)\n",
      "\n",
      "  gpu-agent1 : TCC_EA1_WRREQ_64B_sum : Number of 64-byte transactions going (64-byte write or CMPSWAP) over the TC_EA_wrreq interface. Sum over TCC EA1s.\n",
      "      TCC_EA1_WRREQ_64B_sum = sum(TCC_EA1_WRREQ_64B,16)\n",
      "\n",
      "  gpu-agent1 : TCC_WRREQ1_STALL_max : Number of cycles a write request was stalled. Max over TCC instances.\n",
      "      TCC_WRREQ1_STALL_max = max(TCC_EA1_WRREQ_STALL,16)\n",
      "\n",
      "  gpu-agent1 : RDATA1_SIZE : The total kilobytes fetched from the video memory. This is measured on EA1s.\n",
      "      RDATA1_SIZE = (TCC_EA1_RDREQ_32B_sum*32+(TCC_EA1_RDREQ_sum-TCC_EA1_RDREQ_32B_sum)*64)\n",
      "\n",
      "  gpu-agent1 : WDATA1_SIZE : The total kilobytes written to the video memory. This is measured on EA1s.\n",
      "      WDATA1_SIZE = ((TCC_EA1_WRREQ_sum-TCC_EA1_WRREQ_64B_sum)*32+TCC_EA1_WRREQ_64B_sum*64)\n",
      "\n",
      "  gpu-agent1 : FETCH_SIZE : The total kilobytes fetched from the video memory. This is measured with all extra fetches and any cache or memory effects taken into account.\n",
      "      FETCH_SIZE = (TCC_EA_RDREQ_32B_sum*32+(TCC_EA_RDREQ_sum-TCC_EA_RDREQ_32B_sum)*64+RDATA1_SIZE)/1024\n",
      "\n",
      "  gpu-agent1 : WRITE_SIZE : The total kilobytes written to the video memory. This is measured with all extra fetches and any cache or memory effects taken into account.\n",
      "      WRITE_SIZE = ((TCC_EA_WRREQ_sum-TCC_EA_WRREQ_64B_sum)*32+TCC_EA_WRREQ_64B_sum*64+WDATA1_SIZE)/1024\n",
      "\n",
      "  gpu-agent1 : WRITE_REQ_32B : The total number of 32-byte effective memory writes.\n",
      "      WRITE_REQ_32B = (TCC_EA_WRREQ_sum-TCC_EA_WRREQ_64B_sum)+(TCC_EA1_WRREQ_sum-TCC_EA1_WRREQ_64B_sum)+(TCC_EA_WRREQ_64B_sum+TCC_EA1_WRREQ_64B_sum)*2\n",
      "\n",
      "  gpu-agent1 : TA_BUSY_avr : TA block is busy. Average over TA instances.\n",
      "      TA_BUSY_avr = avr(TA_TA_BUSY,16)\n",
      "\n",
      "  gpu-agent1 : TA_BUSY_max : TA block is busy. Max over TA instances.\n",
      "      TA_BUSY_max = max(TA_TA_BUSY,16)\n",
      "\n",
      "  gpu-agent1 : TA_BUSY_min : TA block is busy. Min over TA instances.\n",
      "      TA_BUSY_min = min(TA_TA_BUSY,16)\n",
      "\n",
      "  gpu-agent1 : TA_FLAT_READ_WAVEFRONTS_sum : Number of flat opcode reads processed by the TA. Sum over TA instances.\n",
      "      TA_FLAT_READ_WAVEFRONTS_sum = sum(TA_FLAT_READ_WAVEFRONTS,16)\n",
      "\n",
      "  gpu-agent1 : TA_FLAT_WRITE_WAVEFRONTS_sum : Number of flat opcode writes processed by the TA. Sum over TA instances.\n",
      "      TA_FLAT_WRITE_WAVEFRONTS_sum = sum(TA_FLAT_WRITE_WAVEFRONTS,16)\n",
      "\n",
      "  gpu-agent1 : TCC_HIT_sum : Number of cache hits. Sum over TCC instances.\n",
      "      TCC_HIT_sum = sum(TCC_HIT,16)\n",
      "\n",
      "  gpu-agent1 : TCC_MISS_sum : Number of cache misses. Sum over TCC instances.\n",
      "      TCC_MISS_sum = sum(TCC_MISS,16)\n",
      "\n",
      "  gpu-agent1 : TCC_EA_RDREQ_32B_sum : Number of 32-byte TCC/EA read requests. Sum over TCC instances.\n",
      "      TCC_EA_RDREQ_32B_sum = sum(TCC_EA_RDREQ_32B,16)\n",
      "\n",
      "  gpu-agent1 : TCC_EA_RDREQ_sum : Number of TCC/EA read requests (either 32-byte or 64-byte). Sum over TCC instances.\n",
      "      TCC_EA_RDREQ_sum = sum(TCC_EA_RDREQ,16)\n",
      "\n",
      "  gpu-agent1 : TCC_EA_WRREQ_sum : Number of transactions (either 32-byte or 64-byte) going over the TC_EA_wrreq interface. Sum over TCC instances.\n",
      "      TCC_EA_WRREQ_sum = sum(TCC_EA_WRREQ,16)\n",
      "\n",
      "  gpu-agent1 : TCC_EA_WRREQ_64B_sum : Number of 64-byte transactions going (64-byte write or CMPSWAP) over the TC_EA_wrreq interface. Sum over TCC instances.\n",
      "      TCC_EA_WRREQ_64B_sum = sum(TCC_EA_WRREQ_64B,16)\n",
      "\n",
      "  gpu-agent1 : TCC_WRREQ_STALL_max : Number of cycles a write request was stalled. Max over TCC instances.\n",
      "      TCC_WRREQ_STALL_max = max(TCC_EA_WRREQ_STALL,16)\n",
      "\n",
      "  gpu-agent1 : TCP_TCP_TA_DATA_STALL_CYCLES_sum : Total number of TCP stalls TA data interface.\n",
      "      TCP_TCP_TA_DATA_STALL_CYCLES_sum = sum(TCP_TCP_TA_DATA_STALL_CYCLES,16)\n",
      "\n",
      "  gpu-agent1 : TCP_TCP_TA_DATA_STALL_CYCLES_max : Maximum number of TCP stalls TA data interface.\n",
      "      TCP_TCP_TA_DATA_STALL_CYCLES_max = max(TCP_TCP_TA_DATA_STALL_CYCLES,16)\n",
      "\n",
      "  gpu-agent1 : VFetchInsts : The average number of vector fetch instructions from the video memory executed per work-item (affected by flow control). Excludes FLAT instructions that fetch from video memory.\n",
      "      VFetchInsts = (SQ_INSTS_VMEM_RD-TA_FLAT_READ_WAVEFRONTS_sum)/SQ_WAVES\n",
      "\n",
      "  gpu-agent1 : VWriteInsts : The average number of vector write instructions to the video memory executed per work-item (affected by flow control). Excludes FLAT instructions that write to video memory.\n",
      "      VWriteInsts = (SQ_INSTS_VMEM_WR-TA_FLAT_WRITE_WAVEFRONTS_sum)/SQ_WAVES\n",
      "\n",
      "  gpu-agent1 : FlatVMemInsts : The average number of FLAT instructions that read from or write to the video memory executed per work item (affected by flow control). Includes FLAT instructions that read from or write to scratch.\n",
      "      FlatVMemInsts = (SQ_INSTS_FLAT-SQ_INSTS_FLAT_LDS_ONLY)/SQ_WAVES\n",
      "\n",
      "  gpu-agent1 : LDSInsts : The average number of LDS read or LDS write instructions executed per work item (affected by flow control).  Excludes FLAT instructions that read from or write to LDS.\n",
      "      LDSInsts = (SQ_INSTS_LDS-SQ_INSTS_FLAT_LDS_ONLY)/SQ_WAVES\n",
      "\n",
      "  gpu-agent1 : FlatLDSInsts : The average number of FLAT instructions that read or write to LDS executed per work item (affected by flow control).\n",
      "      FlatLDSInsts = SQ_INSTS_FLAT_LDS_ONLY/SQ_WAVES\n",
      "\n",
      "  gpu-agent1 : VALUUtilization : The percentage of active vector ALU threads in a wave. A lower number can mean either more thread divergence in a wave or that the work-group size is not a multiple of 64. Value range: 0% (bad), 100% (ideal - no thread divergence).\n",
      "      VALUUtilization = 100*SQ_THREAD_CYCLES_VALU/(SQ_ACTIVE_INST_VALU*MAX_WAVE_SIZE)\n",
      "\n",
      "  gpu-agent1 : VALUBusy : The percentage of GPUTime vector ALU instructions are processed. Value range: 0% (bad) to 100% (optimal).\n",
      "      VALUBusy = 100*SQ_ACTIVE_INST_VALU*4/SIMD_NUM/GRBM_GUI_ACTIVE\n",
      "\n",
      "  gpu-agent1 : SALUBusy : The percentage of GPUTime scalar ALU instructions are processed. Value range: 0% (bad) to 100% (optimal).\n",
      "      SALUBusy = 100*SQ_INST_CYCLES_SALU*4/SIMD_NUM/GRBM_GUI_ACTIVE\n",
      "\n",
      "  gpu-agent1 : FetchSize : The total kilobytes fetched from the video memory. This is measured with all extra fetches and any cache or memory effects taken into account.\n",
      "      FetchSize = FETCH_SIZE\n",
      "\n",
      "  gpu-agent1 : WriteSize : The total kilobytes written to the video memory. This is measured with all extra fetches and any cache or memory effects taken into account.\n",
      "      WriteSize = WRITE_SIZE\n",
      "\n",
      "  gpu-agent1 : MemWrites32B : The total number of effective 32B write transactions to the memory\n",
      "      MemWrites32B = WRITE_REQ_32B\n",
      "\n",
      "  gpu-agent1 : L2CacheHit : The percentage of fetch, write, atomic, and other instructions that hit the data in L2 cache. Value range: 0% (no hit) to 100% (optimal).\n",
      "      L2CacheHit = 100*sum(TCC_HIT,16)/(sum(TCC_HIT,16)+sum(TCC_MISS,16))\n",
      "\n",
      "  gpu-agent1 : MemUnitStalled : The percentage of GPUTime the memory unit is stalled. Try reducing the number or size of fetches and writes if possible. Value range: 0% (optimal) to 100% (bad).\n",
      "      MemUnitStalled = 100*max(TCP_TCP_TA_DATA_STALL_CYCLES,16)/GRBM_GUI_ACTIVE/SE_NUM\n",
      "\n",
      "  gpu-agent1 : WriteUnitStalled : The percentage of GPUTime the Write unit is stalled. Value range: 0% to 100% (bad).\n",
      "      WriteUnitStalled = 100*TCC_WRREQ_STALL_max/GRBM_GUI_ACTIVE\n",
      "\n",
      "  gpu-agent1 : LDSBankConflict : The percentage of GPUTime LDS is stalled by bank conflicts. Value range: 0% (optimal) to 100% (bad).\n",
      "      LDSBankConflict = 100*SQ_LDS_BANK_CONFLICT/GRBM_GUI_ACTIVE/CU_NUM\n",
      "\n",
      "  gpu-agent1 : GPUBusy : The percentage of time GPU was busy.\n",
      "      GPUBusy = 100*GRBM_GUI_ACTIVE/GRBM_COUNT\n",
      "\n",
      "  gpu-agent1 : Wavefronts : Total wavefronts.\n",
      "      Wavefronts = SQ_WAVES\n",
      "\n",
      "  gpu-agent1 : VALUInsts : The average number of vector ALU instructions executed per work-item (affected by flow control).\n",
      "      VALUInsts = SQ_INSTS_VALU/SQ_WAVES\n",
      "\n",
      "  gpu-agent1 : SALUInsts : The average number of scalar ALU instructions executed per work-item (affected by flow control).\n",
      "      SALUInsts = SQ_INSTS_SALU/SQ_WAVES\n",
      "\n",
      "  gpu-agent1 : SFetchInsts : The average number of scalar fetch instructions from the video memory executed per work-item (affected by flow control).\n",
      "      SFetchInsts = SQ_INSTS_SMEM/SQ_WAVES\n",
      "\n",
      "  gpu-agent1 : GDSInsts : The average number of GDS read or GDS write instructions executed per work item (affected by flow control).\n",
      "      GDSInsts = SQ_INSTS_GDS/SQ_WAVES\n",
      "\n",
      "  gpu-agent1 : MemUnitBusy : The percentage of GPUTime the memory unit is active. The result includes the stall time (MemUnitStalled). This is measured with all extra fetches and writes and any cache or memory effects taken into account. Value range: 0% to 100% (fetch-bound).\n",
      "      MemUnitBusy = 100*max(TA_TA_BUSY,16)/GRBM_GUI_ACTIVE/SE_NUM\n",
      "\n",
      "  gpu-agent1 : ALUStalledByLDS : The percentage of GPUTime ALU units are stalled by the LDS input queue being full or the output queue being not ready. If there are LDS bank conflicts, reduce them. Otherwise, try reducing the number of LDS accesses if possible. Value range: 0% (optimal) to 100% (bad).\n",
      "      ALUStalledByLDS = 100*SQ_WAIT_INST_LDS*4/SQ_WAVES/GRBM_GUI_ACTIVE\n",
      "\n",
      "ROCPRofiler: 0 contexts collected\n"
     ]
    }
   ],
   "source": [
    "!run rocprof --list-derived"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce2b350-4a1c-4408-b807-9b79e5f911c6",
   "metadata": {},
   "source": [
    "We can specify the counters to collect in a file such as [rocprof_counters.txt](rocprof_counters.txt). Here we specify some commonly used metrics for collection. Each **pmc** line is a unique experiment involving an individual run of the code. In this example we collect stats for the **mat_mult** kernel for the first 64 work-items on GPU 0.\n",
    "\n",
    "```txt\n",
    "# Cache hits and Cache misses\n",
    "pmc: TCC_HIT_sum, TCC_MISS_sum\n",
    "\n",
    "# Total video memory fetched and written\n",
    "pmc: FETCH_SIZE, WRITE_SIZE\n",
    "\n",
    "# Percentage of time the GPU was busy, total wavefronts executed\n",
    "pmc: GPUBusy, Wavefronts\n",
    "\n",
    "# Average number of vector and scalar instructions executed per work-item\n",
    "pmc: VALUInsts, SALUInsts\n",
    "\n",
    "# Average number of vector and scalar fetch instructions per work-item\n",
    "pmc: VFetchInsts, SFetchInsts\n",
    "\n",
    "# Average number of vector write instructions per work-item\n",
    "pmc: VWriteInsts\n",
    "\n",
    "# Average number of shared and global memory read or write instructions per work item\n",
    "pmc: LDSInsts, GDSInsts\n",
    "\n",
    "# Percentage of active vector ALU threads in a wave, percentage of GPU time vector and scalar instructions are processed\n",
    "pmc: VALUUtilization, VALUBusy, SALUBusy, \n",
    "\n",
    "# Percentage of fetch, write, atomic, and other instructions that hit the L2 cache\n",
    "pmc: L2CacheHit\n",
    "\n",
    "# Percentage of time the memory unit is active (including stalled), and just stalled, percentage of time the write unit is stalled\n",
    "pmc: MemUnitBusy, MemUnitStalled, WriteUnitStalled\n",
    "\n",
    "# Percentage of time ALU's are stalled by shared memory access, percentage of GPU time local memory is stalled by bank conflicts\n",
    "pmc: ALUStalledByLDS, LDSBankConflict\n",
    "\n",
    "# Dispatches range, which work-items to profile\n",
    "range: 0 : 64\n",
    "# Which GPU's to profile\n",
    "gpu: 0\n",
    "# Names of kernels to profile\n",
    "kernel: mat_mult\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee575685-ab1f-40bf-aa3f-50224668d1aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "Then we can use rocprof to collect the data for these counters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e6071ab-48a0-4529-bc46-d7ea4d554965",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RPL: on '240517_162606' from '/opt/rocm-6.0.2' in '/nethome/tpotter/Pelagos/Projects/HIP_Course/course_material/L5_Profiling'\n",
      "RPL: profiling '\"mat_mult_profiling.exe\"'\n",
      "RPL: input file 'rocprof_counters.txt'\n",
      "RPL: output dir '/tmp/rpl_data_240517_162606_3415'\n",
      "RPL: result dir '/tmp/rpl_data_240517_162606_3415/input0_results_240517_162606'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_240517_162606_3415/input0.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  2 metrics\n",
      "    TCC_HIT_sum, TCC_MISS_sum\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon VII\n",
      "\tglobal memory size:                      17163 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2560 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65536,65536)\n",
      "Time for event \"memcpy\": 0.906 ms (1754.30 MB/s)\n",
      "Time for event \"mat_mult kernel\": 10.319 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 1 contexts collected, output directory /tmp/rpl_data_240517_162606_3415/input0_results_240517_162606\n",
      "RPL: result dir '/tmp/rpl_data_240517_162606_3415/input10_results_240517_162606'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_240517_162606_3415/input10.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  3 metrics\n",
      "    MemUnitBusy, MemUnitStalled, WriteUnitStalled\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon VII\n",
      "\tglobal memory size:                      17163 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2560 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65536,65536)\n",
      "Time for event \"memcpy\": 0.890 ms (1785.52 MB/s)\n",
      "Time for event \"mat_mult kernel\": 10.394 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 1 contexts collected, output directory /tmp/rpl_data_240517_162606_3415/input10_results_240517_162606\n",
      "RPL: result dir '/tmp/rpl_data_240517_162606_3415/input11_results_240517_162606'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_240517_162606_3415/input11.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  2 metrics\n",
      "    ALUStalledByLDS, LDSBankConflict\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon VII\n",
      "\tglobal memory size:                      17163 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2560 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65536,65536)\n",
      "Time for event \"memcpy\": 0.909 ms (1747.81 MB/s)\n",
      "Time for event \"mat_mult kernel\": 10.348 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 1 contexts collected, output directory /tmp/rpl_data_240517_162606_3415/input11_results_240517_162606\n",
      "RPL: result dir '/tmp/rpl_data_240517_162606_3415/input1_results_240517_162606'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_240517_162606_3415/input1.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  1 metrics\n",
      "    FETCH_SIZE\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon VII\n",
      "\tglobal memory size:                      17163 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2560 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65536,65536)\n",
      "Time for event \"memcpy\": 0.914 ms (1739.24 MB/s)\n",
      "Time for event \"mat_mult kernel\": 10.494 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 1 contexts collected, output directory /tmp/rpl_data_240517_162606_3415/input1_results_240517_162606\n",
      "RPL: result dir '/tmp/rpl_data_240517_162606_3415/input2_results_240517_162606'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_240517_162606_3415/input2.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  1 metrics\n",
      "    WRITE_SIZE\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon VII\n",
      "\tglobal memory size:                      17163 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2560 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65536,65536)\n",
      "Time for event \"memcpy\": 0.906 ms (1753.37 MB/s)\n",
      "Time for event \"mat_mult kernel\": 10.396 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 1 contexts collected, output directory /tmp/rpl_data_240517_162606_3415/input2_results_240517_162606\n",
      "RPL: result dir '/tmp/rpl_data_240517_162606_3415/input3_results_240517_162606'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_240517_162606_3415/input3.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  2 metrics\n",
      "    GPUBusy, Wavefronts\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon VII\n",
      "\tglobal memory size:                      17163 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2560 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65536,65536)\n",
      "Time for event \"memcpy\": 0.918 ms (1731.67 MB/s)\n",
      "Time for event \"mat_mult kernel\": 10.645 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 1 contexts collected, output directory /tmp/rpl_data_240517_162606_3415/input3_results_240517_162606\n",
      "RPL: result dir '/tmp/rpl_data_240517_162606_3415/input4_results_240517_162606'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_240517_162606_3415/input4.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  2 metrics\n",
      "    VALUInsts, SALUInsts\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon VII\n",
      "\tglobal memory size:                      17163 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2560 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65536,65536)\n",
      "Time for event \"memcpy\": 0.901 ms (1763.95 MB/s)\n",
      "Time for event \"mat_mult kernel\": 10.724 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 1 contexts collected, output directory /tmp/rpl_data_240517_162606_3415/input4_results_240517_162606\n",
      "RPL: result dir '/tmp/rpl_data_240517_162606_3415/input5_results_240517_162606'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_240517_162606_3415/input5.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  2 metrics\n",
      "    VFetchInsts, SFetchInsts\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon VII\n",
      "\tglobal memory size:                      17163 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2560 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65536,65536)\n",
      "Time for event \"memcpy\": 0.899 ms (1767.09 MB/s)\n",
      "Time for event \"mat_mult kernel\": 10.340 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 1 contexts collected, output directory /tmp/rpl_data_240517_162606_3415/input5_results_240517_162606\n",
      "RPL: result dir '/tmp/rpl_data_240517_162606_3415/input6_results_240517_162606'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_240517_162606_3415/input6.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  1 metrics\n",
      "    VWriteInsts\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon VII\n",
      "\tglobal memory size:                      17163 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2560 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65536,65536)\n",
      "Time for event \"memcpy\": 0.917 ms (1733.17 MB/s)\n",
      "Time for event \"mat_mult kernel\": 10.614 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 1 contexts collected, output directory /tmp/rpl_data_240517_162606_3415/input6_results_240517_162606\n",
      "RPL: result dir '/tmp/rpl_data_240517_162606_3415/input7_results_240517_162606'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_240517_162606_3415/input7.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  2 metrics\n",
      "    LDSInsts, GDSInsts\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon VII\n",
      "\tglobal memory size:                      17163 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2560 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65536,65536)\n",
      "Time for event \"memcpy\": 0.889 ms (1787.12 MB/s)\n",
      "Time for event \"mat_mult kernel\": 10.781 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 1 contexts collected, output directory /tmp/rpl_data_240517_162606_3415/input7_results_240517_162606\n",
      "RPL: result dir '/tmp/rpl_data_240517_162606_3415/input8_results_240517_162606'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_240517_162606_3415/input8.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  3 metrics\n",
      "    VALUUtilization, VALUBusy, SALUBusy\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon VII\n",
      "\tglobal memory size:                      17163 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2560 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65536,65536)\n",
      "Time for event \"memcpy\": 0.899 ms (1768.35 MB/s)\n",
      "Time for event \"mat_mult kernel\": 10.307 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 1 contexts collected, output directory /tmp/rpl_data_240517_162606_3415/input8_results_240517_162606\n",
      "RPL: result dir '/tmp/rpl_data_240517_162606_3415/input9_results_240517_162606'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_240517_162606_3415/input9.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  1 metrics\n",
      "    L2CacheHit\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon VII\n",
      "\tglobal memory size:                      17163 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2560 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65536,65536)\n",
      "Time for event \"memcpy\": 0.899 ms (1767.72 MB/s)\n",
      "Time for event \"mat_mult kernel\": 10.508 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 1 contexts collected, output directory /tmp/rpl_data_240517_162606_3415/input9_results_240517_162606\n",
      "File 'rocprof_counters/result.csv' is generating\n"
     ]
    }
   ],
   "source": [
    "!run rocprof -i rocprof_counters.txt -o rocprof_counters/result.csv mat_mult_profiling.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7440b698-587e-42ec-a944-382dc9d2c481",
   "metadata": {
    "tags": []
   },
   "source": [
    "If your chosen performance counters are supported, then the file [rocprof_counters/result.csv](rocprof_counters/result.csv) should contain a count for every time the counter was triggered. The file [rocprof_counters/example.csv](rocprof_counters/example.csv) is an example file collected with rocprof on **mat_mult_profiling.exe**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3090705-8e92-4834-b46b-af432243382c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Rocprof under a job manager\n",
    "\n",
    "Rocprof runs fine under a job manager like SLURM, you just need to make a unique output file for each process launched. For example on SLURM the `$SLURM_JOBID` and `$SLURM_PROCID` environment variables are helpful in constructing a unique output. We put the rocprof commands in a script called **profile.sh**.\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "rocprof -i rocprof_counters.txt -o rocprof_counters/result-$SLURM_JOBID-$SLURM_PROCID.csv mat_mult_profiling_mpi.exe\n",
    "```\n",
    "\n",
    "Then you can run the script from **srun** like this so it picks up the environment variable **$SLURM_PROCID** from within the script.\n",
    "\n",
    "```bash\n",
    "srun -N $SLURM_JOB_NUM_NODES -n 2 -c 8 --gpus-per-task=1 --gpu-bind=closest -c $OMP_NUM_THREADS ./profile.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadc4531-2f2a-4437-a5d5-7ef16e670537",
   "metadata": {},
   "source": [
    "A complete example for using rocprof with an MPI-enabled application is in **course_material/L2_Using_HIP_On_Setonix/rocprof_mpi**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c822fc0-0f3b-4f05-b259-8bc690a7d250",
   "metadata": {},
   "source": [
    "### Rocprofiler API\n",
    "\n",
    "If you'd like to instrument code with profiling calls the **[rocprofiler API](https://github.com/ROCm-Developer-Tools/rocprofiler/blob/amd-master/doc/rocprofiler_spec.md)** is available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a97cbb-bf34-4053-b3bd-6c5e20ef9e56",
   "metadata": {},
   "source": [
    "### Tracing with Omnitrace\n",
    "\n",
    "[Omnitrace](https://github.com/AMDResearch/omnitrace) is an AMD research project to collect performance information on a program at runtime. It supports programs written in C, C++, Fortran and Python, as well as compute frameworks like OpenCL (via HSA) and HIP. Omnitrace looks promising as a comprehensive trace collection framework. In order to get Omnitrace functionality you need to load the modules for it, (you will find these commands in either the welcome letter or in Lesson 2). There is also a Pawsey-sponsored [video](https://youtu.be/xwNzreM0oqk) with a tutorial on Omnitrace.\n",
    "\n",
    "```bash\n",
    "course_material/L5_Profiling\n",
    "source ../env\n",
    "```\n",
    "\n",
    "#### Generate an Omnitrace configuration file\n",
    "\n",
    "A config file can govern the behaviour of Omnitrace. Normally this file lives in ${HOME}/omnitrace.cfg. We generate one for the purpose of the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b502bb12-a8f0-4b06-90f2-26a5e160ce13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m[omnitrace][avail] Found 1 HIP devices and 320 GPU HW counters\n",
      "\u001b[0m[omnitrace-avail] Outputting text configuration file './omnitrace.cfg'...\n"
     ]
    }
   ],
   "source": [
    "!run omnitrace-avail -G omnitrace.cfg --available --description --force"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5077f265-30b5-48d0-8991-d93d83704645",
   "metadata": {},
   "source": [
    "There is a multitude of options that are available in the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61c42f60-260e-4d98-b011-51fa2bcc6636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array_A.dat\t\tomnitrace_example\n",
      "array_B.dat\t\tomnitrace-mat_mult_profiling.inst-output\n",
      "array_C.dat\t\tomnitrace-mat_mult_profiling-output\n",
      "CMakeLists.txt\t\tProfiling.html\n",
      "jobscript.sh\t\tProfiling.ipynb\n",
      "Makefile\t\trocprof_counters\n",
      "mat_mult_profiling.cpp\trocprof_counters_reduced.txt\n",
      "mat_size.hpp\t\trocprof_counters.txt\n",
      "nsys_trace\t\trocprof_trace\n",
      "omniperf_example\trocprof_trace_example\n",
      "omnitrace.cfg\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e02b05f-7502-4ee0-b9e2-e5ee6f69ef43",
   "metadata": {},
   "source": [
    "#### Build the program to be profiled\n",
    "\n",
    "Now make sure the software is built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6753e33-47c1-4dd0-a1d3-28c4ae4ed8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 66%] Built target hip_helper\n",
      "[100%] Built target mat_mult_profiling.exe\n",
      "\u001b[36mInstall the project...\u001b[0m\n",
      "-- Install configuration: \"RELEASE\"\n"
     ]
    }
   ],
   "source": [
    "!build mat_mult_profiling.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901a0cd3-1ecd-4bea-9922-d1a34b4899a4",
   "metadata": {},
   "source": [
    "#### Run Omnitrace and instrument on the fly\n",
    "\n",
    "Then we can use Omnitrace to make a trace of **mat_mult_profiling.exe**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d57db5f6-5ced-4035-8545-03e910d802f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[omnitrace][exe] \n",
      "[omnitrace][exe] command :: '/nethome/tpotter/Pelagos/Projects/HIP_Course/course_material/install/bin/mat_mult_profiling.exe'...\n",
      "[omnitrace][exe] \n",
      "[omnitrace][exe] DYNINST_API_RT: /netsoft/omnitrace/rocm-6.0/lib/omnitrace/libdyninstAPI_RT.so\n",
      "[omnitrace][exe] [internal] parsing library: '/netsoft/gcc/13.2.0/lib64/libgcc_s.so.1'...\n",
      "[omnitrace][exe] [internal] parsing library: '/netsoft/gcc/13.2.0/lib64/libstdc++.so.6.0.32'...\n",
      "[omnitrace][exe] [internal] parsing library: '/netsoft/omnitrace/rocm-6.0/lib/libomnitrace-dl.so.1.11.2'...\n",
      "[omnitrace][exe] [internal] parsing library: '/netsoft/omnitrace/rocm-6.0/lib/libomnitrace-rt.so.11.0.1'...\n",
      "[omnitrace][exe] [internal] parsing library: '/netsoft/omnitrace/rocm-6.0/lib/libomnitrace-user.so.1.11.2'...\n",
      "[omnitrace][exe] [internal] parsing library: '/netsoft/omnitrace/rocm-6.0/lib/omnitrace/libcommon.so.11.0.1'...\n",
      "[omnitrace][exe] [internal] parsing library: '/netsoft/omnitrace/rocm-6.0/lib/omnitrace/libdw-0.182.so'...\n",
      "[omnitrace][exe] [internal] parsing library: '/netsoft/omnitrace/rocm-6.0/lib/omnitrace/libelf-0.182.so'...\n",
      "[omnitrace][exe] [internal] parsing library: '/netsoft/omnitrace/rocm-6.0/lib/omnitrace/libpfm.so.4.11.1'...\n",
      "[omnitrace][exe] [internal] parsing library: '/netsoft/omnitrace/rocm-6.0/lib/omnitrace/libtbb.so.2'...\n",
      "[omnitrace][exe] [internal] parsing library: '/netsoft/omnitrace/rocm-6.0/lib/omnitrace/libtbbmalloc.so.2'...\n",
      "[omnitrace][exe] [internal] parsing library: '/netsoft/omnitrace/rocm-6.0/lib/omnitrace/libtbbmalloc_proxy.so.2'...\n",
      "[omnitrace][exe] [internal] parsing library: '/netsoft/omnitrace/rocm-6.0/lib/omnitrace/libunwind.so.99.0.0'...\n",
      "[omnitrace][exe] [internal] parsing library: '/opt/rocm-6.0.2/lib/librocm_smi64.so.6.0.60002'...\n",
      "[omnitrace][exe] [internal] parsing library: '/opt/rocm-6.0.2/lib/librocprofiler64.so.1.0.60002'...\n",
      "[omnitrace][exe] [internal] parsing library: '/opt/rocm-6.0.2/lib/libroctracer64.so.4.1.60002'...\n",
      "[omnitrace][exe] [internal] parsing library: '/opt/rocm-6.0.2/lib/libroctx64.so.4.1.60002'...\n",
      "[omnitrace][exe] [internal] parsing library: '/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2'...\n",
      "[omnitrace][exe] [internal] parsing library: '/usr/lib/x86_64-linux-gnu/libBrokenLocale.so.1'...\n",
      "[omnitrace][exe] [internal] parsing library: '/usr/lib/x86_64-linux-gnu/libanl.so.1'...\n",
      "[omnitrace][exe] [internal] parsing library: '/usr/lib/x86_64-linux-gnu/libbz2.so.1.0.4'...\n",
      "[omnitrace][exe] [internal] parsing library: '/usr/lib/x86_64-linux-gnu/libc.so.6'...\n",
      "[omnitrace][exe] [internal] parsing library: '/usr/lib/x86_64-linux-gnu/libcrypt.so.1.1.0'...\n",
      "[omnitrace][exe] [internal] parsing library: '/usr/lib/x86_64-linux-gnu/libdl.so.2'...\n",
      "[omnitrace][exe] [internal] parsing library: '/usr/lib/x86_64-linux-gnu/liblzma.so.5.2.5'...\n",
      "[omnitrace][exe] [internal] parsing library: '/usr/lib/x86_64-linux-gnu/libnsl.so.1'...\n",
      "[omnitrace][exe] [internal] parsing library: '/usr/lib/x86_64-linux-gnu/libnss_compat.so.2'...\n",
      "[omnitrace][exe] [internal] parsing library: '/usr/lib/x86_64-linux-gnu/libnss_dns.so.2'...\n",
      "[omnitrace][exe] [internal] parsing library: '/usr/lib/x86_64-linux-gnu/libnss_files.so.2'...\n",
      "[omnitrace][exe] [internal] parsing library: '/usr/lib/x86_64-linux-gnu/libnss_hesiod.so.2'...\n",
      "[omnitrace][exe] [internal] parsing library: '/usr/lib/x86_64-linux-gnu/libnss_ldap-2.33.so'...\n",
      "[omnitrace][exe] [internal] parsing library: '/usr/lib/x86_64-linux-gnu/libpthread.so.0'...\n",
      "[omnitrace][exe] [internal] parsing library: '/usr/lib/x86_64-linux-gnu/libresolv.so.2'...\n",
      "[omnitrace][exe] [internal] parsing library: '/usr/lib/x86_64-linux-gnu/librt.so.1'...\n",
      "[omnitrace][exe] [internal] parsing library: '/usr/lib/x86_64-linux-gnu/libthread_db.so.1'...\n",
      "[omnitrace][exe] [internal] parsing library: '/usr/lib/x86_64-linux-gnu/libutil.so.1'...\n",
      "[omnitrace][exe] [internal] parsing library: '/usr/lib/x86_64-linux-gnu/libz.so.1.2.11'...\n",
      "[omnitrace][exe] [internal] binary info processing required 3.330 sec and 857.588 MB\n",
      "[omnitrace][exe] Processing 2679 modules...\n",
      "[omnitrace][exe] Processing 2679 modules... Done (8.594 sec, 88.704 MB)\n",
      "[omnitrace][exe] Finding instrumentation functions...\n",
      "\u001b[01;32m[omnitrace][exe] Outputting 'omnitrace-mat_mult_profiling-output/instrumentation/available.json'... Done\n",
      "\u001b[0m\u001b[01;32m[omnitrace][exe] Outputting 'omnitrace-mat_mult_profiling-output/instrumentation/available.txt'... Done\n",
      "\u001b[0m\u001b[01;32m[omnitrace][exe] Outputting 'omnitrace-mat_mult_profiling-output/instrumentation/instrumented.json'... Done\n",
      "\u001b[0m\u001b[01;32m[omnitrace][exe] Outputting 'omnitrace-mat_mult_profiling-output/instrumentation/instrumented.txt'... Done\n",
      "\u001b[0m\u001b[01;32m[omnitrace][exe] Outputting 'omnitrace-mat_mult_profiling-output/instrumentation/excluded.json'... Done\n",
      "\u001b[0m\u001b[01;32m[omnitrace][exe] Outputting 'omnitrace-mat_mult_profiling-output/instrumentation/excluded.txt'... Done\n",
      "\u001b[0m\u001b[01;32m[omnitrace][exe] Outputting 'omnitrace-mat_mult_profiling-output/instrumentation/overlapping.json'... Done\n",
      "\u001b[0m\u001b[01;32m[omnitrace][exe] Outputting 'omnitrace-mat_mult_profiling-output/instrumentation/overlapping.txt'... Done\n",
      "\u001b[0m[omnitrace][exe] Executing...\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[01;34m[omnitrace][4640][omnitrace_init_tooling] Instrumentation mode: Sampling\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[01;34m\n",
      "\n",
      "      ______   .___  ___. .__   __.  __  .___________..______          ___       ______  _______\n",
      "     /  __  \\  |   \\/   | |  \\ |  | |  | |           ||   _  \\        /   \\     /      ||   ____|\n",
      "    |  |  |  | |  \\  /  | |   \\|  | |  | `---|  |----`|  |_)  |      /  ^  \\   |  ,----'|  |__\n",
      "    |  |  |  | |  |\\/|  | |  . `  | |  |     |  |     |      /      /  /_\\  \\  |  |     |   __|\n",
      "    |  `--'  | |  |  |  | |  |\\   | |  |     |  |     |  |\\  \\----./  _____  \\ |  `----.|  |____\n",
      "     \\______/  |__|  |__| |__| \\__| |__|     |__|     | _| `._____/__/     \\__\\ \\______||_______|\n",
      "\n",
      "    omnitrace v1.11.2 (rev: 1df597e049b240fb263e7fcd7bddc78097d27f00, tag: v1.11.2, x86_64-linux-gnu, compiler: GNU v11.4.0, rocm: v6.0.x)\u001b[0m\n",
      "\u001b[90m[918.041]       perfetto.cc:58649\u001b[0m \u001b[39mConfigured tracing session 1, #sources:1, duration:0 ms, #buffers:1, total buffer size:1024000 KB, total sessions:1, uid:0 session name: \"\"\u001b[0m\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon VII\n",
      "\tglobal memory size:                      17163 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2560 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65536,65536)\n",
      "Time for event \"memcpy\": 0.972 ms (1634.49 MB/s)\n",
      "Time for event \"mat_mult kernel\": 0.854 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[01;34m[omnitrace][4640][0][omnitrace_finalize] finalizing...\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[01;34m[omnitrace][4640][0][omnitrace_finalize] \n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[01;34m[omnitrace][4640][0][omnitrace_finalize] omnitrace/process/4640 : 0.378906 sec wall_clock,  131.712 MB peak_rss,  131.244 MB page_rss, 0.290000 sec cpu_clock,   76.5 % cpu_util [laps: 1]\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[01;34m[omnitrace][4640][0][omnitrace_finalize] omnitrace/process/4640/thread/0 : 0.375548 sec wall_clock, 0.264539 sec thread_cpu_clock,   70.4 % thread_cpu_util,  131.328 MB peak_rss [laps: 1]\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[01;34m[omnitrace][4640][0][omnitrace_finalize] omnitrace/process/4640/thread/2 : 0.000006 sec wall_clock, 0.000005 sec thread_cpu_clock,   99.3 % thread_cpu_util,    0.000 MB peak_rss [laps: 1]\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[01;34m[omnitrace][4640][0][omnitrace_finalize] \n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[01;34m[omnitrace][4640][0][omnitrace_finalize] Finalizing perfetto...\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[01;32m[omnitrace][4640][perfetto]> Outputting '/nethome/tpotter/Pelagos/Projects/HIP_Course/course_material/L5_Profiling/omnitrace-mat_mult_profiling-output/2024-05-17_16.28/perfetto-trace-4640.proto' (58.31 KB / 0.06 MB / 0.00 GB)... \u001b[01;32mDone\u001b[0m\n",
      "\u001b[01;32m[omnitrace][4640][metadata]> Outputting 'omnitrace-mat_mult_profiling-output/2024-05-17_16.28/metadata-4640.json' and 'omnitrace-mat_mult_profiling-output/2024-05-17_16.28/functions-4640.json'\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[01;34m[omnitrace][4640][0][omnitrace_finalize] Finalized: 0.091067 sec wall_clock,    0.000 MB peak_rss,    0.655 MB page_rss, 0.030000 sec cpu_clock,   32.9 % cpu_util\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[90m[918.513]       perfetto.cc:60128\u001b[0m \u001b[39mTracing session 1 ended, total sessions:0\u001b[0m\n",
      "[omnitrace][exe] End of omnitrace\n"
     ]
    }
   ],
   "source": [
    "!run omnitrace-instrument --env OMNITRACE_CONFIG_FILE=$(pwd)/omnitrace.cfg -M sampling -- mat_mult_profiling.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968b74f7-590b-40e2-9173-2f59cf6d9bf9",
   "metadata": {},
   "source": [
    "#### Examine the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0a85185a-b6bb-43b8-a4c3-f229aee58fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysis.txt\t\t     omniperf_example\n",
      "array_A.dat\t\t     omnitrace.cfg\n",
      "array_B.dat\t\t     omnitrace_example\n",
      "array_C.dat\t\t     omnitrace-mat_mult_profiling.inst-output\n",
      "CMakeLists.txt\t\t     omnitrace-mat_mult_profiling-output\n",
      "jobscript.sh\t\t     Profiling.html\n",
      "Makefile\t\t     Profiling.ipynb\n",
      "mat_mult_profiling.cpp\t     rocprof_counters\n",
      "mat_mult_profiling.inst.exe  rocprof_counters_reduced.txt\n",
      "mat_size.hpp\t\t     rocprof_counters.txt\n",
      "ncu_counters\t\t     rocprof_trace\n",
      "nsys_trace\t\t     rocprof_trace_example\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1495d65f-fac6-41d5-a9df-858fa292824e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available.json\texcluded.json  instrumented.json  overlapping.json\n",
      "available.txt\texcluded.txt   instrumented.txt   overlapping.txt\n"
     ]
    }
   ],
   "source": [
    "!ls omnitrace-mat_mult_profiling-output/instrumentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1e403-4006-4862-850b-86dbcc417a50",
   "metadata": {},
   "source": [
    "#### Instrument prior to execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023e1d1f-c664-4327-ae7e-58b70eb6fc16",
   "metadata": {},
   "source": [
    "Or we can have omnitrace **instrument** the application for profiling. This is useful if we want to run an application with MPI support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "214c9835-1eb2-4bea-8e3e-1f88583c623d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[omnitrace][exe] Exit code: 0\n"
     ]
    }
   ],
   "source": [
    "!run omnitrace-instrument --env OMNITRACE_CONFIG_FILE=$(pwd)/omnitrace.cfg -M sampling -v -1 -o ./mat_mult_profiling.inst.exe -- mat_mult_profiling.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef6bbcd7-56c3-4022-bcd9-8f990a68e3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array_A.dat\t\t     omnitrace.cfg\n",
      "array_B.dat\t\t     omnitrace_example\n",
      "array_C.dat\t\t     omnitrace-mat_mult_profiling.inst-output\n",
      "CMakeLists.txt\t\t     omnitrace-mat_mult_profiling-output\n",
      "jobscript.sh\t\t     Profiling.html\n",
      "Makefile\t\t     Profiling.ipynb\n",
      "mat_mult_profiling.cpp\t     rocprof_counters\n",
      "mat_mult_profiling.inst.exe  rocprof_counters_reduced.txt\n",
      "mat_size.hpp\t\t     rocprof_counters.txt\n",
      "nsys_trace\t\t     rocprof_trace\n",
      "omniperf_example\t     rocprof_trace_example\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca9c866-b707-490a-96f7-3ea004ac9d7e",
   "metadata": {},
   "source": [
    "#### Running an instrumented code\n",
    "\n",
    "Once the code has been instrumented, then you use `omnitrace-run` to run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5baf238-f5f1-4550-a68d-0bb84dfda250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[01;32mOMNITRACE: HSA_TOOLS_LIB=/netsoft/omnitrace/rocm-6.0/lib/libomnitrace-dl.so.1.11.2\n",
      "\u001b[0m\u001b[01;32mOMNITRACE: HSA_TOOLS_REPORT_LOAD_FAILURE=1\n",
      "\u001b[0m\u001b[01;32mOMNITRACE: LD_PRELOAD=/netsoft/omnitrace/rocm-6.0/lib/libomnitrace-dl.so.1.11.2\n",
      "\u001b[0m\u001b[01;32mOMNITRACE: OMP_TOOL_LIBRARIES=/netsoft/omnitrace/rocm-6.0/lib/libomnitrace-dl.so.1.11.2\n",
      "\u001b[0m\u001b[01;32mOMNITRACE: ROCP_HSA_INTERCEPT=1\n",
      "\u001b[0m\u001b[01;32mOMNITRACE: ROCP_TOOL_LIB=/netsoft/omnitrace/rocm-6.0/lib/libomnitrace.so.1.11.2\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[01;34m[omnitrace][dl][4780] omnitrace_main\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[01;34m[omnitrace][4780][omnitrace_init_tooling] Instrumentation mode: Sampling\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[01;34m\n",
      "\n",
      "      ______   .___  ___. .__   __.  __  .___________..______          ___       ______  _______\n",
      "     /  __  \\  |   \\/   | |  \\ |  | |  | |           ||   _  \\        /   \\     /      ||   ____|\n",
      "    |  |  |  | |  \\  /  | |   \\|  | |  | `---|  |----`|  |_)  |      /  ^  \\   |  ,----'|  |__\n",
      "    |  |  |  | |  |\\/|  | |  . `  | |  |     |  |     |      /      /  /_\\  \\  |  |     |   __|\n",
      "    |  `--'  | |  |  |  | |  |\\   | |  |     |  |     |  |\\  \\----./  _____  \\ |  `----.|  |____\n",
      "     \\______/  |__|  |__| |__| \\__| |__|     |__|     | _| `._____/__/     \\__\\ \\______||_______|\n",
      "\n",
      "    omnitrace v1.11.2 (rev: 1df597e049b240fb263e7fcd7bddc78097d27f00, tag: v1.11.2, x86_64-linux-gnu, compiler: GNU v11.4.0, rocm: v6.0.x)\u001b[0m\n",
      "\u001b[90m[938.519]       perfetto.cc:58649\u001b[0m \u001b[39mConfigured tracing session 1, #sources:1, duration:0 ms, #buffers:1, total buffer size:1024000 KB, total sessions:1, uid:0 session name: \"\"\u001b[0m\n",
      "Device id: 0\n",
      "\tname:                                    AMD Radeon VII\n",
      "\tglobal memory size:                      17163 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                2560 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum shared memory size per SM or CU: 65 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65536,65536)\n",
      "Time for event \"memcpy\": 1.260 ms (1261.79 MB/s)\n",
      "Time for event \"mat_mult kernel\": 0.840 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[01;34m[omnitrace][4780][0][omnitrace_finalize] finalizing...\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[01;34m[omnitrace][4780][0][omnitrace_finalize] \n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[01;34m[omnitrace][4780][0][omnitrace_finalize] omnitrace/process/4780 : 0.369683 sec wall_clock,  131.488 MB peak_rss,  131.019 MB page_rss, 0.270000 sec cpu_clock,   73.0 % cpu_util [laps: 1]\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[01;34m[omnitrace][4780][0][omnitrace_finalize] omnitrace/process/4780/thread/0 : 0.366421 sec wall_clock, 0.257101 sec thread_cpu_clock,   70.2 % thread_cpu_util,  131.232 MB peak_rss [laps: 1]\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[01;34m[omnitrace][4780][0][omnitrace_finalize] omnitrace/process/4780/thread/2 : 0.000006 sec wall_clock, 0.000006 sec thread_cpu_clock,   99.8 % thread_cpu_util,    0.000 MB peak_rss [laps: 1]\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[01;34m[omnitrace][4780][0][omnitrace_finalize] \n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[01;34m[omnitrace][4780][0][omnitrace_finalize] Finalizing perfetto...\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[01;32m[omnitrace][4780][perfetto]> Outputting '/nethome/tpotter/Pelagos/Projects/HIP_Course/course_material/L5_Profiling/omnitrace-mat_mult_profiling.inst-output/2024-05-17_16.29/perfetto-trace-4780.proto' (56.60 KB / 0.06 MB / 0.00 GB)... \u001b[01;32mDone\u001b[0m\n",
      "\u001b[01;32m[omnitrace][4780][metadata]> Outputting 'omnitrace-mat_mult_profiling.inst-output/2024-05-17_16.29/metadata-4780.json' and 'omnitrace-mat_mult_profiling.inst-output/2024-05-17_16.29/functions-4780.json'\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[01;34m[omnitrace][4780][0][omnitrace_finalize] Finalized: 0.105019 sec wall_clock,    0.000 MB peak_rss,    0.655 MB page_rss, 0.050000 sec cpu_clock,   47.6 % cpu_util\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[90m[938.995]       perfetto.cc:60128\u001b[0m \u001b[39mTracing session 1 ended, total sessions:0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!run omnitrace-run -- ./mat_mult_profiling.inst.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bee8406-d101-4c4c-884e-a6a86cfc872f",
   "metadata": {},
   "source": [
    "If you look in the subfolders \n",
    "\n",
    "* **omnitrace-mat_mult_profiling-output**\n",
    "* **omnitrace-mat_mult_profiling.inst-output**, \n",
    "\n",
    "either in **course_material/L5_Profiling** or in the example folder **course_material/L5_Profiling/omnitrace_example** there are subfolders with dates on them. In those subfolders are `*.proto` files for use with perfetto. Download the **.proto** file to your computer and open it with [ui.perfetto.dev](https://ui.perfetto.dev) in a similar way to the json trace files from rocprof. You should see when and for how long functions are executed on the host and for how long kernels are executed on the device, along with a more detailed set of metrics such as CPU frequency and power consumption.\n",
    "\n",
    "<figure style=\"margin-left:0; margin-right:auto; width:100%;\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/omnitrace.png\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Examining the output from Omnitrace using <a href=\"https://ui.perfetto.dev\">ui.perfetto.dev</a></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a0a9d0-0406-47a1-bf01-b1eb087a7c75",
   "metadata": {},
   "source": [
    "### Performance measurement with Omniperf\n",
    "\n",
    "The AMD research tool Omniperf [Omniperf](https://github.com/AMDResearch/omniperf) is a powerful tool for measuring the performance of applications on AMD Instinct GPU's like the MI250X on Setonix. It can perform feats like [Roofline Analysis](https://en.wikipedia.org/wiki/Roofline_model). Load the Omniperf modules, using the module load commands from either the welcome letter or from Lesson 2. Then use Omniperf like this to make an analysis. Omniperf doesn't support my graphics card, so we will have to use a machine that has a supported card and run this command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fd27630-78ec-421c-becc-94a038cf4f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../install/bin/run: line 4: omniperf: command not found\n"
     ]
    }
   ],
   "source": [
    "!run omniperf profile -n mat_mult -- mat_mult_profiling.exe -o mat_mult.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732da7f1-ddba-434a-ae93-d945401af617",
   "metadata": {},
   "source": [
    "The resulting hardware collection information is in a directory called **workloads/mat_mult**. You can view the output in text format using the command\n",
    "\n",
    "```bash\n",
    "omniperf analyze -p workloads/mat_mult/mi200 &> analysis.txt\n",
    "```\n",
    "\n",
    "or, if you have Omniperf installed to your laptop you can see the results from your web browser. Download the **workloads** directory to your computer and run this command. \n",
    "\n",
    "```bash\n",
    "omniperf analyze -p workloads/mat_mult/mi200 --gui\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcdbb79-6c0e-4133-98be-d54c7631dfba",
   "metadata": {},
   "source": [
    "Then you should be able to go to the location [http://127.0.0.1:8050](http://127.0.0.1:8050) and view the profiling information collected. An example data collection is in **course_material/L5_Profiling/omniperf_example**.\n",
    "\n",
    "#### Roofline models with Omniperf\n",
    "\n",
    "The **[Arithmetic intensity](https://en.wikipedia.org/wiki/Roofline_model#Arithmetic_intensity)** of an algorithm is the ratio of floating point operations (FLOPS) computed per byte transferred. It helps us gauge if an algorithm is likely to be constrained by either the bandwidth or floating point performance of a compute resource. In matrix multiplication the input matrix **A** is of size ($N_{0,C}, N_{1,A}$) and **B** is of size ($N_{1,A}, N_{1,C}$). Every element of matrix **C** requires $N_{1,A}$ loads from A, $N_{1,A}$ loads from B, and 1 store to **C**. It also requires $N_{1,A}$ multiplications and $N_{1,A}$ additions. The arithmetic intensity of matrix multiplication is then\n",
    "\n",
    "$$ a = \\frac{2N1_A}{(2N1_A+1)b} $$\n",
    "\n",
    "where **b** is the number of bytes stored per element. When $N1_A$ is large the theoretical arithmetic intensity for matrix multiplication is\n",
    "\n",
    "$$ a \\approx \\frac{1}{b}. $$\n",
    "\n",
    "If a processor has a peak floating point performance of $\\textbf{F}_{P}$ FLOP/second, and a particular cache can feed that processor at a peak bandwidth of $\\textbf{B}_{P}$ bytes/second, then we can calculate a floating point limit that is dependent on memory bandwidth.\n",
    "\n",
    "$$F_{B} = a  \\frac{\\mbox{FLOP}}{\\mbox{byte}} B_{P}\\frac{\\mbox{byte}}{\\mbox{second}} = a B_{P} \\frac{\\mbox{FLOP}}{\\mbox{second}}$$ \n",
    "\n",
    "The actual attainable floating point performance will be either $F_{B}$ or $F_{P}$, whatever is lower. If we set $F_{B} = F_{P}$ then we can solve for the crossover point in arithmetic intensity.\n",
    "\n",
    "$$a_{0}=\\frac{F_{P}}{B_{P}}$$\n",
    "\n",
    "Therefore the limits (or roofline) on performance is as follows:  \n",
    "\n",
    "$$\n",
    "F = \\left \\{\n",
    "\\begin{array}{rl}\n",
    "aB_{P} & \\mbox{if} \\space a<\\frac{F_{P}}{B_{P}},\\\\\n",
    "F_{P}& \\mbox{otherwise}\n",
    "\\end{array}\n",
    "\\right .\n",
    "$$\n",
    "\n",
    "For example, a single compute device in a AMD Mi250x GPU processor has a peak 32-bit floating point processing rate of $F_{P} = 23.95$ TFLOPS and a peak memory bandwidth of $F_{B}=1.6$ TB/s from global memory. Problems will be constrained by memory bandwidth up to an arithmetic intensity of \n",
    "\n",
    "$$a_{0}=\\frac{23.95}{1.6} \\approx 15$$\n",
    "\n",
    "Shown below is a roofline plot of **mat_mult_profiling.cpp**, showing the various rooflines for the L1, L2, and global memory (HBM) caches. The crossover point for 32-bit floating point arithmetic and global memory is correctly situated around **a=15**. Performance in loading memory cache appears to be close to optimal at the theoretical arithmetic intensity of a=0.25, however significant gains in performance look possible if we can improve loads from main memory.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3038dfaa-3008-4326-bd52-cbf8e5fd9baa",
   "metadata": {},
   "source": [
    "<figure style=\"margin-left:0; margin-right:auto; width:70%;\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/roofline_plot.png\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Roofline model created from mat_mult_profiling.exe</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302f458b-1bd1-4fb9-8856-b2d94677ded9",
   "metadata": {},
   "source": [
    "If you just want **pdf** versions of the roofline models then run this command\n",
    "\n",
    "```bash\n",
    "omniperf profile -n mat_mult --roof-only -- $RUN_DIR/mat_mult_profiling.exe\n",
    "```\n",
    "\n",
    "The pdf files will be available in **workloads/mat_mult/mi200**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b2aaf2-ef12-44af-b96a-1e2a35a75082",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Performance measurement with NVIDIA tools\n",
    "\n",
    "HIP applications that use the CUDA backend (when CUDA is available and the environment variable `HIP_PLATFORM` is set to `nvidia`) have access to the NVIDIA performance measurement tools such as [NVIDIA Nsight Systems](https://developer.nvidia.com/nsight-systems) and [NVIDIA Nsight Compute](https://developer.nvidia.com/nsight-compute). Here we briefly cover how to use these tools.\n",
    "\n",
    "### Tracing with Nsight Systems\n",
    "\n",
    "The command line application **nsys** can collect traces on **mat_mult_profiling.exe**. Make sure you have re-compiled **mat_mult_profiling.exe** with the `HIP_PLATFORM` environment variable set to `nvidia`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8e3e699-6e16-431b-94f8-39c07100ac87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device id: 0\n",
      "\tname:                                    Tesla T4\n",
      "\tglobal memory size:                      15652 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                1024 \n",
      "\tmaximum shared memory size per block:    49 KB\n",
      "\tmaximum shared memory size per SM or CU: 0 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,64)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65535,65535)\n",
      "Time for event \"memcpy\": 0.297 ms (5356.34 MB/s)\n",
      "Time for event \"mat_mult kernel\": 1.196 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "Generating '/tmp/nsys-report-8ba2.qdstrm'\n",
      "[1/1] [========================100%] results.nsys-rep\n",
      "Generated:\n",
      "    /nethome/tpotter/HIP_Course/course_material/L5_Profiling/nsys_trace/results.nsys-rep\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p nsys_trace; run nsys profile -f true -o nsys_trace/results mat_mult_profiling.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451fd761-d14f-407b-afd4-319424719b78",
   "metadata": {},
   "source": [
    "Then you can use this command under Linux to view the application trace \n",
    "\n",
    "```bash\n",
    "nsys-ui nsys_trace/results.nsys-rep\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51171e76-80dc-4c03-a1b8-0fe557832350",
   "metadata": {},
   "source": [
    "It's important to note that when using the NVIDIA backend it is important to note that HIP is a **thin layer** over CUDA. NVIDIA performance tools will report usage for the underlying CUDA functions instead of the HIP labelled functions. For example, when using Nsight Systems it will report a call to **cudaDeviceSynchronize** instead of **hipDeviceSynchronize**. One has to make the mental mapping between HIP and CUDA API calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5b9bfd-94ac-4158-bb20-300b732adf93",
   "metadata": {},
   "source": [
    "### Hardware collection with Nsight compute\n",
    "\n",
    "Nsight compute has the ability to collect hardware performance counters, however this ability needs either administrator access or access granted to performance counters at the OS level. If this access is possible then the following command will collect hardware performance counters on **mat_mult_profiling.exe**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "220a1c36-8bb4-44ff-b508-1437fb28234f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==PROF== Connected to process 2088 (/nethome/tpotter/HIP_Course/course_material/install/bin/mat_mult_profiling.exe)\n",
      "Device id: 0\n",
      "\tname:                                    Tesla T4\n",
      "\tglobal memory size:                      15652 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmax threads per SM or CU:                1024 \n",
      "\tmaximum shared memory size per block:    49 KB\n",
      "\tmaximum shared memory size per SM or CU: 0 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,64)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65535,65535)\n",
      "Time for event \"memcpy\": 0.296 ms (5369.66 MB/s)\n",
      "==PROF== Profiling \"mat_mult\" - 0: 0%....50%....100% - 8 passes\n",
      "Time for event \"mat_mult kernel\": 1166.193 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "==PROF== Disconnected from process 2088\n",
      "==PROF== Report: /nethome/tpotter/HIP_Course/course_material/L5_Profiling/ncu_counters/results.ncu-rep\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ncu_counters; run ncu -f -o ncu_counters/results mat_mult_profiling.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cd0f55-b0c0-48d1-98bd-b7be9ef65991",
   "metadata": {},
   "source": [
    "Then you can run the command:\n",
    "\n",
    "```bash\n",
    "ncu-ui\n",
    "```\n",
    "\n",
    "On a local machine to view the hardware performance counter information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ded4596-2705-4315-800b-624f8e062d9c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This chapter covers how to measure performance in HIP applications. HIP events are tools within the HIP framework to measure the execution time of kernels or memory copies. External tools such as `rocprof` can trace applications to collect information on **when** and for **how long** compute resources are used, as well as collecting low level information from hardware performance counters. Higher level tools like **Omnitrace** and **Omniperf** collect additional information and make the information obtained through rocprof more accessible through GUI-based reporting tools. Since HIP is a cross-platform environment, we conclude the chapter by walking through some performance monitoring tools that NVIDIA backends can make use of."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c38c04c-7c8d-4646-a382-0b02fb91a42e",
   "metadata": {},
   "source": [
    "<address>\n",
    "Written by Dr. Toby Potter of <a href=\"https://www.pelagos-consulting.com\">Pelagos Consulting and Education</a> for the <a href=\"https://pawsey.org.au\">Pawsey Supercomputing Centre</a>.<br>\n",
    "</address>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
