{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da6d105-2df5-40a6-9baa-02497fcef0d8",
   "metadata": {},
   "source": [
    "# Measuring performance in HIP applications\n",
    "\n",
    "Having an understanding of how well HIP applications perform is a vital part of the development process. The two main tools, **profiling** and **tracing** collect information about how well an application is performing. **Profiling** is the statistical collection of the cumulative time that threads spend in each program component. **Tracing** is a collection of both **when** and **for how long** threads spend in each application component. Since HIP applications use either an AMD or a CUDA backend, the profiling tools from each platform are available for use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4bd9a5-220f-4ebc-99b5-fe753e6eca84",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Event based profiling\n",
    "\n",
    "Events in HIP are used to check the progress of work that has been submitted and establish dependencies between workflows. They can also be used to time the execution of work such as kernels and memory copies. The code [mat_mult_profiling.cpp](mat_mult_profiling.cpp) contains a complete example where events are used to time the execution of the host to device memory copy as well as the timing of the matrix multiplication kernel. The data type **HipEvent_t** stores event data. \n",
    "\n",
    "### Source code changes\n",
    "\n",
    "In [mat_mult_profiling.cpp](mat_mult_profiling.cpp) we use the function **hipEventCreate** to create two events **t1** and **t2** as follows:\n",
    "\n",
    "```C++\n",
    "    // Create events for the memory copies and kernel runs\n",
    "    hipEvent_t t1=0, t2=0;\n",
    "    // Create the events\n",
    "    H_ERRCHK(hipEventCreate(&t1));\n",
    "    H_ERRCHK(hipEventCreate(&t2));\n",
    "```\n",
    "\n",
    "Now we wish to use these events to time the upload of host matrices **A_h** and **B_h** to the compute device. The HIP function **hipEventRecord** inserts the event into the \"flow\" of a stream. We haven't talked in depth about HIP streams yet and at this stage we can think of them as a queue to which work is submitted. Since we are not using a particular stream we are using the default stream (denoted by 0). We insert t1 into the default stream, perform the memory copies, and insert t2 after the copies are complete.\n",
    "\n",
    "```C++\n",
    "    // Record the start event into the default stream\n",
    "    H_ERRCHK(hipEventRecord(t1,0));\n",
    "    \n",
    "    // Peform the memory copies\n",
    "    H_ERRCHK(hipMemcpy(A_d, A_h, nbytes_A, hipMemcpyHostToDevice));\n",
    "    H_ERRCHK(hipMemcpy(B_d, B_h, nbytes_B, hipMemcpyHostToDevice));\n",
    "    \n",
    "    // Record the stop event into the default stream\n",
    "    H_ERRCHK(hipEventRecord(t2,0));\n",
    "```\n",
    "\n",
    "The function **hipEventSynchronize** waits until events are complete. Then we can use the function **hipEventElapsedTime** to get the time elapsed between the two events. The helper function **h_get_event_time_ms** takes care of calling these functions, prints performance measurement information, and returns the number of milliseconds between the two events.\n",
    "\n",
    "```C++\n",
    "    // Total number of Bytes copied\n",
    "    size_t total_bytes = nbytes_A + nbytes_B;\n",
    "\n",
    "    // Get the elapsed time in milliseconds\n",
    "    float elapsed_ms = h_get_event_time_ms(t1, t2, \"memcpy\", &total_bytes);\n",
    "```\n",
    "\n",
    "The source code of **h_get_event_time_ms** is in <a href=\"../include/hip_helper.hpp\">hip_helper.hpp</a> and reproduced below:\n",
    "\n",
    "```C++\n",
    "// Get how much time elapsed between two events that were recorded\n",
    "float h_get_event_time_ms(\n",
    "        // Assumes start and stop events have been recorded\n",
    "        // with the hipEventRecord() function\n",
    "        hipEvent_t t1,\n",
    "        hipEvent_t t2,\n",
    "        const char* message, \n",
    "        size_t* nbytes) {\n",
    "    \n",
    "    // Make sure the stop and start events have finished\n",
    "    H_ERRCHK(hipEventSynchronize(t2));\n",
    "    H_ERRCHK(hipEventSynchronize(t1));\n",
    "\n",
    "    // Elapsed time in milliseconds\n",
    "    float elapsed_ms=0;\n",
    "\n",
    "    // Convert the time into milliseconds\n",
    "    H_ERRCHK(hipEventElapsedTime(&elapsed_ms, t1, t2));\n",
    "        \n",
    "    // Print the timing message if necessary\n",
    "    if ((message != NULL) && (strlen(message)>0)) {\n",
    "        std::printf(\"Time for event \\\"%s\\\": %.3f ms\", message, elapsed_ms);\n",
    "        \n",
    "        // Print transfer rate if nbytes is not NULL\n",
    "        if (nbytes != NULL) {\n",
    "            double io_rate_MBs = h_get_io_rate_MBs(\n",
    "                elapsed_ms, \n",
    "                *nbytes\n",
    "            );\n",
    "            std::printf(\" (%.2f MB/s)\", io_rate_MBs);\n",
    "        }\n",
    "        std::printf(\"\\n\");\n",
    "    }\n",
    "    \n",
    "    return elapsed_ms;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db80de96-12f4-474c-b8e7-fcb4e0d50a24",
   "metadata": {},
   "source": [
    "We can reuse the events to time the execution of the kernel. \n",
    "\n",
    "```C++\n",
    "    // Record the start event into the default stream\n",
    "    H_ERRCHK(hipEventRecord(t1,0));\n",
    "\n",
    "    // Launch the kernel using hipLaunchKernelGGL method\n",
    "    hipLaunchKernelGGL(mat_mult, \n",
    "            grid_nblocks, \n",
    "            block_size, sharedMemBytes, 0, \n",
    "            A_d, B_d, C_d,\n",
    "            N1_A,\n",
    "            N0_C,\n",
    "            N1_C\n",
    "    );\n",
    "\n",
    "    // Record the stop event into the default stream \n",
    "    H_ERRCHK(hipEventRecord(t2,0));\n",
    "\n",
    "    // Get the elapsed time in milliseconds\n",
    "    elapsed_ms = h_get_event_time_ms(t1, t2, \"mat_mult kernel\", NULL);\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bd3ff9-409d-4f1a-a692-24e512e58bcb",
   "metadata": {},
   "source": [
    "In this manner we instrument the uploads, downloads, and kernel execution in the source file [mat_mult_profiling.cpp](mat_mult_profiling.cpp). Now we run the instrumented code and view the timing results. Change directory to **L5_Profiling** and run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63a7ca7f-acee-40b7-9ec5-30bd9d0ba996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: 'mat_mult_profiling.exe' is up to date.\n",
      "Device id: 0\n",
      "\tname:                                    \n",
      "\tglobal memory size:                      536 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum pitch size for memory copies:    536 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,2147483647,2147483647)\n",
      "Time for event \"memcpy\": 0.506 ms (3141.97 MB/s)\n",
      "Time for event \"mat_mult kernel\": 2.876 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n"
     ]
    }
   ],
   "source": [
    "!make mat_mult_profiling.exe; ./mat_mult_profiling.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec2892-86fe-46df-9e43-8446ebb8f977",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Performance measurement with AMD tools\n",
    "\n",
    "The AMD profiler **ROCPROF** has the ability to collect traces and information from hardware performance counters.\n",
    "\n",
    "#### HIP application traces with rocprof\n",
    "\n",
    "An application trace is information on when functions execute and for how long they took to execute. Collecting HIP application traces with **rocprof** is accomplished with the **--hip-trace** flag. Tracing with **rocprof** only seems to work with the **AMD** backend at present. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a09e5d94-80c2-4f99-99c5-afc97c285458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RPL: on '230327_162637' from '/opt/rocm-5.4.1' in '/home/toby/Pelagos/Projects/HIP_Course/course_material/L5_Profiling'\n",
      "RPL: profiling '\"./mat_mult_profiling.exe\"'\n",
      "RPL: input file ''\n",
      "RPL: output dir '/tmp/rpl_data_230327_162637_23484'\n",
      "RPL: result dir '/tmp/rpl_data_230327_162637_23484/input_results_230327_162637'\n",
      "ROCtracer (23510):\n",
      "    HIP-trace(*)\n",
      "Device id: 0\n",
      "\tname:                                    \n",
      "\tglobal memory size:                      536 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum pitch size for memory copies:    536 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,2147483647,2147483647)\n",
      "Time for event \"memcpy\": 0.385 ms (4129.99 MB/s)\n",
      "Time for event \"mat_mult kernel\": 1.665 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "hsa_copy_deps: 0\n",
      "scan ops data 3:4                                                                                                    File 'rocprof_trace/result.copy_stats.csv' is generating\n",
      "dump json 2:3                                                                                                    \n",
      "File 'rocprof_trace/result.json' is generating\n",
      "File 'rocprof_trace/result.hip_stats.csv' is generating\n",
      "dump json 45:46                                                                                                    \n",
      "File 'rocprof_trace/result.json' is generating\n",
      "File 'rocprof_trace/result.stats.csv' is generating\n",
      "dump json 0:1                                                                                                    \n",
      "File 'rocprof_trace/result.json' is generating\n"
     ]
    }
   ],
   "source": [
    "!rocprof --hip-trace -o rocprof_trace/result.csv ./mat_mult_profiling.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b616c5e-6096-427a-af49-74a933c7e64b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Inside the **rocprof_trace** folder you will find the following files:\n",
    "\n",
    "| file | purpose |\n",
    "| --- | --- |\n",
    "| profile.sysinfo.txt | System information on available devices |\n",
    "| profile.copy_stats.csv | Statistics on all IO calls |\n",
    "| profile.hip_stats.csv | Statistics on non-IO HIP function calls |\n",
    "| profile.stats.csv | Statistics on all kernel calls |\n",
    "| profile.db | SQLITE3 database of profiling information |\n",
    "| profile.json | Trace information in JSON format |\n",
    "\n",
    "We can load the trace file using a web browser. In a web browser you can go to this site for a user interface on viewing trace information.\n",
    "\n",
    "[https://ui.perfetto.dev/](https://ui.perfetto.dev/)\n",
    "\n",
    "Download the trace file **profile.json** to your computer and open it with the Perfetto UI in your web browser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3383f3-4c27-4daf-b506-68486ce77bd1",
   "metadata": {},
   "source": [
    "<figure style=\"margin-left:0; margin-right:auto; width:100%;\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/Perfetto_UI.png\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Viewing rocprof application traces with Perfetto UI.</figcaption>\n",
    "</figure>\n",
    "\n",
    "If you zoom (using the \"wasd\" keys) in you can see calls in GPU threads, COPY threads and HOST threads on the CPU. Notice how the **hipEventRecord** function is executed before and after the **hipMemcpy** calls and the **mat_mult** kernel execution. If you click on the **mat_mult** function you can see how long the kernel took to execute.\n",
    "\n",
    "<figure style=\"margin-left:0; margin-right:auto; width:100%;\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/Perfetto_UI_kernel.png\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Determining the time for a kernel call</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f522f0f1-ac3f-4105-b90b-4658d82674d0",
   "metadata": {},
   "source": [
    "Other files that are generated with a trace include: \n",
    "\n",
    "|File|Purpose|\n",
    "|:--- | :--- |\n",
    "|\\<experiment_name\\>.copy_stats.csv| Profile of all IO operations such as performed by **hipMemcpy**. |\n",
    "|\\<experiment_name\\>.db| SQLite database of profiling results |\n",
    "|\\<experiment_name\\>.hip_stats.csv | Profile of all HIP calls such as **hipDeviceSynchronize** |\n",
    "|\\<experiment_name\\>.stats.csv | Profile of all kernel calls such as **mat_mult** |\n",
    "|\\<experiment_name\\>.json | Application trace to be viewed with Chrome tracing or Perfetto |\n",
    "|\\<experiment_name\\>.sysinfo.txt | System information obtained through rocminfo. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb51c18-aeb9-47f5-b5c2-57d8d7ee99cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hardware performance counters with rocprof\n",
    "\n",
    "Hardware performance counters are devices in a processor that measure events, such as the number of wavefronts executed, or the number of times a cache is missed. Rocprof can collect performance counters on kernels. The type of performance counter information that can be captured is obtained with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bedf65ea-5b6d-4262-b447-d4839f33216f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RPL: on '230329_173950' from '/opt/rocm-5.4.1' in '/home/toby/Pelagos/Projects/HIP_Course/course_material/L5_Profiling'\n",
      "Derived metrics:\n",
      "\n",
      "  gpu-agent0 : GPU_UTIL : Percentage of the time that GUI is active\n",
      "      GPU_UTIL = 100*GRBM_GUI_ACTIVE/GRBM_COUNT\n",
      "\n",
      "  gpu-agent0 : CP_UTIL : Percentage of the GRBM_GUI_ACTIVE time that any of the Command Processor (CPG/CPC/CPF) blocks are busy\n",
      "      CP_UTIL = 100*GRBM_CP_BUSY/GRBM_GUI_ACTIVE\n",
      "\n",
      "  gpu-agent0 : SPI_UTIL : Percentage of the GRBM_GUI_ACTIVE time that any of the Shader Pipe Interpolators (SPI) are busy in the shader engine(s)\n",
      "      SPI_UTIL = 100*GRBM_SPI_BUSY/GRBM_GUI_ACTIVE\n",
      "\n",
      "  gpu-agent0 : TA_UTIL : Percentage of the GRBM_GUI_ACTIVE time that any of the Texture Pipes (TA) are busy in the shader engine(s).\n",
      "      TA_UTIL = 100*GRBM_TA_BUSY/GRBM_GUI_ACTIVE\n",
      "\n",
      "  gpu-agent0 : GDS_UTIL : Percentage of the GRBM_GUI_ACTIVE time that the Global Data Share (GDS) is busy.\n",
      "      GDS_UTIL = 100*GRBM_GDS_BUSY/GRBM_GUI_ACTIVE\n",
      "\n",
      "  gpu-agent0 : EA_UTIL : Percentage of the GRBM_GUI_ACTIVE time that the Efficiency Arbiter (EA) block is busy.\n",
      "      EA_UTIL = 100*GRBM_EA_BUSY/GRBM_GUI_ACTIVE\n",
      "\n",
      "  gpu-agent0 : WAVE_DEP_WAIT : Percentage of the SQ_WAVE_CYCLE time spent waiting for anything.\n",
      "      WAVE_DEP_WAIT = 100*SQ_WAIT_ANY/SQ_WAVE_CYCLES\n",
      "\n",
      "  gpu-agent0 : WAVE_ISSUE_WAIT : Percentage of the SQ_WAVE_CYCLE time spent waiting for any instruction issue.\n",
      "      WAVE_ISSUE_WAIT = 100*SQ_WAIT_INST_ANY/SQ_WAVE_CYCLES\n",
      "\n",
      "  gpu-agent0 : TA_BUSY_avr : TA block is busy. Average over TA instances.\n",
      "      TA_BUSY_avr = avr(TA_TA_BUSY,16)\n",
      "\n",
      "  gpu-agent0 : TA_BUSY_max : TA block is busy. Max over TA instances.\n",
      "      TA_BUSY_max = max(TA_TA_BUSY,16)\n",
      "\n",
      "  gpu-agent0 : TA_BUSY_min : TA block is busy. Min over TA instances.\n",
      "      TA_BUSY_min = min(TA_TA_BUSY,16)\n",
      "\n",
      "  gpu-agent0 : TA_FLAT_LOAD_WAVEFRONTS_sum : Number of flat load vec32 packets processed by the TA. Sum over TA instances.\n",
      "      TA_FLAT_LOAD_WAVEFRONTS_sum = sum(TA_FLAT_LOAD_WAVEFRONTS,16)\n",
      "\n",
      "  gpu-agent0 : TA_FLAT_STORE_WAVEFRONTS_sum : Number of flat store vec32 packets processed by the TA. Sum over TA instances.\n",
      "      TA_FLAT_STORE_WAVEFRONTS_sum = sum(TA_FLAT_STORE_WAVEFRONTS,16)\n",
      "\n",
      "  gpu-agent0 : GL2C_HIT_sum : Number of cache hits. Sum over GL2C instances.\n",
      "      GL2C_HIT_sum = sum(GL2C_HIT,16)\n",
      "\n",
      "  gpu-agent0 : GL2C_MISS_sum : Number of cache misses. Sum over GL2C instances.\n",
      "      GL2C_MISS_sum = sum(GL2C_MISS,16)\n",
      "\n",
      "  gpu-agent0 : GL2C_EA_RDREQ_32B_sum : Number of 32-byte GL2C/EA read requests. Sum over GL2C instances.\n",
      "      GL2C_EA_RDREQ_32B_sum = sum(GL2C_EA_RDREQ_32B,16)\n",
      "\n",
      "  gpu-agent0 : GL2C_EA_RDREQ_64B_sum : Number of 64-byte GL2C/EA read requests. Sum over GL2C instances.\n",
      "      GL2C_EA_RDREQ_64B_sum = sum(GL2C_EA_RDREQ_64B,16)\n",
      "\n",
      "  gpu-agent0 : GL2C_EA_RDREQ_96B_sum : Number of 96-byte GL2C/EA read requests. Sum over GL2C instances.\n",
      "      GL2C_EA_RDREQ_96B_sum = sum(GL2C_EA_RDREQ_96B,16)\n",
      "\n",
      "  gpu-agent0 : GL2C_EA_RDREQ_128B_sum : Number of 128-byte GL2C/EA read requests. Sum over GL2C instances.\n",
      "      GL2C_EA_RDREQ_128B_sum = sum(GL2C_EA_RDREQ_128B,16)\n",
      "\n",
      "  gpu-agent0 : GL2C_MC_RDREQ_sum : Number of GL2C/EA read requests (either 32-byte or 64-byte or 128-byte). Sum over GL2C instances.\n",
      "      GL2C_MC_RDREQ_sum = sum(GL2C_MC_RDREQ,16)\n",
      "\n",
      "  gpu-agent0 : GL2C_MC_WRREQ_sum : Number of transactions (either 32-byte or 64-byte) going over the GL2C_MC_wrreq interface. Sum over GL2C instances.\n",
      "      GL2C_MC_WRREQ_sum = sum(GL2C_MC_WRREQ,16)\n",
      "\n",
      "  gpu-agent0 : GL2C_EA_WRREQ_64B_sum : Number of 64-byte transactions going (64-byte write or CMPSWAP) over the GL2C_EA_wrreq interface. Sum over GL2C instances.\n",
      "      GL2C_EA_WRREQ_64B_sum = sum(GL2C_EA_WRREQ_64B,16)\n",
      "\n",
      "  gpu-agent0 : GL2C_WRREQ_STALL_max : Number of cycles a write request was stalled. Max over GL2C instances.\n",
      "      GL2C_WRREQ_STALL_max = max(GL2C_MC_WRREQ_STALL,16)\n",
      "\n",
      "  gpu-agent0 : L2CacheHit : The percentage of fetch, write, atomic, and other instructions that hit the data in L2 cache. Value range: 0% (no hit) to 100% (optimal).\n",
      "      L2CacheHit = 100*sum(GL2C_HIT,16)/(sum(GL2C_HIT,16)+sum(GL2C_MISS,16))\n",
      "\n",
      "  gpu-agent0 : FETCH_SIZE : The total kilobytes fetched from the video memory. This is measured with all extra fetches and any cache or memory effects taken into account.\n",
      "      FETCH_SIZE = (GL2C_EA_RDREQ_32B_sum*32+GL2C_EA_RDREQ_64B_sum*64+GL2C_EA_RDREQ_96B_sum*96+GL2C_EA_RDREQ_128B_sum*128)/1024\n",
      "\n",
      "  gpu-agent0 : WriteUnitStalled : The percentage of GPUTime the Write unit is stalled. Value range: 0% to 100% (bad).\n",
      "      WriteUnitStalled = 100*GL2C_WRREQ_STALL_max/GRBM_GUI_ACTIVE\n",
      "\n",
      "  gpu-agent0 : LDSBankConflict : The percentage of GPUTime LDS is stalled by bank conflicts. Value range: 0% (optimal) to 100% (bad).\n",
      "      LDSBankConflict = 100*SQC_LDS_BANK_CONFLICT/SQC_LDS_IDX_ACTIVE\n",
      "\n",
      "  gpu-agent0 : GPUBusy : The percentage of time GPU was busy.\n",
      "      GPUBusy = 100*GRBM_GUI_ACTIVE/GRBM_COUNT\n",
      "\n",
      "  gpu-agent0 : Wavefronts : Total wavefronts.\n",
      "      Wavefronts = SQ_WAVES\n",
      "\n",
      "  gpu-agent0 : VALUInsts : The average number of vector ALU instructions executed per work-item (affected by flow control).\n",
      "      VALUInsts = SQ_INSTS_VALU/SQ_WAVES\n",
      "\n",
      "  gpu-agent0 : SALUInsts : The average number of scalar ALU instructions executed per work-item (affected by flow control).\n",
      "      SALUInsts = SQ_INSTS_SALU/SQ_WAVES\n",
      "\n",
      "  gpu-agent0 : SFetchInsts : The average number of scalar fetch instructions from the video memory executed per work-item (affected by flow control).\n",
      "      SFetchInsts = SQ_INSTS_SMEM/SQ_WAVES\n",
      "\n",
      "  gpu-agent0 : GDSInsts : The average number of GDS read or GDS write instructions executed per work item (affected by flow control).\n",
      "      GDSInsts = SQ_INSTS_GDS/SQ_WAVES\n",
      "\n",
      "  gpu-agent0 : MemUnitBusy : The percentage of GPUTime the memory unit is active. The result includes the stall time (MemUnitStalled). This is measured with all extra fetches and writes and any cache or memory effects taken into account. Value range: 0% to 100% (fetch-bound).\n",
      "      MemUnitBusy = 100*max(TA_TA_BUSY,16)/GRBM_GUI_ACTIVE/SE_NUM\n",
      "\n",
      "  gpu-agent0 : ALUStalledByLDS : The percentage of GPUTime ALU units are stalled by the LDS input queue being full or the output queue being not ready. If there are LDS bank conflicts, reduce them. Otherwise, try reducing the number of LDS accesses if possible. Value range: 0% (optimal) to 100% (bad).\n",
      "      ALUStalledByLDS = 100*SQ_WAIT_INST_LDS*4/SQ_WAVES/GRBM_GUI_ACTIVE\n",
      "\n",
      "ROCPRofiler: 0 contexts collected\n"
     ]
    }
   ],
   "source": [
    "!rocprof --list-derived"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce2b350-4a1c-4408-b807-9b79e5f911c6",
   "metadata": {},
   "source": [
    "We can specify the counters to collect in a file such as [rocprof_counters.txt](rocprof_counters.txt). Here we specify some commonly used metrics for collection. Each **pmc** line is a unique experiment involving an individual run of the code. In this example we collect stats for the **mat_mult** kernel for the first 64 work-items on GPU 0.\n",
    "\n",
    "```txt\n",
    "# Cache hits and Cache misses\n",
    "pmc: TCC_HIT_sum, TCC_MISS_sum\n",
    "\n",
    "# Total video memory fetched and written\n",
    "pmc: FETCH_SIZE, WRITE_SIZE\n",
    "\n",
    "# Percentage of time the GPU was busy, total wavefronts executed\n",
    "pmc: GPUBusy, Wavefronts\n",
    "\n",
    "# Average number of vector and scalar instructions executed per work-item\n",
    "pmc: VALUInsts, SALUInsts\n",
    "\n",
    "# Average number of vector and scalar fetch instructions per work-item\n",
    "pmc: VFetchInsts, SFetchInsts\n",
    "\n",
    "# Average number of vector write instructions per work-item\n",
    "pmc: VWriteInsts\n",
    "\n",
    "# Average number of shared and global memory read or write instructions per work item\n",
    "pmc: LDSInsts, GDSInsts\n",
    "\n",
    "# Percentage of active vector ALU threads in a wave, percentage of GPU time vector and scalar instructions are processed\n",
    "pmc: VALUUtilization, VALUBusy, SALUBusy, \n",
    "\n",
    "# Percentage of fetch, write, atomic, and other instructions that hit the L2 cache\n",
    "pmc: L2CacheHit\n",
    "\n",
    "# Percentage of time the memory unit is active (including stalled), and just stalled, percentage of time the write unit is stalled\n",
    "pmc: MemUnitBusy, MemUnitStalled, WriteUnitStalled\n",
    "\n",
    "# Percentage of time ALU's are stalled by shared memory access, percentage of GPU time local memory is stalled by bank conflicts\n",
    "pmc: ALUStalledByLDS, LDSBankConflict\n",
    "\n",
    "# Dispatches range, which work-items to profile\n",
    "range: 0 : 64\n",
    "# Which GPU's to profile\n",
    "gpu: 0\n",
    "# Names of kernels to profile\n",
    "kernel: mat_mult\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee575685-ab1f-40bf-aa3f-50224668d1aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "Then we can use rocprof to collect the data for these counters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e6071ab-48a0-4529-bc46-d7ea4d554965",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RPL: on '230329_174856' from '/opt/rocm-5.4.1' in '/home/toby/Pelagos/Projects/HIP_Course/course_material/L5_Profiling'\n",
      "RPL: profiling '\"./mat_mult_profiling.exe\"'\n",
      "RPL: input file 'rocprof_counters.txt'\n",
      "RPL: output dir '/tmp/rpl_data_230329_174856_84187'\n",
      "RPL: result dir '/tmp/rpl_data_230329_174856_84187/input0_results_230329_174856'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_230329_174856_84187/input0.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  2 metrics\n",
      "    TCC_HIT_sum, TCC_MISS_sum\n",
      "Device id: 0\n",
      "\tname:                                    \n",
      "\tglobal memory size:                      536 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum pitch size for memory copies:    536 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,2147483647,2147483647)\n",
      "Time for event \"memcpy\": 1.370 ms (1160.10 MB/s)\n",
      "Time for event \"mat_mult kernel\": 1.554 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 0 contexts collected, output directory /tmp/rpl_data_230329_174856_84187/input0_results_230329_174856\n",
      "RPL: result dir '/tmp/rpl_data_230329_174856_84187/input10_results_230329_174856'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_230329_174856_84187/input10.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  2 metrics\n",
      "    ALUStalledByLDS, LDSBankConflict\n",
      "Device id: 0\n",
      "\tname:                                    \n",
      "\tglobal memory size:                      536 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum pitch size for memory copies:    536 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,2147483647,2147483647)\n",
      "Time for event \"memcpy\": 0.104 ms (15246.92 MB/s)\n",
      "Time for event \"mat_mult kernel\": 3.403 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 0 contexts collected, output directory /tmp/rpl_data_230329_174856_84187/input10_results_230329_174856\n",
      "RPL: result dir '/tmp/rpl_data_230329_174856_84187/input1_results_230329_174856'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_230329_174856_84187/input1.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  2 metrics\n",
      "    FETCH_SIZE, WRITE_SIZE\n",
      "Device id: 0\n",
      "\tname:                                    \n",
      "\tglobal memory size:                      536 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum pitch size for memory copies:    536 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,2147483647,2147483647)\n",
      "Time for event \"memcpy\": 0.139 ms (11445.05 MB/s)\n",
      "Time for event \"mat_mult kernel\": 3.062 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 0 contexts collected, output directory /tmp/rpl_data_230329_174856_84187/input1_results_230329_174856\n",
      "RPL: result dir '/tmp/rpl_data_230329_174856_84187/input2_results_230329_174856'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_230329_174856_84187/input2.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  2 metrics\n",
      "    GPUBusy, Wavefronts\n",
      "Device id: 0\n",
      "\tname:                                    \n",
      "\tglobal memory size:                      536 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum pitch size for memory copies:    536 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,2147483647,2147483647)\n",
      "Time for event \"memcpy\": 0.106 ms (15055.83 MB/s)\n",
      "Time for event \"mat_mult kernel\": 3.182 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 0 contexts collected, output directory /tmp/rpl_data_230329_174856_84187/input2_results_230329_174856\n",
      "RPL: result dir '/tmp/rpl_data_230329_174856_84187/input3_results_230329_174856'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_230329_174856_84187/input3.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  2 metrics\n",
      "    VALUInsts, SALUInsts\n",
      "Device id: 0\n",
      "\tname:                                    \n",
      "\tglobal memory size:                      536 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum pitch size for memory copies:    536 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,2147483647,2147483647)\n",
      "Time for event \"memcpy\": 0.476 ms (3341.72 MB/s)\n",
      "Time for event \"mat_mult kernel\": 2.820 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 0 contexts collected, output directory /tmp/rpl_data_230329_174856_84187/input3_results_230329_174856\n",
      "RPL: result dir '/tmp/rpl_data_230329_174856_84187/input4_results_230329_174856'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_230329_174856_84187/input4.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  2 metrics\n",
      "    VFetchInsts, SFetchInsts\n",
      "Device id: 0\n",
      "\tname:                                    \n",
      "\tglobal memory size:                      536 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum pitch size for memory copies:    536 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,2147483647,2147483647)\n",
      "Time for event \"memcpy\": 0.354 ms (4486.08 MB/s)\n",
      "Time for event \"mat_mult kernel\": 1.813 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 0 contexts collected, output directory /tmp/rpl_data_230329_174856_84187/input4_results_230329_174856\n",
      "RPL: result dir '/tmp/rpl_data_230329_174856_84187/input5_results_230329_174856'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_230329_174856_84187/input5.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  1 metrics\n",
      "    VWriteInsts\n",
      "Device id: 0\n",
      "\tname:                                    \n",
      "\tglobal memory size:                      536 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum pitch size for memory copies:    536 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,2147483647,2147483647)\n",
      "Time for event \"memcpy\": 0.104 ms (15235.23 MB/s)\n",
      "Time for event \"mat_mult kernel\": 3.415 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 0 contexts collected, output directory /tmp/rpl_data_230329_174856_84187/input5_results_230329_174856\n",
      "RPL: result dir '/tmp/rpl_data_230329_174856_84187/input6_results_230329_174856'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_230329_174856_84187/input6.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  2 metrics\n",
      "    LDSInsts, GDSInsts\n",
      "Device id: 0\n",
      "\tname:                                    \n",
      "\tglobal memory size:                      536 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum pitch size for memory copies:    536 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,2147483647,2147483647)\n",
      "Time for event \"memcpy\": 0.105 ms (15206.08 MB/s)\n",
      "Time for event \"mat_mult kernel\": 3.415 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 0 contexts collected, output directory /tmp/rpl_data_230329_174856_84187/input6_results_230329_174856\n",
      "RPL: result dir '/tmp/rpl_data_230329_174856_84187/input7_results_230329_174856'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_230329_174856_84187/input7.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  3 metrics\n",
      "    VALUUtilization, VALUBusy, SALUBusy\n",
      "Device id: 0\n",
      "\tname:                                    \n",
      "\tglobal memory size:                      536 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum pitch size for memory copies:    536 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,2147483647,2147483647)\n",
      "Time for event \"memcpy\": 0.381 ms (4172.88 MB/s)\n",
      "Time for event \"mat_mult kernel\": 2.193 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 0 contexts collected, output directory /tmp/rpl_data_230329_174856_84187/input7_results_230329_174856\n",
      "RPL: result dir '/tmp/rpl_data_230329_174856_84187/input8_results_230329_174856'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_230329_174856_84187/input8.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  1 metrics\n",
      "    L2CacheHit\n",
      "Device id: 0\n",
      "\tname:                                    \n",
      "\tglobal memory size:                      536 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum pitch size for memory copies:    536 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,2147483647,2147483647)\n",
      "Time for event \"memcpy\": 0.183 ms (8694.82 MB/s)\n",
      "Time for event \"mat_mult kernel\": 3.127 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 0 contexts collected, output directory /tmp/rpl_data_230329_174856_84187/input8_results_230329_174856\n",
      "RPL: result dir '/tmp/rpl_data_230329_174856_84187/input9_results_230329_174856'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_230329_174856_84187/input9.xml\"\n",
      "  gpu_index = 0\n",
      "  kernel = mat_mult\n",
      "  range = 0:64\n",
      "  3 metrics\n",
      "    MemUnitBusy, MemUnitStalled, WriteUnitStalled\n",
      "Device id: 0\n",
      "\tname:                                    \n",
      "\tglobal memory size:                      536 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum pitch size for memory copies:    536 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,2147483647,2147483647)\n",
      "Time for event \"memcpy\": 0.360 ms (4415.78 MB/s)\n",
      "Time for event \"mat_mult kernel\": 1.794 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "\n",
      "ROCPRofiler: 0 contexts collected, output directory /tmp/rpl_data_230329_174856_84187/input9_results_230329_174856\n",
      "File 'rocprof_counters/result.csv' is generating\n"
     ]
    }
   ],
   "source": [
    "!rocprof -i rocprof_counters.txt -o rocprof_counters/result.csv ./mat_mult_profiling.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7440b698-587e-42ec-a944-382dc9d2c481",
   "metadata": {
    "tags": []
   },
   "source": [
    "If your chosen performance counters are supported, then the file [rocprof_counters/result.csv](rocprof_counters/result.csv) should contain a count for every time the counter was triggered. The file [rocprof_counters/example.csv](rocprof_counters/example.csv) is an example file collected with rocprof on **mat_mult_profiling.exe**. This [page](https://docs.amd.com/bundle/ROCProfiler-User-Guide-v5.1/page/rocprof_Command_Line_Tool.html) has information on what the keys in the CSV file mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3090705-8e92-4834-b46b-af432243382c",
   "metadata": {},
   "source": [
    "### Rocprof under a job manager\n",
    "\n",
    "Rocprof runs fine under a job manager like SLURM, you just need to make an output file for each process launched. For example on SLURM the **SLURM_JOBID** and **SLURM_PROCID** environment variables are helpful in separating the output. Put the rocprof commands in a script called **profile.sh**.\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "rocprof -i rocprof_counters.txt -o rocprof_counters/result-$SLURM_JOBID-$SLURM_PROCID.csv ./mat_mult_profiling.exe\n",
    "```\n",
    "\n",
    "Then you can run the script from **srun** like this so it picks up the environment variable **$SLURM_PROCID** from within the script.\n",
    "\n",
    "```bash\n",
    "srun -N $SLURM_JOB_NUM_NODES -n $SLURM_NTASKS -c $OMP_NUM_THREADS ./profile.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c822fc0-0f3b-4f05-b259-8bc690a7d250",
   "metadata": {},
   "source": [
    "### Rocprofiler API\n",
    "\n",
    "If you'd like to instrument code with profiling calls the **[rocprofiler API](https://github.com/ROCm-Developer-Tools/rocprofiler/blob/amd-master/doc/rocprofiler_spec.md)** is available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c16d9e3-0dc9-478f-8d84-3f71f67579c9",
   "metadata": {},
   "source": [
    "### Tracing with Omnitrace\n",
    "\n",
    "[Omnitrace](https://github.com/AMDResearch/omnitrace) is an AMD research project to collect performance information on a program at runtime. It supports programs written in C, C++, Fortran and Python, as well as compute frameworks like OpenCL and HIP. We load the Omnitrace module using something like: \n",
    "\n",
    "```bash\n",
    "module load omnitrace/version\n",
    "```\n",
    "\n",
    "Then we can use Omnitrace to make a trace of **mat_mult_profiling.exe**.\n",
    "\n",
    "```bash\n",
    "cd omni_trace\n",
    "omnitrace -- ../mat_mult_profiling.exe\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bee8406-d101-4c4c-884e-a6a86cfc872f",
   "metadata": {},
   "source": [
    "If you look in the sub folder **omni_trace/omnitrace-mat_mult_profiling-output** there is a folder with the date of the trace. Download the **.proto** file and open it with [ui.perfetto.dev](https://ui.perfetto.dev) in a similar way to the json trace files from rocprof. You should see when and for how long functions are executed on the host and for how long kernels are executed on the device, along with a more detailed set of metrics such as CPU frequency and power consumption.\n",
    "\n",
    "<figure style=\"margin-left:0; margin-right:auto; width:100%;\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/omnitrace.png\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Examining the output from Omnitrace using <a href=\"https://ui.perfetto.dev\">ui.perfetto.dev</a></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a0a9d0-0406-47a1-bf01-b1eb087a7c75",
   "metadata": {},
   "source": [
    "### Performance measurement with Omniperf\n",
    "\n",
    "The AMD research tool Omniperf [Omniperf](https://github.com/AMDResearch/omniperf) is a powerful tool for measuring the performance of applications on AMD Instinct GPU's like the MI250X on Setonix. It can perform feats like [Roofline Analysis](https://en.wikipedia.org/wiki/Roofline_model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b2aaf2-ef12-44af-b96a-1e2a35a75082",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Performance measurement with NVIDIA tools\n",
    "\n",
    "HIP applications that use the CUDA backend (i.e compiled with HIP_PLATFORM=nvidia) have access to the NVIDIA performance measurement tools such as [NVIDIA Nsight Systems](https://developer.nvidia.com/nsight-systems) and [NVIDIA Nsight Compute](https://developer.nvidia.com/nsight-compute). Here we briefly cover how to use these tools.\n",
    "\n",
    "### Tracing with Nsight Systems\n",
    "\n",
    "The command line application **nsys** can collect traces on **mat_mult_profiling.exe**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8e3e699-6e16-431b-94f8-39c07100ac87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: CPU IP/backtrace sampling not supported, disabling.\n",
      "Try the 'nsys status --environment' command to learn more.\n",
      "\n",
      "WARNING: CPU context switch tracing not supported, disabling.\n",
      "Try the 'nsys status --environment' command to learn more.\n",
      "\n",
      "Device id: 0\n",
      "\tname:                                    NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "\tglobal memory size:                      6226 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    49 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,64)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65535,65535)\n",
      "Time for event \"memcpy\": 0.152 ms (10464.39 MB/s)\n",
      "Time for event \"mat_mult kernel\": 0.452 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "Generating '/tmp/nsys-report-ce04.qdstrm'\n",
      "Failed to create '/home/toby/Pelagos/Projects/HIP_Course/course_material/L5_Profiling/nsys_trace/results.nsys-rep': File exists.\n",
      "Use `--force-overwrite true` to overwrite existing files.\n",
      "[1/1] [========================100%] nsys-report-077a.nsys-rep\n",
      "Generated:\n",
      "    /tmp/nsys-report-077a.nsys-rep\n"
     ]
    }
   ],
   "source": [
    "!nsys profile -o nsys_trace/results ./mat_mult_profiling.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451fd761-d14f-407b-afd4-319424719b78",
   "metadata": {},
   "source": [
    "Then you can use this command under Linux to view the application trace \n",
    "\n",
    "```bash\n",
    "nsys-ui nsys_trace/results.nsys-rep\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51171e76-80dc-4c03-a1b8-0fe557832350",
   "metadata": {},
   "source": [
    "It's important to note that when using the NVIDIA backend it is important to note that HIP is a thin layer over CUDA. NVIDIA performance tools will report usage for the underlying CUDA functions instead of the HIP labelled functions. For example, when using Nsight Systems it will report a call to **cudaDeviceSynchronize** instead of **hipDeviceSynchronize**. One has to make the mental mapping between HIP and CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5b9bfd-94ac-4158-bb20-300b732adf93",
   "metadata": {},
   "source": [
    "### Hardware collection with Nsight compute\n",
    "\n",
    "Nsight compute has the ability to collect hardware performance counters, however this ability needs either administrator access or access granted to performance counters at the OS level. If this access is possible then the following command will collect hardware performance counters on **mat_mult_profiling.exe**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "220a1c36-8bb4-44ff-b508-1437fb28234f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==PROF== Connected to process 9050 (/home/toby/Pelagos/Projects/HIP_Course/course_material/L5_Profiling/mat_mult_profiling.exe)\n",
      "Device id: 0\n",
      "\tname:                                    NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "\tglobal memory size:                      6226 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    49 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,64)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65535,65535)\n",
      "Time for event \"memcpy\": 0.140 ms (11341.40 MB/s)\n",
      "==PROF== Profiling \"mat_mult\" - 0: 0%....50%....100% - 9 passes\n",
      "Time for event \"mat_mult kernel\": 200.655 ms\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n",
      "==PROF== Disconnected from process 9050\n",
      "==PROF== Report: /home/toby/Pelagos/Projects/HIP_Course/course_material/L5_Profiling/ncu_counters/results.ncu-rep\n"
     ]
    }
   ],
   "source": [
    "!ncu -f -o ncu_counters/results ./mat_mult_profiling.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cd0f55-b0c0-48d1-98bd-b7be9ef65991",
   "metadata": {},
   "source": [
    "Then you can run the command:\n",
    "\n",
    "```bash\n",
    "ncu-ui\n",
    "```\n",
    "\n",
    "To view the hardware performance counter information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c38c04c-7c8d-4646-a382-0b02fb91a42e",
   "metadata": {},
   "source": [
    "<address>\n",
    "Written by Dr. Toby Potter of <a href=\"https://www.pelagos-consulting.com\">Pelagos Consulting and Education</a> for the Pawsey Supercomputing Centre\n",
    "</address>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
