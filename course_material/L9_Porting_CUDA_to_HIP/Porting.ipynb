{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "916435d8-e663-4bd0-b3e2-8d8344fd6830",
   "metadata": {},
   "source": [
    "# Porting CUDA programs to HIP\n",
    "\n",
    "HIP API calls are designed to closely match their CUDA equivalents. This enables HIP to function as a thin layer over CUDA and allows for reasonably easy porting of CUDA code to HIP code. Often it is just a matter of replacing **cuda -> hip** in the function calls. The ROCM suite provides two different tools **hipify-perl** and **hipify-clang** to help with the porting process. The tool **hipify-perl** is robust and uses perl to perform an intelligent search and replace of cuda calls with hip calls, while the **hipify-clang** tool uses the clang preprocessor to produce a high quality port. The perl-based method is better for quick ports of small codes, while the clang-based method is intended for ports of large codebases. The hipify-clang tool is much more picky though and fails easily unless it has access to all the header files used in the compilation of the CUDA code.\n",
    "\n",
    "## Supported API's\n",
    "\n",
    "A large subset of CUDA API calls are supported by HIP, including those in supporting libraries like **cuBLAS**. Tables in [this Github site](https://github.com/ROCm-Developer-Tools/HIPIFY/blob/amd-staging/docs/supported_apis.md) provides some guidance as to what is supported."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aef7d88-273f-4ee5-8a56-a7a4fb64cf2f",
   "metadata": {},
   "source": [
    "## Setup and installation\n",
    "\n",
    "From [this source](https://sep5.readthedocs.io/en/latest/Programming_Guides/HIP-porting-guide.html) it is recommended to attempt porting on a machine that has access to both CUDA and HIP libraries. This usually means doing the port on a machine with an NVIDIA GPU. Then one can try porting portions of the code at a time and compare results. For best results with hipify-clang you need to have a version of CUDA that is compatible with your installed version of hipify-clang. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "679e253f-cc77-40e8-826d-cc8e904394e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMD LLVM version 16.0.0git\n",
      "  Optimized build.\n"
     ]
    }
   ],
   "source": [
    "!hipify-clang --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3341127e-3ad1-45b6-bb3f-0624e4d60c33",
   "metadata": {},
   "source": [
    "Here is a page which describes compatibility between CUDA and hipify-clang.\n",
    "\n",
    "[HIPIFY Documentation](https://rocm.docs.amd.com/projects/HIPIFY/en/latest/hipify-clang.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d427b5-af01-452e-8bb8-b72c57e47ca7",
   "metadata": {},
   "source": [
    "## Trial setup\n",
    "\n",
    "There are two sub-directories in this module:\n",
    "\n",
    "* cuda_mat_mult\n",
    "* hip_mat_mult\n",
    "\n",
    "In the directory **cuda_mat_mult** is a CUDA version of the HIP matrix multiplication code in **hip_mat_mult**. It was manually ported from HIP to CUDA. We are going to use the HIP tools to try and port back the CUDA code to HIP code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158aeee9-6c9b-452d-8ab6-510f9d30850f",
   "metadata": {},
   "source": [
    "## Porting techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea2199-4187-41fa-8f26-2ef36342b579",
   "metadata": {},
   "source": [
    "### Port a single file\n",
    "\n",
    "Let's first make a temporary of **cuda_mat_mult** for the purpose of conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "454c9ad4-cf6f-4c41-9718-3f0111b8fc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p temp_mat_mult; cp -r cuda_mat_mult/* temp_mat_mult/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff01866-4aa5-456c-b250-59bb193fb640",
   "metadata": {},
   "source": [
    "The **hipify-perl** command can port a single file to use the HIP API. We use it to port the file **mat_mult.cu** in the directory **temp_mat_mult**. The flag `-hip-kernel-execution-syntax` changes kernel launch syntax from the CUDA-style triple Chevron `<<< >>>` method to the ANSI C++ compliant method of **hipLaunchKernelGGL**. The following command dumps the output to the command line, but you can use the `-o` flag to specify an output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9a82eaa-6360-4b8c-8c96-7c121b8fb5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#include \"hip/hip_runtime.h\"\n",
      "/* Code to perform a Matrix multiplication using cuda\n",
      "Written by Dr Toby M. Potter\n",
      "*/\n",
      "\n",
      "// Setup headers\n",
      "#include <cassert>\n",
      "#include <cmath>\n",
      "#include <iostream>\n",
      "\n",
      "// Bring in the size of the matrices\n",
      "#include \"mat_size.hpp\"\n",
      "\n",
      "// Bring in a library to manage matrices on the CPU\n",
      "#include \"mat_helper.hpp\"\n",
      "\n",
      "// Bring in helper header to manage boilerplate code\n",
      "#include \"cuda_helper.cu\"\n",
      "\n",
      "// standard matrix multiply kernel \n",
      "__global__ void mat_mult (\n",
      "        float* A, \n",
      "        float* B, \n",
      "        float* C, \n",
      "        size_t N1_A, \n",
      "        size_t N0_C,\n",
      "        size_t N1_C) { \n",
      "            \n",
      "    // A is of size (N0_C, N1_A)\n",
      "    // B is of size (N1_A, N1_C)\n",
      "    // C is of size (N0_C, N1_C)   \n",
      "    \n",
      "    // i0 and i1 represent the coordinates in Matrix C \n",
      "    // We use row-major ordering for the matrices\n",
      "    \n",
      "    size_t i0 = blockIdx.y * blockDim.y + threadIdx.y;\n",
      "    size_t i1 = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "    \n",
      "    // Scratch variable\n",
      "    float temp=0.0f; \n",
      "\n",
      "    // Guard mechanism to make sure we do not go\n",
      "    // outside the boundaries of matrix C \n",
      "    if ((i0<N0_C) && (i1<N1_C)) {\n",
      "        // Get the offset within the memory allocation of C\n",
      "        size_t offset = i0*N1_C+i1;\n",
      "        \n",
      "        // Loop over columns of A and rows of B\n",
      "        for (size_t n=0; n<N1_A; n++) {\n",
      "            \n",
      "            // A is of size (N0_C, N1_A)\n",
      "            // B is of size (N1_A, N1_C)\n",
      "            \n",
      "            // Loop across row i0 of A\n",
      "            // and down column i1 of B\n",
      "            temp+=A[i0*N1_A+n]*B[i1+n*N1_C]; \n",
      "        }\n",
      "        \n",
      "        // Set the value in C at offset\n",
      "        C[offset]=temp;\n",
      "        \n",
      "        // Uncomment this to perform elementwise matrix multiplication instead\n",
      "        // C[offset]=A[offset]*B[offset];\n",
      "    }\n",
      "} \n",
      "\n",
      "int main(int argc, char** argv) {\n",
      "    \n",
      "    //// Step 1. Parse program arguments ////\n",
      "\n",
      "    // Parse command line arguments\n",
      "    int dev_index = h_parse_args(argc, argv);\n",
      "    \n",
      "    // Number of devices discovered\n",
      "    int num_devices=0;\n",
      "    \n",
      "    //// Step 2. Discover resources and choose a compute device ////\n",
      "    \n",
      "    // Helper function to acquire devices\n",
      "    // This sets the default device\n",
      "    h_acquire_devices(&num_devices, dev_index);\n",
      "        \n",
      "    // Report on the device in use\n",
      "    h_report_on_device(dev_index);\n",
      "    \n",
      "    // We are going to do a simple array multiplication for this example, \n",
      "    // using raw binary files for input and output\n",
      "    \n",
      "    // A is of size (N0_C, N1_A)\n",
      "    // B is of size (N1_A, N1_C)    \n",
      "    // C is of size (N0_C, N1_C)\n",
      "\n",
      "    size_t N1_A = NCOLS_A, N0_C = NROWS_C, N1_C = NCOLS_C;\n",
      "\n",
      "    //// Step 3. Construct matrices A_h and B_h on the host \n",
      "    //// and fill them with random numbers ////\n",
      "    \n",
      "    // Number of bytes in each array\n",
      "    size_t nbytes_A = N0_C*N1_A*sizeof(float);\n",
      "    size_t nbytes_B = N1_A*N1_C*sizeof(float);\n",
      "    size_t nbytes_C = N0_C*N1_C*sizeof(float);\n",
      "\n",
      "    // Allocate memory for the host arrays\n",
      "    float* A_h = (float*)h_alloc(nbytes_A);\n",
      "    float* B_h = (float*)h_alloc(nbytes_B);\n",
      "    float* C_h = (float*)h_alloc(nbytes_C);\n",
      "\n",
      "    // Fill the host arrays with random numbers \n",
      "    // using the matrix helper library\n",
      "    m_random(A_h, N0_C, N1_A);\n",
      "    m_random(B_h, N1_A, N1_C);\n",
      "    \n",
      "    //// Step 4. Allocate memory for arrays //// \n",
      "    //// A_d, B_d, and C_d on the compute device ////\n",
      "\n",
      "    float *A_d, *B_d, *C_d;\n",
      "    H_ERRCHK(hipMalloc((void**)&A_d, nbytes_A));\n",
      "    H_ERRCHK(hipMalloc((void**)&B_d, nbytes_B));\n",
      "    H_ERRCHK(hipMalloc((void**)&C_d, nbytes_C));\n",
      "\n",
      "    //// Step 5. 1. Upload matrices A_h and B_h from the host //// \n",
      "    //// to A_d and B_d on the device ////\n",
      "    H_ERRCHK(hipMemcpy(A_d, A_h, nbytes_A, hipMemcpyHostToDevice));\n",
      "    H_ERRCHK(hipMemcpy(B_d, B_h, nbytes_B, hipMemcpyHostToDevice));\n",
      " \n",
      "    //// Step 6. Run the kernel to compute C_d ///\n",
      "    //// from A_d and B_d on the device ////\n",
      "        \n",
      "    // Desired block size\n",
      "    dim3 block_size = { 8, 8, 1 };\n",
      "    dim3 global_size = { (uint32_t)N1_C, (uint32_t)N0_C, 1 };\n",
      "    dim3 grid_nblocks;\n",
      "    \n",
      "    // Choose the number of blocks so that Grid fits within it.\n",
      "    h_fit_blocks(&grid_nblocks, global_size, block_size);\n",
      "\n",
      "    // Amount of shared memory to use in the kernel\n",
      "    size_t sharedMemBytes=0;\n",
      "    \n",
      "    // Launch the kernel using CUDA triple Chevron syntax\n",
      "    // Use 0 when choosing the default (null) stream\n",
      "    hipLaunchKernelGGL(mat_mult, grid_nblocks, block_size, sharedMemBytes, 0, A_d, B_d, C_d, N1_A, N0_C, N1_C);\n",
      "    \n",
      "    // Check the status of the kernel launch\n",
      "    H_ERRCHK(hipGetLastError());\n",
      "    \n",
      "    // Wait for any commands to complete on the compute device\n",
      "    H_ERRCHK(hipDeviceSynchronize());\n",
      "\n",
      "    //// Step 7. Copy the buffer for matrix C_d //// \n",
      "    //// on the device back to C_h on the host ////\n",
      "    H_ERRCHK(hipMemcpy((void*)C_h, (const void*)C_d, nbytes_C, hipMemcpyDeviceToHost));\n",
      "    \n",
      "    //// Step 8. Test the computed matrix **C_h** against a known answer\n",
      "    \n",
      "    // Compute the serial solution using the matrix helper library\n",
      "    float* C_answer_h = (float*)calloc(nbytes_C, 1);\n",
      "    m_mat_mult(A_h, B_h, C_answer_h, N1_A, N0_C, N1_C);\n",
      "    \n",
      "    // Uncomment this to check against elementwise matrix multiplication\n",
      "    // m_hadamard(A_h, B_h, C_answer_h, N0_C, N1_C);\n",
      "\n",
      "    // Print the maximum error between matrices\n",
      "    float max_err = m_max_error(C_h, C_answer_h, N0_C, N1_C);\n",
      "    \n",
      "    //// Step 9. Write the contents of matrices A_h, B_h, and C_h to disk ////\n",
      "\n",
      "    // Write out the host arrays to file\n",
      "    h_write_binary(A_h, \"array_A.dat\", nbytes_A);\n",
      "    h_write_binary(B_h, \"array_B.dat\", nbytes_B);\n",
      "    h_write_binary(C_h, \"array_C.dat\", nbytes_C);\n",
      "    \n",
      "    //// Step 10. Clean up memory alllocations and release resources\n",
      "    \n",
      "    // Free the cuda buffers\n",
      "    H_ERRCHK(hipFree(A_d));\n",
      "    H_ERRCHK(hipFree(B_d));\n",
      "    H_ERRCHK(hipFree(C_d));\n",
      "\n",
      "    // Clean up host memory\n",
      "    free(A_h);\n",
      "    free(B_h);\n",
      "    free(C_h);\n",
      "\n",
      "    // Free the answer matrix\n",
      "    free(C_answer_h);\n",
      "    \n",
      "    // Reset compute devices\n",
      "    h_reset_devices(num_devices);\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cd temp_mat_mult; hipify-perl -hip-kernel-execution-syntax  mat_mult.cu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747905a4-2e43-4269-900f-de3bd4205f5f",
   "metadata": {},
   "source": [
    "If we use the `-inplace` flag, **hipify-perl** copies the file [mat_mult.cu](temp_mat_mult/mat_mult.cu) first to [mat_mult.cu.prehip](temp_mat_mult/mat_mult.cu.prehip) **if that file doesn't already exist**. Then it performs the conversion from [mat_mult.cu.prehip](temp_mat_mult/mat_mult.cu.prehip) to [mat_mult.cu](temp_mat_mult/mat_mult.cu). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4704d730-6ab7-4216-89d9-6b13eeb0024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd temp_mat_mult; hipify-perl -inplace -hip-kernel-execution-syntax mat_mult.cu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498f4116-e38e-4113-a148-844a92d8b83a",
   "metadata": {},
   "source": [
    "Subsequent edits to [mat_mult.cu.prehip](temp_mat_mult/mat_mult.cu.prehip) will be propagated across to [mat_mult.cu](temp_mat_mult/mat_mult.cu). This allows for an iterative porting process. Use the `--help` flag for more porting options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f4a7d3-8c4d-431b-901a-870916d81a58",
   "metadata": {},
   "source": [
    "### Examine a directory structure for porting potential\n",
    "\n",
    "We use the scripts **hipexamine-perl.sh** or **hipexamine.sh** to recursively search through a directory and examine the potential for porting a code. Note the summary that is produced for each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12620b08-509c-4d8b-ae80-8534892a134b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  warning: cuda_mat_mult/cuda_helper.cu:480: removed identifier \"cudaLaunch\" since CUDA 10.1\n",
      "\n",
      "[HIPIFY] info: file 'cuda_mat_mult/cuda_helper.cu' statistics:\n",
      "  CONVERTED refs count: 55\n",
      "  TOTAL lines of code: 789\n",
      "  WARNINGS: 1\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  CUDA_SUCCESS => hipSuccess: 4\n",
      "  CUresult => hipError_t: 4\n",
      "  cuGetErrorString => hipDrvGetErrorString: 1\n",
      "  cuInit => hipInit: 1\n",
      "  cuda.h => hip/hip_runtime.h: 2\n",
      "  cudaDevAttrManagedMemory => hipDeviceAttributeManagedMemory: 1\n",
      "  cudaDeviceGetAttribute => hipDeviceGetAttribute: 1\n",
      "  cudaDeviceProp => hipDeviceProp_t: 2\n",
      "  cudaDeviceReset => hipDeviceReset: 1\n",
      "  cudaDeviceSynchronize => hipDeviceSynchronize: 1\n",
      "  cudaError_t => hipError_t: 4\n",
      "  cudaEventCreate => hipEventCreate: 2\n",
      "  cudaEventDestroy => hipEventDestroy: 2\n",
      "  cudaEventElapsedTime => hipEventElapsedTime: 1\n",
      "  cudaEventRecord => hipEventRecord: 3\n",
      "  cudaEventSynchronize => hipEventSynchronize: 2\n",
      "  cudaEvent_t => hipEvent_t: 3\n",
      "  cudaGetDevice => hipGetDevice: 1\n",
      "  cudaGetDeviceCount => hipGetDeviceCount: 2\n",
      "  cudaGetDeviceProperties => hipGetDeviceProperties: 2\n",
      "  cudaGetErrorString => hipGetErrorString: 2\n",
      "  cudaLaunchKernel => hipLaunchKernel: 1\n",
      "  cudaSetDevice => hipSetDevice: 3\n",
      "  cudaStreamCreateWithFlags => hipStreamCreateWithFlags: 1\n",
      "  cudaStreamDefault => hipStreamDefault: 1\n",
      "  cudaStreamDestroy => hipStreamDestroy: 1\n",
      "  cudaStreamNonBlocking => hipStreamNonBlocking: 1\n",
      "  cudaStream_t => hipStream_t: 7\n",
      "  cudaSuccess => hipSuccess: 4\n",
      "  cuda_runtime.h => hip/hip_runtime.h: 2\n",
      "\n",
      "[HIPIFY] info: file 'cuda_mat_mult/mat_mult.cu' statistics:\n",
      "  CONVERTED refs count: 14\n",
      "  TOTAL lines of code: 190\n",
      "  WARNINGS: 0\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  cudaDeviceSynchronize => hipDeviceSynchronize: 1\n",
      "  cudaFree => hipFree: 3\n",
      "  cudaGetLastError => hipGetLastError: 1\n",
      "  cudaMalloc => hipMalloc: 3\n",
      "  cudaMemcpy => hipMemcpy: 3\n",
      "  cudaMemcpyDeviceToHost => hipMemcpyDeviceToHost: 1\n",
      "  cudaMemcpyHostToDevice => hipMemcpyHostToDevice: 2\n",
      "\n",
      "[HIPIFY] info: file 'cuda_mat_mult/.ipynb_checkpoints/mat_mult-checkpoint.cu' statistics:\n",
      "  CONVERTED refs count: 14\n",
      "  TOTAL lines of code: 190\n",
      "  WARNINGS: 0\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  cudaDeviceSynchronize => hipDeviceSynchronize: 1\n",
      "  cudaFree => hipFree: 3\n",
      "  cudaGetLastError => hipGetLastError: 1\n",
      "  cudaMalloc => hipMalloc: 3\n",
      "  cudaMemcpy => hipMemcpy: 3\n",
      "  cudaMemcpyDeviceToHost => hipMemcpyDeviceToHost: 1\n",
      "  cudaMemcpyHostToDevice => hipMemcpyHostToDevice: 2\n",
      "  warning: cuda_mat_mult/.ipynb_checkpoints/cuda_helper-checkpoint.cu:480: removed identifier \"cudaLaunch\" since CUDA 10.1\n",
      "\n",
      "[HIPIFY] info: file 'cuda_mat_mult/.ipynb_checkpoints/cuda_helper-checkpoint.cu' statistics:\n",
      "  CONVERTED refs count: 55\n",
      "  TOTAL lines of code: 789\n",
      "  WARNINGS: 1\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  CUDA_SUCCESS => hipSuccess: 4\n",
      "  CUresult => hipError_t: 4\n",
      "  cuGetErrorString => hipDrvGetErrorString: 1\n",
      "  cuInit => hipInit: 1\n",
      "  cuda.h => hip/hip_runtime.h: 2\n",
      "  cudaDevAttrManagedMemory => hipDeviceAttributeManagedMemory: 1\n",
      "  cudaDeviceGetAttribute => hipDeviceGetAttribute: 1\n",
      "  cudaDeviceProp => hipDeviceProp_t: 2\n",
      "  cudaDeviceReset => hipDeviceReset: 1\n",
      "  cudaDeviceSynchronize => hipDeviceSynchronize: 1\n",
      "  cudaError_t => hipError_t: 4\n",
      "  cudaEventCreate => hipEventCreate: 2\n",
      "  cudaEventDestroy => hipEventDestroy: 2\n",
      "  cudaEventElapsedTime => hipEventElapsedTime: 1\n",
      "  cudaEventRecord => hipEventRecord: 3\n",
      "  cudaEventSynchronize => hipEventSynchronize: 2\n",
      "  cudaEvent_t => hipEvent_t: 3\n",
      "  cudaGetDevice => hipGetDevice: 1\n",
      "  cudaGetDeviceCount => hipGetDeviceCount: 2\n",
      "  cudaGetDeviceProperties => hipGetDeviceProperties: 2\n",
      "  cudaGetErrorString => hipGetErrorString: 2\n",
      "  cudaLaunchKernel => hipLaunchKernel: 1\n",
      "  cudaSetDevice => hipSetDevice: 3\n",
      "  cudaStreamCreateWithFlags => hipStreamCreateWithFlags: 1\n",
      "  cudaStreamDefault => hipStreamDefault: 1\n",
      "  cudaStreamDestroy => hipStreamDestroy: 1\n",
      "  cudaStreamNonBlocking => hipStreamNonBlocking: 1\n",
      "  cudaStream_t => hipStream_t: 7\n",
      "  cudaSuccess => hipSuccess: 4\n",
      "  cuda_runtime.h => hip/hip_runtime.h: 2\n",
      "\n",
      "[HIPIFY] info: file 'GLOBAL' statistics:\n",
      "  CONVERTED refs count: 138\n",
      "  TOTAL lines of code: 2141\n",
      "  WARNINGS: 2\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  CUDA_SUCCESS => hipSuccess: 8\n",
      "  CUresult => hipError_t: 8\n",
      "  cuGetErrorString => hipDrvGetErrorString: 2\n",
      "  cuInit => hipInit: 2\n",
      "  cuda.h => hip/hip_runtime.h: 4\n",
      "  cudaDevAttrManagedMemory => hipDeviceAttributeManagedMemory: 2\n",
      "  cudaDeviceGetAttribute => hipDeviceGetAttribute: 2\n",
      "  cudaDeviceProp => hipDeviceProp_t: 4\n",
      "  cudaDeviceReset => hipDeviceReset: 2\n",
      "  cudaDeviceSynchronize => hipDeviceSynchronize: 4\n",
      "  cudaError_t => hipError_t: 8\n",
      "  cudaEventCreate => hipEventCreate: 4\n",
      "  cudaEventDestroy => hipEventDestroy: 4\n",
      "  cudaEventElapsedTime => hipEventElapsedTime: 2\n",
      "  cudaEventRecord => hipEventRecord: 6\n",
      "  cudaEventSynchronize => hipEventSynchronize: 4\n",
      "  cudaEvent_t => hipEvent_t: 6\n",
      "  cudaFree => hipFree: 6\n",
      "  cudaGetDevice => hipGetDevice: 2\n",
      "  cudaGetDeviceCount => hipGetDeviceCount: 4\n",
      "  cudaGetDeviceProperties => hipGetDeviceProperties: 4\n",
      "  cudaGetErrorString => hipGetErrorString: 4\n",
      "  cudaGetLastError => hipGetLastError: 2\n",
      "  cudaLaunchKernel => hipLaunchKernel: 2\n",
      "  cudaMalloc => hipMalloc: 6\n",
      "  cudaMemcpy => hipMemcpy: 6\n",
      "  cudaMemcpyDeviceToHost => hipMemcpyDeviceToHost: 2\n",
      "  cudaMemcpyHostToDevice => hipMemcpyHostToDevice: 4\n",
      "  cudaSetDevice => hipSetDevice: 6\n",
      "  cudaStreamCreateWithFlags => hipStreamCreateWithFlags: 2\n",
      "  cudaStreamDefault => hipStreamDefault: 2\n",
      "  cudaStreamDestroy => hipStreamDestroy: 2\n",
      "  cudaStreamNonBlocking => hipStreamNonBlocking: 2\n",
      "  cudaStream_t => hipStream_t: 14\n",
      "  cudaSuccess => hipSuccess: 8\n",
      "  cuda_runtime.h => hip/hip_runtime.h: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hipexamine-perl.sh cuda_mat_mult -exclude-dirs=cuda_mat_mult/.ipynb_checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3117e3-f3bf-4c7f-9351-085154143a0e",
   "metadata": {},
   "source": [
    "If we try the hip-clang version we see that it doesn't handle preprocessor directives very well. The following errors with `_aligned_malloc` are due to it not picking up the windows-specific `#define` clauses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a5ac422-ce12-4499-9aed-93f4db9559bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: CUDA version is newer than the latest partially supported version 11.8\n",
      "warning: CUDA version is newer than the latest partially supported version 11.8\n",
      "error: unsupported architecture 'nvptx64' for host compilation\n",
      "\u001b[0;1;35mwarning: \u001b[0m\u001b[1mCUDA version is newer than the latest partially supported version 11.8 [-Wunknown-cuda-version]\u001b[0m\n",
      "\u001b[1m/tmp/cuda_helper.cu-462e57.hip:95:5: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1munknown type name 'SYSTEM_INFO'\u001b[0m\n",
      "    SYSTEM_INFO sys_info;\n",
      "\u001b[0;1;32m    ^\n",
      "\u001b[0m\u001b[1m/tmp/cuda_helper.cu-462e57.hip:381:20: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1muse of undeclared identifier '_aligned_malloc'; did you mean 'aligned_alloc'?\u001b[0m\n",
      "    void* buffer = _aligned_malloc(nbytes, alignment);\n",
      "\u001b[0;1;32m                   ^~~~~~~~~~~~~~~\n",
      "\u001b[0m\u001b[0;32m                   aligned_alloc\n",
      "\u001b[0m\u001b[1m/usr/include/stdlib.h:592:14: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'aligned_alloc' declared here\u001b[0m\n",
      "extern void *aligned_alloc (size_t __alignment, size_t __size)\n",
      "\u001b[0;1;32m             ^\n",
      "\u001b[0m\u001b[1m/tmp/cuda_helper.cu-462e57.hip:383:11: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1mredefinition of 'buffer'\u001b[0m\n",
      "    void* buffer = aligned_alloc(alignment, nbytes);\n",
      "\u001b[0;1;32m          ^\n",
      "\u001b[0m\u001b[1m/tmp/cuda_helper.cu-462e57.hip:381:11: \u001b[0m\u001b[0;1;30mnote: \u001b[0mprevious definition is here\u001b[0m\n",
      "    void* buffer = _aligned_malloc(nbytes, alignment);\n",
      "\u001b[0;1;32m          ^\n",
      "\u001b[0m3 errors generated when compiling for host.\n",
      "Error while processing /tmp/cuda_helper.cu-462e57.hip.\n",
      "\n",
      "[HIPIFY] info: file './cuda_mat_mult/cuda_helper.cu' statistics:\n",
      "\n",
      "  ERROR: Statistics is invalid due to failed hipification.\n",
      "\n",
      "  CONVERTED refs count: 52\n",
      "  UNCONVERTED refs count: 0\n",
      "  CONVERSION %: 100.0\n",
      "  REPLACED bytes: 814\n",
      "  TOTAL bytes: 24629\n",
      "  CHANGED lines of code: 48\n",
      "  TOTAL lines of code: 789\n",
      "  CODE CHANGED (in bytes) %: 3.3\n",
      "  CODE CHANGED (in lines) %: 6.1\n",
      "  TIME ELAPSED s: 0.44\n",
      "[HIPIFY] info: CONVERTED refs by type:\n",
      "  error: 3\n",
      "  init: 1\n",
      "  device: 11\n",
      "  stream: 2\n",
      "  event: 9\n",
      "  execution: 1\n",
      "  include_cuda_main_header: 2\n",
      "  type: 16\n",
      "  numeric_literal: 5\n",
      "  define: 2\n",
      "[HIPIFY] info: CONVERTED refs by API:\n",
      "  CUDA Driver API: 7\n",
      "  CUDA RT API: 45\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  CUDA_SUCCESS: 2\n",
      "  CUresult: 2\n",
      "  cuGetErrorString: 1\n",
      "  cuInit: 1\n",
      "  cuda.h: 1\n",
      "  cudaDevAttrManagedMemory: 1\n",
      "  cudaDeviceGetAttribute: 1\n",
      "  cudaDeviceProp: 2\n",
      "  cudaDeviceReset: 1\n",
      "  cudaDeviceSynchronize: 1\n",
      "  cudaError_t: 2\n",
      "  cudaEventCreate: 2\n",
      "  cudaEventDestroy: 2\n",
      "  cudaEventElapsedTime: 1\n",
      "  cudaEventRecord: 2\n",
      "  cudaEventSynchronize: 2\n",
      "  cudaEvent_t: 3\n",
      "  cudaGetDevice: 1\n",
      "  cudaGetDeviceCount: 2\n",
      "  cudaGetDeviceProperties: 2\n",
      "  cudaGetErrorString: 2\n",
      "  cudaLaunchKernel: 1\n",
      "  cudaSetDevice: 3\n",
      "  cudaStreamCreateWithFlags: 1\n",
      "  cudaStreamDefault: 1\n",
      "  cudaStreamDestroy: 1\n",
      "  cudaStreamNonBlocking: 1\n",
      "  cudaStream_t: 7\n",
      "  cudaSuccess: 2\n",
      "  cuda_runtime.h: 1\n",
      "\u001b[0;1;35mwarning: \u001b[0m\u001b[1mCUDA version is newer than the latest partially supported version 11.8 [-Wunknown-cuda-version]\u001b[0m\n",
      "\n",
      "[HIPIFY] info: file './cuda_mat_mult/mat_mult.cu' statistics:\n",
      "  CONVERTED refs count: 14\n",
      "  UNCONVERTED refs count: 0\n",
      "  CONVERSION %: 100.0\n",
      "  REPLACED bytes: 187\n",
      "  TOTAL bytes: 5943\n",
      "  CHANGED lines of code: 12\n",
      "  TOTAL lines of code: 190\n",
      "  CODE CHANGED (in bytes) %: 3.1\n",
      "  CODE CHANGED (in lines) %: 6.3\n",
      "  TIME ELAPSED s: 0.58\n",
      "[HIPIFY] info: CONVERTED refs by type:\n",
      "  error: 1\n",
      "  device: 1\n",
      "  memory: 9\n",
      "  numeric_literal: 3\n",
      "[HIPIFY] info: CONVERTED refs by API:\n",
      "  CUDA RT API: 14\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  cudaDeviceSynchronize: 1\n",
      "  cudaFree: 3\n",
      "  cudaGetLastError: 1\n",
      "  cudaMalloc: 3\n",
      "  cudaMemcpy: 3\n",
      "  cudaMemcpyDeviceToHost: 1\n",
      "  cudaMemcpyHostToDevice: 2\n",
      "\u001b[0;1;35mwarning: \u001b[0m\u001b[1mCUDA version is newer than the latest partially supported version 11.8 [-Wunknown-cuda-version]\u001b[0m\n",
      "\u001b[1m/tmp/mat_mult-checkpoint.cu-c62df4.hip:11:10: \u001b[0m\u001b[0;1;31mfatal error: \u001b[0m\u001b[1m'mat_size.hpp' file not found\u001b[0m\n",
      "#include \"mat_size.hpp\"\n",
      "\u001b[0;1;32m         ^~~~~~~~~~~~~~\n",
      "\u001b[0mStack dump without symbol names (ensure you have llvm-symbolizer in your PATH or set the environment var `LLVM_SYMBOLIZER_PATH` to point to it):\n",
      "0  hipify-clang 0x00000000023c3016\n",
      "1  hipify-clang 0x00000000023c0914\n",
      "2  libc.so.6    0x00007fd151042520\n",
      "3  hipify-clang 0x0000000000a12510\n",
      "4  hipify-clang 0x00000000019b74f1\n",
      "5  hipify-clang 0x00000000019c8016\n",
      "6  hipify-clang 0x00000000019c9881\n",
      "7  hipify-clang 0x00000000019ca6b8\n",
      "8  hipify-clang 0x0000000001984a6d\n",
      "9  hipify-clang 0x0000000001a03f37\n",
      "10 hipify-clang 0x0000000000bea1af\n",
      "11 hipify-clang 0x0000000000bf69f2\n",
      "12 hipify-clang 0x0000000000c792d6\n",
      "13 hipify-clang 0x0000000000c5e635\n",
      "14 hipify-clang 0x0000000000bf77e7\n",
      "15 hipify-clang 0x0000000000bf90a6\n",
      "16 hipify-clang 0x0000000000be9bca\n",
      "17 hipify-clang 0x0000000000a12343\n",
      "18 hipify-clang 0x0000000000a7e259\n",
      "19 hipify-clang 0x0000000000a928b1\n",
      "20 hipify-clang 0x0000000000be9334\n",
      "21 hipify-clang 0x0000000000be21f5\n",
      "22 hipify-clang 0x0000000000be5588\n",
      "23 hipify-clang 0x0000000000be77a0\n",
      "24 hipify-clang 0x0000000000bdf8c7\n",
      "25 hipify-clang 0x0000000000a26ebf\n",
      "26 libc.so.6    0x00007fd151029d90\n",
      "27 libc.so.6    0x00007fd151029e40 __libc_start_main + 128\n",
      "28 hipify-clang 0x00000000009c7265\n",
      "/opt/rocm-5.6.1/bin/hipexamine.sh: line 23: 10595 Segmentation fault      (core dumped) $SCRIPT_DIR/hipify-clang -examine $hipify_args `$PRIV_SCRIPT_DIR/findcode.sh $SEARCH_DIR` -- -x cuda $clang_args\n"
     ]
    }
   ],
   "source": [
    "!hipexamine.sh ./cuda_mat_mult "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec657b6f-2a15-4ad1-b8ef-650aedeb25fb",
   "metadata": {},
   "source": [
    "### Porting a directory structure inplace\n",
    "\n",
    "Both the **hipconvertinplace-perl.sh** and **hipconvertinplace.sh** scripts have the ability to convert a code tree inplace. The additional option **-hip-kernel-execution-syntax** replaces CUDA triple chevron kernel calls with the equivalent call to **hipLaunchKernelGGL** macro.\n",
    "\n",
    "#### Porting inplace with hipify-perl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23846221-b541-45a8-950b-9278b477b207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  warning: temp_mat_mult/cuda_helper.cu:480: removed identifier \"cudaLaunch\" since CUDA 10.1\n",
      "\n",
      "[HIPIFY] info: file 'temp_mat_mult/cuda_helper.cu' statistics:\n",
      "  CONVERTED refs count: 56\n",
      "  TOTAL lines of code: 789\n",
      "  WARNINGS: 1\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  CUDA_SUCCESS => hipSuccess: 4\n",
      "  CUresult => hipError_t: 4\n",
      "  cuGetErrorString => hipDrvGetErrorString: 1\n",
      "  cuInit => hipInit: 1\n",
      "  cuda.h => hip/hip_runtime.h: 2\n",
      "  cudaDevAttrManagedMemory => hipDeviceAttributeManagedMemory: 1\n",
      "  cudaDeviceGetAttribute => hipDeviceGetAttribute: 1\n",
      "  cudaDeviceProp => hipDeviceProp_t: 2\n",
      "  cudaDeviceReset => hipDeviceReset: 1\n",
      "  cudaDeviceSynchronize => hipDeviceSynchronize: 1\n",
      "  cudaError_t => hipError_t: 4\n",
      "  cudaEventCreate => hipEventCreate: 2\n",
      "  cudaEventDestroy => hipEventDestroy: 2\n",
      "  cudaEventElapsedTime => hipEventElapsedTime: 1\n",
      "  cudaEventRecord => hipEventRecord: 3\n",
      "  cudaEventSynchronize => hipEventSynchronize: 2\n",
      "  cudaEvent_t => hipEvent_t: 3\n",
      "  cudaGetDevice => hipGetDevice: 1\n",
      "  cudaGetDeviceCount => hipGetDeviceCount: 2\n",
      "  cudaGetDeviceProperties => hipGetDeviceProperties: 2\n",
      "  cudaGetErrorString => hipGetErrorString: 2\n",
      "  cudaLaunchKernel => hipLaunchKernel: 1\n",
      "  cudaSetDevice => hipSetDevice: 3\n",
      "  cudaStreamCreateWithFlags => hipStreamCreateWithFlags: 1\n",
      "  cudaStreamDefault => hipStreamDefault: 1\n",
      "  cudaStreamDestroy => hipStreamDestroy: 1\n",
      "  cudaStreamNonBlocking => hipStreamNonBlocking: 1\n",
      "  cudaStream_t => hipStream_t: 7\n",
      "  cudaSuccess => hipSuccess: 4\n",
      "  cuda_runtime.h => hip/hip_runtime.h: 2\n",
      "\n",
      "[HIPIFY] info: file 'temp_mat_mult/mat_mult.cu' statistics:\n",
      "  CONVERTED refs count: 15\n",
      "  TOTAL lines of code: 190\n",
      "  WARNINGS: 0\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  cudaDeviceSynchronize => hipDeviceSynchronize: 1\n",
      "  cudaFree => hipFree: 3\n",
      "  cudaGetLastError => hipGetLastError: 1\n",
      "  cudaMalloc => hipMalloc: 3\n",
      "  cudaMemcpy => hipMemcpy: 3\n",
      "  cudaMemcpyDeviceToHost => hipMemcpyDeviceToHost: 1\n",
      "  cudaMemcpyHostToDevice => hipMemcpyHostToDevice: 2\n",
      "\n",
      "[HIPIFY] info: file 'GLOBAL' statistics:\n",
      "  CONVERTED refs count: 71\n",
      "  TOTAL lines of code: 1162\n",
      "  WARNINGS: 1\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  CUDA_SUCCESS => hipSuccess: 4\n",
      "  CUresult => hipError_t: 4\n",
      "  cuGetErrorString => hipDrvGetErrorString: 1\n",
      "  cuInit => hipInit: 1\n",
      "  cuda.h => hip/hip_runtime.h: 2\n",
      "  cudaDevAttrManagedMemory => hipDeviceAttributeManagedMemory: 1\n",
      "  cudaDeviceGetAttribute => hipDeviceGetAttribute: 1\n",
      "  cudaDeviceProp => hipDeviceProp_t: 2\n",
      "  cudaDeviceReset => hipDeviceReset: 1\n",
      "  cudaDeviceSynchronize => hipDeviceSynchronize: 2\n",
      "  cudaError_t => hipError_t: 4\n",
      "  cudaEventCreate => hipEventCreate: 2\n",
      "  cudaEventDestroy => hipEventDestroy: 2\n",
      "  cudaEventElapsedTime => hipEventElapsedTime: 1\n",
      "  cudaEventRecord => hipEventRecord: 3\n",
      "  cudaEventSynchronize => hipEventSynchronize: 2\n",
      "  cudaEvent_t => hipEvent_t: 3\n",
      "  cudaFree => hipFree: 3\n",
      "  cudaGetDevice => hipGetDevice: 1\n",
      "  cudaGetDeviceCount => hipGetDeviceCount: 2\n",
      "  cudaGetDeviceProperties => hipGetDeviceProperties: 2\n",
      "  cudaGetErrorString => hipGetErrorString: 2\n",
      "  cudaGetLastError => hipGetLastError: 1\n",
      "  cudaLaunchKernel => hipLaunchKernel: 1\n",
      "  cudaMalloc => hipMalloc: 3\n",
      "  cudaMemcpy => hipMemcpy: 3\n",
      "  cudaMemcpyDeviceToHost => hipMemcpyDeviceToHost: 1\n",
      "  cudaMemcpyHostToDevice => hipMemcpyHostToDevice: 2\n",
      "  cudaSetDevice => hipSetDevice: 3\n",
      "  cudaStreamCreateWithFlags => hipStreamCreateWithFlags: 1\n",
      "  cudaStreamDefault => hipStreamDefault: 1\n",
      "  cudaStreamDestroy => hipStreamDestroy: 1\n",
      "  cudaStreamNonBlocking => hipStreamNonBlocking: 1\n",
      "  cudaStream_t => hipStream_t: 7\n",
      "  cudaSuccess => hipSuccess: 4\n",
      "  cuda_runtime.h => hip/hip_runtime.h: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hipconvertinplace-perl.sh temp_mat_mult -exclude-dirs=temp_mat_mult/.ipynb_checkpoints -hip-kernel-execution-syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2326375-a616-4704-81c3-9c3c671151bb",
   "metadata": {},
   "source": [
    "#### Porting inplace with hipify-clang\n",
    "\n",
    "Here is the same port with **hipify-clang**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3be9bcbc-4135-41ab-903d-c048cabbf961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: CUDA version is newer than the latest partially supported version 11.8\n",
      "warning: CUDA version is newer than the latest partially supported version 11.8\n",
      "error: unsupported architecture 'nvptx64' for host compilation\n",
      "\u001b[0;1;35mwarning: \u001b[0m\u001b[1mCUDA version is newer than the latest partially supported version 11.8 [-Wunknown-cuda-version]\u001b[0m\n",
      "In file included from /tmp/cuda_helper.cu-62b330.hip:1:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime.h:64:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime.h:28:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime_api.h:8361:\n",
      "\u001b[1m/opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:2667:34: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'cuCtxDetach' is deprecated [-Wdeprecated-declarations]\u001b[0m\n",
      "    return hipCUResultTohipError(cuCtxDetach(ctx));\n",
      "\u001b[0;1;32m                                 ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:6307:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'cuCtxDetach' has been explicitly marked deprecated here\u001b[0m\n",
      "__CUDA_DEPRECATED CUresult CUDAAPI cuCtxDetach(CUcontext ctx);\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:71:42: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__CUDA_DEPRECATED'\u001b[0m\n",
      "#define __CUDA_DEPRECATED __attribute__((deprecated))\n",
      "\u001b[0;1;32m                                         ^\n",
      "\u001b[0mIn file included from /tmp/cuda_helper.cu-62b330.hip:1:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime.h:64:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime.h:28:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime_api.h:8361:\n",
      "\u001b[1m/opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:2675:34: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'cuDeviceComputeCapability' is deprecated [-Wdeprecated-declarations]\u001b[0m\n",
      "    return hipCUResultTohipError(cuDeviceComputeCapability(major, minor, device));\n",
      "\u001b[0;1;32m                                 ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:4910:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'cuDeviceComputeCapability' has been explicitly marked deprecated here\u001b[0m\n",
      "__CUDA_DEPRECATED CUresult CUDAAPI cuDeviceComputeCapability(int *major, int *minor, CUdevice dev);\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:71:42: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__CUDA_DEPRECATED'\u001b[0m\n",
      "#define __CUDA_DEPRECATED __attribute__((deprecated))\n",
      "\u001b[0;1;32m                                         ^\n",
      "\u001b[0mIn file included from /tmp/cuda_helper.cu-62b330.hip:1:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime.h:64:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime.h:28:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime_api.h:8361:\n",
      "\u001b[1m/opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:2741:34: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'cuModuleGetTexRef' is deprecated [-Wdeprecated-declarations]\u001b[0m\n",
      "    return hipCUResultTohipError(cuModuleGetTexRef(pTexRef, hmod, name));\n",
      "\u001b[0;1;32m                                 ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:6821:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'cuModuleGetTexRef' has been explicitly marked deprecated here\u001b[0m\n",
      "__CUDA_DEPRECATED CUresult CUDAAPI cuModuleGetTexRef(CUtexref *pTexRef, CUmodule hmod, const char *name);\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:71:42: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__CUDA_DEPRECATED'\u001b[0m\n",
      "#define __CUDA_DEPRECATED __attribute__((deprecated))\n",
      "\u001b[0;1;32m                                         ^\n",
      "\u001b[0mIn file included from /tmp/cuda_helper.cu-62b330.hip:1:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime.h:64:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime.h:28:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime_api.h:8361:\n",
      "\u001b[1m/opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:2869:35: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'cudaLaunchCooperativeKernelMultiDevice' is deprecated [-Wdeprecated-declarations]\u001b[0m\n",
      "    return hipCUDAErrorTohipError(cudaLaunchCooperativeKernelMultiDevice(launchParamsList, numDevices, flags));\n",
      "\u001b[0;1;32m                                  ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda_runtime_api.h:4377:8: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'cudaLaunchCooperativeKernelMultiDevice' has been explicitly marked deprecated here\u001b[0m\n",
      "extern __CUDA_DEPRECATED __host__ cudaError_t CUDARTAPI cudaLaunchCooperativeKernelMultiDevice(struct cudaLaunchParams *launchParamsList, unsigned int numDevices, unsigned int flags  __dv(0));\n",
      "\u001b[0;1;32m       ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda_runtime_api.h:265:42: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__CUDA_DEPRECATED'\u001b[0m\n",
      "#define __CUDA_DEPRECATED __attribute__((deprecated))\n",
      "\u001b[0;1;32m                                         ^\n",
      "\u001b[0mIn file included from /tmp/cuda_helper.cu-62b330.hip:1:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime.h:64:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime.h:28:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime_api.h:8361:\n",
      "\u001b[1m/opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:2876:34: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'cuLaunchCooperativeKernelMultiDevice' is deprecated [-Wdeprecated-declarations]\u001b[0m\n",
      "    return hipCUResultTohipError(cuLaunchCooperativeKernelMultiDevice(launchParamsList,\n",
      "\u001b[0;1;32m                                 ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:15803:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'cuLaunchCooperativeKernelMultiDevice' has been explicitly marked deprecated here\u001b[0m\n",
      "__CUDA_DEPRECATED CUresult CUDAAPI cuLaunchCooperativeKernelMultiDevice(CUDA_LAUNCH_PARAMS *launchParamsList, unsigned int numDevices, unsigned int flags);\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:71:42: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__CUDA_DEPRECATED'\u001b[0m\n",
      "#define __CUDA_DEPRECATED __attribute__((deprecated))\n",
      "\u001b[0;1;32m                                         ^\n",
      "\u001b[0mIn file included from /tmp/cuda_helper.cu-62b330.hip:1:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime.h:64:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime.h:28:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime_api.h:8361:\n",
      "\u001b[1m/opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:3165:34: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'cuTexRefSetAddress_v2' is deprecated [-Wdeprecated-declarations]\u001b[0m\n",
      "    return hipCUResultTohipError(cuTexRefSetAddress(ByteOffset,hTexRef,dptr,bytes));\n",
      "\u001b[0;1;32m                                 ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:128:45: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro 'cuTexRefSetAddress'\u001b[0m\n",
      "#define cuTexRefSetAddress                  cuTexRefSetAddress_v2\n",
      "\u001b[0;1;32m                                            ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:19426:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'cuTexRefSetAddress_v2' has been explicitly marked deprecated here\u001b[0m\n",
      "__CUDA_DEPRECATED CUresult CUDAAPI cuTexRefSetAddress(size_t *ByteOffset, CUtexref hTexRef, CUdeviceptr dptr, size_t bytes);\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:71:42: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__CUDA_DEPRECATED'\u001b[0m\n",
      "#define __CUDA_DEPRECATED __attribute__((deprecated))\n",
      "\u001b[0;1;32m                                         ^\n",
      "\u001b[0mIn file included from /tmp/cuda_helper.cu-62b330.hip:1:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime.h:64:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime.h:28:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime_api.h:8361:\n",
      "\u001b[1m/opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:3169:34: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'cuTexRefSetAddress2D_v3' is deprecated [-Wdeprecated-declarations]\u001b[0m\n",
      "    return hipCUResultTohipError(cuTexRefSetAddress2D(hTexRef,desc,dptr,Pitch));\n",
      "\u001b[0;1;32m                                 ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:136:45: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro 'cuTexRefSetAddress2D'\u001b[0m\n",
      "#define cuTexRefSetAddress2D                cuTexRefSetAddress2D_v3\n",
      "\u001b[0;1;32m                                            ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:19481:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'cuTexRefSetAddress2D_v3' has been explicitly marked deprecated here\u001b[0m\n",
      "__CUDA_DEPRECATED CUresult CUDAAPI cuTexRefSetAddress2D(CUtexref hTexRef, const CUDA_ARRAY_DESCRIPTOR *desc, CUdeviceptr dptr, size_t Pitch);\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:71:42: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__CUDA_DEPRECATED'\u001b[0m\n",
      "#define __CUDA_DEPRECATED __attribute__((deprecated))\n",
      "\u001b[0;1;32m                                         ^\n",
      "\u001b[0m\u001b[1m/tmp/cuda_helper.cu-62b330.hip:54:6: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1mredefinition of 'h_errchk'\u001b[0m\n",
      "void h_errchk(hipError_t errcode, const char* message) {\n",
      "\u001b[0;1;32m     ^\n",
      "\u001b[0m\u001b[1m/tmp/cuda_helper.cu-62b330.hip:39:6: \u001b[0m\u001b[0;1;30mnote: \u001b[0mprevious definition is here\u001b[0m\n",
      "void h_errchk(hipError_t errcode, const char* message) {\n",
      "\u001b[0;1;32m     ^\n",
      "\u001b[0m\u001b[1m/tmp/cuda_helper.cu-62b330.hip:96:5: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1munknown type name 'SYSTEM_INFO'\u001b[0m\n",
      "    SYSTEM_INFO sys_info;\n",
      "\u001b[0;1;32m    ^\n",
      "\u001b[0m\u001b[1m/tmp/cuda_helper.cu-62b330.hip:382:20: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1muse of undeclared identifier '_aligned_malloc'; did you mean 'aligned_alloc'?\u001b[0m\n",
      "    void* buffer = _aligned_malloc(nbytes, alignment);\n",
      "\u001b[0;1;32m                   ^~~~~~~~~~~~~~~\n",
      "\u001b[0m\u001b[0;32m                   aligned_alloc\n",
      "\u001b[0m\u001b[1m/usr/include/stdlib.h:592:14: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'aligned_alloc' declared here\u001b[0m\n",
      "extern void *aligned_alloc (size_t __alignment, size_t __size)\n",
      "\u001b[0;1;32m             ^\n",
      "\u001b[0m\u001b[1m/tmp/cuda_helper.cu-62b330.hip:384:11: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1mredefinition of 'buffer'\u001b[0m\n",
      "    void* buffer = aligned_alloc(alignment, nbytes);\n",
      "\u001b[0;1;32m          ^\n",
      "\u001b[0m\u001b[1m/tmp/cuda_helper.cu-62b330.hip:382:11: \u001b[0m\u001b[0;1;30mnote: \u001b[0mprevious definition is here\u001b[0m\n",
      "    void* buffer = _aligned_malloc(nbytes, alignment);\n",
      "\u001b[0;1;32m          ^\n",
      "\u001b[0m7 warnings and 4 errors generated when compiling for host.\n",
      "Error while processing /tmp/cuda_helper.cu-62b330.hip.\n",
      "\n",
      "[HIPIFY] info: file 'temp_mat_mult/cuda_helper.cu' statistics:\n",
      "\n",
      "  ERROR: Statistics is invalid due to failed hipification.\n",
      "\n",
      "  CONVERTED refs count: 0\n",
      "  UNCONVERTED refs count: 0\n",
      "  CONVERSION %: 0.0\n",
      "  REPLACED bytes: 0\n",
      "  TOTAL bytes: 24660\n",
      "  CHANGED lines of code: 1\n",
      "  TOTAL lines of code: 790\n",
      "  CODE CHANGED (in bytes) %: 0.0\n",
      "  CODE CHANGED (in lines) %: 0.1\n",
      "  TIME ELAPSED s: 0.57\n",
      "\u001b[0;1;35mwarning: \u001b[0m\u001b[1mCUDA version is newer than the latest partially supported version 11.8 [-Wunknown-cuda-version]\u001b[0m\n",
      "In file included from /tmp/mat_mult.cu-1324d7.hip:1:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime.h:64:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime.h:28:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime_api.h:8361:\n",
      "\u001b[1m/opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:2667:34: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'cuCtxDetach' is deprecated [-Wdeprecated-declarations]\u001b[0m\n",
      "    return hipCUResultTohipError(cuCtxDetach(ctx));\n",
      "\u001b[0;1;32m                                 ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:6307:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'cuCtxDetach' has been explicitly marked deprecated here\u001b[0m\n",
      "__CUDA_DEPRECATED CUresult CUDAAPI cuCtxDetach(CUcontext ctx);\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:71:42: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__CUDA_DEPRECATED'\u001b[0m\n",
      "#define __CUDA_DEPRECATED __attribute__((deprecated))\n",
      "\u001b[0;1;32m                                         ^\n",
      "\u001b[0mIn file included from /tmp/mat_mult.cu-1324d7.hip:1:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime.h:64:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime.h:28:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime_api.h:8361:\n",
      "\u001b[1m/opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:2675:34: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'cuDeviceComputeCapability' is deprecated [-Wdeprecated-declarations]\u001b[0m\n",
      "    return hipCUResultTohipError(cuDeviceComputeCapability(major, minor, device));\n",
      "\u001b[0;1;32m                                 ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:4910:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'cuDeviceComputeCapability' has been explicitly marked deprecated here\u001b[0m\n",
      "__CUDA_DEPRECATED CUresult CUDAAPI cuDeviceComputeCapability(int *major, int *minor, CUdevice dev);\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:71:42: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__CUDA_DEPRECATED'\u001b[0m\n",
      "#define __CUDA_DEPRECATED __attribute__((deprecated))\n",
      "\u001b[0;1;32m                                         ^\n",
      "\u001b[0mIn file included from /tmp/mat_mult.cu-1324d7.hip:1:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime.h:64:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime.h:28:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime_api.h:8361:\n",
      "\u001b[1m/opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:2741:34: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'cuModuleGetTexRef' is deprecated [-Wdeprecated-declarations]\u001b[0m\n",
      "    return hipCUResultTohipError(cuModuleGetTexRef(pTexRef, hmod, name));\n",
      "\u001b[0;1;32m                                 ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:6821:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'cuModuleGetTexRef' has been explicitly marked deprecated here\u001b[0m\n",
      "__CUDA_DEPRECATED CUresult CUDAAPI cuModuleGetTexRef(CUtexref *pTexRef, CUmodule hmod, const char *name);\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:71:42: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__CUDA_DEPRECATED'\u001b[0m\n",
      "#define __CUDA_DEPRECATED __attribute__((deprecated))\n",
      "\u001b[0;1;32m                                         ^\n",
      "\u001b[0mIn file included from /tmp/mat_mult.cu-1324d7.hip:1:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime.h:64:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime.h:28:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime_api.h:8361:\n",
      "\u001b[1m/opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:2869:35: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'cudaLaunchCooperativeKernelMultiDevice' is deprecated [-Wdeprecated-declarations]\u001b[0m\n",
      "    return hipCUDAErrorTohipError(cudaLaunchCooperativeKernelMultiDevice(launchParamsList, numDevices, flags));\n",
      "\u001b[0;1;32m                                  ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda_runtime_api.h:4377:8: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'cudaLaunchCooperativeKernelMultiDevice' has been explicitly marked deprecated here\u001b[0m\n",
      "extern __CUDA_DEPRECATED __host__ cudaError_t CUDARTAPI cudaLaunchCooperativeKernelMultiDevice(struct cudaLaunchParams *launchParamsList, unsigned int numDevices, unsigned int flags  __dv(0));\n",
      "\u001b[0;1;32m       ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda_runtime_api.h:265:42: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__CUDA_DEPRECATED'\u001b[0m\n",
      "#define __CUDA_DEPRECATED __attribute__((deprecated))\n",
      "\u001b[0;1;32m                                         ^\n",
      "\u001b[0mIn file included from /tmp/mat_mult.cu-1324d7.hip:1:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime.h:64:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime.h:28:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime_api.h:8361:\n",
      "\u001b[1m/opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:2876:34: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'cuLaunchCooperativeKernelMultiDevice' is deprecated [-Wdeprecated-declarations]\u001b[0m\n",
      "    return hipCUResultTohipError(cuLaunchCooperativeKernelMultiDevice(launchParamsList,\n",
      "\u001b[0;1;32m                                 ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:15803:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'cuLaunchCooperativeKernelMultiDevice' has been explicitly marked deprecated here\u001b[0m\n",
      "__CUDA_DEPRECATED CUresult CUDAAPI cuLaunchCooperativeKernelMultiDevice(CUDA_LAUNCH_PARAMS *launchParamsList, unsigned int numDevices, unsigned int flags);\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:71:42: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__CUDA_DEPRECATED'\u001b[0m\n",
      "#define __CUDA_DEPRECATED __attribute__((deprecated))\n",
      "\u001b[0;1;32m                                         ^\n",
      "\u001b[0mIn file included from /tmp/mat_mult.cu-1324d7.hip:1:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime.h:64:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime.h:28:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime_api.h:8361:\n",
      "\u001b[1m/opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:3165:34: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'cuTexRefSetAddress_v2' is deprecated [-Wdeprecated-declarations]\u001b[0m\n",
      "    return hipCUResultTohipError(cuTexRefSetAddress(ByteOffset,hTexRef,dptr,bytes));\n",
      "\u001b[0;1;32m                                 ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:128:45: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro 'cuTexRefSetAddress'\u001b[0m\n",
      "#define cuTexRefSetAddress                  cuTexRefSetAddress_v2\n",
      "\u001b[0;1;32m                                            ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:19426:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'cuTexRefSetAddress_v2' has been explicitly marked deprecated here\u001b[0m\n",
      "__CUDA_DEPRECATED CUresult CUDAAPI cuTexRefSetAddress(size_t *ByteOffset, CUtexref hTexRef, CUdeviceptr dptr, size_t bytes);\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:71:42: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__CUDA_DEPRECATED'\u001b[0m\n",
      "#define __CUDA_DEPRECATED __attribute__((deprecated))\n",
      "\u001b[0;1;32m                                         ^\n",
      "\u001b[0mIn file included from /tmp/mat_mult.cu-1324d7.hip:1:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime.h:64:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime.h:28:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime_api.h:8361:\n",
      "\u001b[1m/opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:3169:34: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'cuTexRefSetAddress2D_v3' is deprecated [-Wdeprecated-declarations]\u001b[0m\n",
      "    return hipCUResultTohipError(cuTexRefSetAddress2D(hTexRef,desc,dptr,Pitch));\n",
      "\u001b[0;1;32m                                 ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:136:45: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro 'cuTexRefSetAddress2D'\u001b[0m\n",
      "#define cuTexRefSetAddress2D                cuTexRefSetAddress2D_v3\n",
      "\u001b[0;1;32m                                            ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:19481:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'cuTexRefSetAddress2D_v3' has been explicitly marked deprecated here\u001b[0m\n",
      "__CUDA_DEPRECATED CUresult CUDAAPI cuTexRefSetAddress2D(CUtexref hTexRef, const CUDA_ARRAY_DESCRIPTOR *desc, CUdeviceptr dptr, size_t Pitch);\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:71:42: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__CUDA_DEPRECATED'\u001b[0m\n",
      "#define __CUDA_DEPRECATED __attribute__((deprecated))\n",
      "\u001b[0;1;32m                                         ^\n",
      "\u001b[0mIn file included from /tmp/mat_mult.cu-1324d7.hip:18:\n",
      "\u001b[1m/home/toby/Pelagos/Projects/HIP_Course/course_material/L9_Porting_CUDA_to_HIP/temp_mat_mult/cuda_helper.cu:54:6: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1mredefinition of 'h_errchk'\u001b[0m\n",
      "void h_errchk(hipError_t errcode, const char* message) {\n",
      "\u001b[0;1;32m     ^\n",
      "\u001b[0m\u001b[1m/home/toby/Pelagos/Projects/HIP_Course/course_material/L9_Porting_CUDA_to_HIP/temp_mat_mult/cuda_helper.cu:39:6: \u001b[0m\u001b[0;1;30mnote: \u001b[0mprevious definition is here\u001b[0m\n",
      "void h_errchk(hipError_t errcode, const char* message) {\n",
      "\u001b[0;1;32m     ^\n",
      "\u001b[0m7 warnings and 1 error generated when compiling for host.\n",
      "Error while processing /tmp/mat_mult.cu-1324d7.hip.\n",
      "\n",
      "[HIPIFY] info: file 'temp_mat_mult/mat_mult.cu' statistics:\n",
      "\n",
      "  ERROR: Statistics is invalid due to failed hipification.\n",
      "\n",
      "  CONVERTED refs count: 1\n",
      "  UNCONVERTED refs count: 0\n",
      "  CONVERSION %: 100.0\n",
      "  REPLACED bytes: 106\n",
      "  TOTAL bytes: 5974\n",
      "  CHANGED lines of code: 2\n",
      "  TOTAL lines of code: 191\n",
      "  CODE CHANGED (in bytes) %: 1.8\n",
      "  CODE CHANGED (in lines) %: 1.0\n",
      "  TIME ELAPSED s: 0.72\n",
      "[HIPIFY] info: CONVERTED refs by type:\n",
      "  kernel_launch: 1\n",
      "[HIPIFY] info: CONVERTED refs by API:\n",
      "  CUDA RT API: 1\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  cudaLaunchKernel: 1\n",
      "\u001b[0;1;35mwarning: \u001b[0m\u001b[1mCUDA version is newer than the latest partially supported version 11.8 [-Wunknown-cuda-version]\u001b[0m\n",
      "\u001b[1m/tmp/mat_helper.hpp-50ca2a.hip:162:5: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1muse of undeclared identifier 'assert'\u001b[0m\n",
      "    assert(len0_src>=K0);\n",
      "\u001b[0;1;32m    ^\n",
      "\u001b[0m\u001b[1m/tmp/mat_helper.hpp-50ca2a.hip:163:5: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1muse of undeclared identifier 'assert'\u001b[0m\n",
      "    assert(len1_src>=K1);    \n",
      "\u001b[0;1;32m    ^\n",
      "\u001b[0m2 errors generated when compiling for host.\n",
      "Error while processing /tmp/mat_helper.hpp-50ca2a.hip.\n",
      "\n",
      "[HIPIFY] info: file 'temp_mat_mult/mat_helper.hpp' statistics:\n",
      "\n",
      "  ERROR: Statistics is invalid due to failed hipification.\n",
      "\n",
      "  CONVERTED refs count: 0\n",
      "  UNCONVERTED refs count: 0\n",
      "  CONVERSION %: 0.0\n",
      "  REPLACED bytes: 0\n",
      "  TOTAL bytes: 4497\n",
      "  CHANGED lines of code: 1\n",
      "  TOTAL lines of code: 180\n",
      "  CODE CHANGED (in bytes) %: 0.0\n",
      "  CODE CHANGED (in lines) %: 0.6\n",
      "  TIME ELAPSED s: 0.47\n",
      "\u001b[0;1;35mwarning: \u001b[0m\u001b[1mCUDA version is newer than the latest partially supported version 11.8 [-Wunknown-cuda-version]\u001b[0m\n",
      "\n",
      "[HIPIFY] info: file 'temp_mat_mult/mat_size.hpp' statistics:\n",
      "  CONVERTED refs count: 0\n",
      "  UNCONVERTED refs count: 0\n",
      "  CONVERSION %: 0.0\n",
      "  REPLACED bytes: 0\n",
      "  TOTAL bytes: 107\n",
      "  CHANGED lines of code: 1\n",
      "  TOTAL lines of code: 3\n",
      "  CODE CHANGED (in bytes) %: 0.0\n",
      "  CODE CHANGED (in lines) %: 33.3\n",
      "  TIME ELAPSED s: 0.19\n",
      "\n",
      "[HIPIFY] info: file 'GLOBAL' statistics:\n",
      "\n",
      "  ERROR: Statistics is invalid due to failed hipification.\n",
      "\n",
      "  CONVERTED refs count: 1\n",
      "  UNCONVERTED refs count: 0\n",
      "  CONVERSION %: 100.0\n",
      "  REPLACED bytes: 106\n",
      "  TOTAL bytes: 35238\n",
      "  CHANGED lines of code: 5\n",
      "  TOTAL lines of code: 1164\n",
      "  CODE CHANGED (in bytes) %: 0.3\n",
      "  CODE CHANGED (in lines) %: 0.4\n",
      "  TIME ELAPSED s: 1.95\n",
      "[HIPIFY] info: CONVERTED refs by type:\n",
      "  kernel_launch: 1\n",
      "[HIPIFY] info: CONVERTED refs by API:\n",
      "  CUDA RT API: 1\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  cudaLaunchKernel: 1\n",
      "\n",
      "[HIPIFY] info: TOTAL statistics:\n",
      "  CONVERTED files: 1\n",
      "  PROCESSED files: 4\n"
     ]
    }
   ],
   "source": [
    "!hipconvertinplace.sh temp_mat_mult -hip-kernel-execution-syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc505b5e-1291-4007-86e8-ae027dfca4cd",
   "metadata": {},
   "source": [
    "#### Building the ported code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f3bf73-e76a-49cc-b006-4b90e1eed6b2",
   "metadata": {},
   "source": [
    "If we examine the source tree we see that every source file that has been hipified has been first copied to a file with suffix `*.prehip`. Then the converted code is overwritten in place of the old file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93c79b1c-e721-4421-9183-a44ed01ccedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 868\n",
      "-rw-rw-r-- 1 toby toby 262144 Sep 25 12:13 array_A.dat\n",
      "-rw-rw-r-- 1 toby toby 262144 Sep 25 12:13 array_B.dat\n",
      "-rw-rw-r-- 1 toby toby 262144 Sep 25 12:13 array_C.dat\n",
      "-rw-rw-r-- 1 toby toby  24660 Sep 25 12:14 cuda_helper.cu\n",
      "-rw-rw-r-- 1 toby toby  24629 Sep 25 12:14 cuda_helper.cu.prehip\n",
      "-rw-rw-r-- 1 toby toby    271 Sep 25 12:13 Makefile\n",
      "-rw-rw-r-- 1 toby toby   4497 Sep 25 12:14 mat_helper.hpp\n",
      "-rw-rw-r-- 1 toby toby   4497 Sep 25 12:14 mat_helper.hpp.prehip\n",
      "-rw-rw-r-- 1 toby toby   5974 Sep 25 12:14 mat_mult.cu\n",
      "-rw-rw-r-- 1 toby toby   5943 Sep 25 12:13 mat_mult.cu.prehip\n",
      "-rw-rw-r-- 1 toby toby    137 Sep 25 12:14 mat_size.hpp\n",
      "-rw-rw-r-- 1 toby toby    107 Sep 25 12:14 mat_size.hpp.prehip\n"
     ]
    }
   ],
   "source": [
    "!ls -l temp_mat_mult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2964cedf-53df-49d3-a98a-812a59e34aeb",
   "metadata": {},
   "source": [
    "Try making the ported code with hipcc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1e07138-163e-4615-93fe-62b653908f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -r *.exe\n",
      "rm: cannot remove '*.exe': No such file or directory\n",
      "make: *** [Makefile:19: clean] Error 1\n",
      "hipcc -g -O2  mat_mult.cu -o mat_mult.exe \n",
      "\u001b[01m\u001b[0m\u001b[01mcuda_helper.cu(54)\u001b[0m: \u001b[01;31merror\u001b[0m: function \u001b[01m\"h_errchk\"\u001b[0m has already been defined\n",
      "  void h_errchk(hipError_t errcode, const char* message) {\n",
      "       ^\n",
      "\n",
      "1 error detected in the compilation of \"mat_mult.cu\".\n",
      "make: *** [Makefile:15: mat_mult.exe] Error 2\n"
     ]
    }
   ],
   "source": [
    "!cd temp_mat_mult; make clean; make CXX=\"hipcc\" LIBFLAGS=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39501460-ca62-4103-be05-aa00069beaaf",
   "metadata": {},
   "source": [
    "In the original file **cuda_mat_mult/cuda_helper.cu** we had overloaded the **h_errchk** function to accept errorcodes of both type **CUResult** and **cudaError_t**. Following conversion to HIP the errorcode has been replaced with just **hipError_t**. Therefore we need to manually delete the duplicate **h_errchk** function in **[temp_mat_mult/cuda_helper.cu.prehip](temp_mat_mult/cuda_helper.cu.prehip)**. Then rerun the conversion and the make. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9142234b-e1d6-4a1c-b387-c679b25df42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  warning: cuda_helper.cu:451: removed identifier \"cudaLaunch\" since CUDA 10.1\n",
      "hipcc -g -O2  mat_mult.cu -o mat_mult.exe \n",
      "Device id: 0\n",
      "\tname:                                    NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "\tglobal memory size:                      6226 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    49 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,64)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65535,65535)\n",
      "Maximum error (infinity norm) is: 1.52588e-05\n"
     ]
    }
   ],
   "source": [
    "!cd temp_mat_mult; hipify-perl -inplace -hip-kernel-execution-syntax cuda_helper.cu\n",
    "!cd temp_mat_mult; make CXX=\"hipcc\" LIBFLAGS=\"\"; ./mat_mult.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218d61e3-fdc4-4dd1-8ac1-e537872577fb",
   "metadata": {},
   "source": [
    "Now we should have a successful port of the CUDA code to HIP!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2d5ccc-66a6-4dff-a76e-2b017591f613",
   "metadata": {},
   "source": [
    "## API differences between CUDA and HIP\n",
    "\n",
    "CUDA has the notion of a driver API and a runtime API. HIP combines the two into one API and then supports a subset of the combined API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42ee95-4dca-44f1-8c96-fd47d1bfd65d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
