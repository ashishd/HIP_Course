{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "916435d8-e663-4bd0-b3e2-8d8344fd6830",
   "metadata": {},
   "source": [
    "# Porting CUDA programs to HIP\n",
    "\n",
    "HIP API calls are designed to closely match their CUDA equivalents. This enables HIP to function as a thin layer over CUDA and allows for reasonably easy porting of CUDA code to HIP code. Often it is just a matter of replacing **cuda -> hip** in the function calls. The ROCM suite provides two different tools **hipify-perl** and **hipify-clang** to help with the porting process. The tool **hipify-perl** is robust and uses perl to perform an intelligent search and replace of cuda calls with hip calls, while the **hipify-clang** tool uses the clang preprocessor to produce a high quality port. The perl-based method is better for quick ports of small codes, while the clang-based method is intended for ports of large codebases. The hipify-clang tool is much more picky though and fails easily unless it has access to all the header files used in the compilation of the CUDA code.\n",
    "\n",
    "## Supported API's\n",
    "\n",
    "A large subset of CUDA API calls are supported by HIP, including those in supporting libraries like **cuBLAS**. Tables in [this Github site](https://github.com/ROCm-Developer-Tools/HIPIFY/blob/amd-staging/docs/supported_apis.md) provides some guidance as to what is supported."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aef7d88-273f-4ee5-8a56-a7a4fb64cf2f",
   "metadata": {},
   "source": [
    "## Setup and installation\n",
    "\n",
    "From [this source](https://sep5.readthedocs.io/en/latest/Programming_Guides/HIP-porting-guide.html) it is recommended to attempt porting on a machine that has access to both CUDA and HIP libraries. This usually means doing the port on a machine with an NVIDIA GPU. Then one can try porting portions of the code at a time and compare results. For best results with hipify-clang you need to have a version of CUDA that is compatible with your installed version of hipify-clang. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "679e253f-cc77-40e8-826d-cc8e904394e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMD LLVM version 16.0.0git\n",
      "  Optimized build.\n"
     ]
    }
   ],
   "source": [
    "!hipify-clang --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3341127e-3ad1-45b6-bb3f-0624e4d60c33",
   "metadata": {},
   "source": [
    "Here is a page which describes compatibility between CUDA and hipify-clang.\n",
    "\n",
    "[HIPIFY Documentation](https://rocm.docs.amd.com/projects/HIPIFY/en/latest/hipify-clang.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d427b5-af01-452e-8bb8-b72c57e47ca7",
   "metadata": {},
   "source": [
    "## Trial setup\n",
    "\n",
    "There are two sub-directories in this module:\n",
    "\n",
    "* cuda_mat_mult\n",
    "* hip_mat_mult\n",
    "\n",
    "In the directory **cuda_mat_mult** is a CUDA version of the HIP matrix multiplication code in **hip_mat_mult**. It was manually ported from HIP to CUDA. We are going to use the HIP tools to try and port back the CUDA code to HIP code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158aeee9-6c9b-452d-8ab6-510f9d30850f",
   "metadata": {},
   "source": [
    "## Porting techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea2199-4187-41fa-8f26-2ef36342b579",
   "metadata": {},
   "source": [
    "### Port a single file\n",
    "\n",
    "Let's first make a temporary of **cuda_mat_mult** for the purpose of conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "454c9ad4-cf6f-4c41-9718-3f0111b8fc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p temp_mat_mult; cp -r cuda_mat_mult/* temp_mat_mult/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff01866-4aa5-456c-b250-59bb193fb640",
   "metadata": {},
   "source": [
    "The **hipify-perl** command can port a single file to use the HIP API. We use it to port the file **mat_mult.cu** in the directory **temp_mat_mult**. The flag `-hip-kernel-execution-syntax` changes kernel launch syntax from the CUDA-style triple Chevron `<<< >>>` method to the ANSI C++ compliant method of **hipLaunchKernelGGL**. The following command dumps the output to the command line, but you can use the `-o` flag to specify an output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d9a82eaa-6360-4b8c-8c96-7c121b8fb5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#include \"hip/hip_runtime.h\"\n",
      "#include \"hip/hip_runtime.h\"\n",
      "/* Code to perform a Matrix multiplication using cuda\n",
      "Written by Dr Toby M. Potter\n",
      "*/\n",
      "\n",
      "// Setup headers\n",
      "#include <cassert>\n",
      "#include <cmath>\n",
      "#include <iostream>\n",
      "\n",
      "// Bring in the size of the matrices\n",
      "#include \"mat_size.hpp\"\n",
      "\n",
      "// Bring in a library to manage matrices on the CPU\n",
      "#include \"mat_helper.hpp\"\n",
      "\n",
      "// Bring in helper header to manage boilerplate code\n",
      "#include \"cuda_helper.hpp\"\n",
      "\n",
      "// standard matrix multiply kernel \n",
      "__global__ void mat_mult (\n",
      "        float* A, \n",
      "        float* B, \n",
      "        float* C, \n",
      "        size_t N1_A, \n",
      "        size_t N0_C,\n",
      "        size_t N1_C) { \n",
      "            \n",
      "    // A is of size (N0_C, N1_A)\n",
      "    // B is of size (N1_A, N1_C)\n",
      "    // C is of size (N0_C, N1_C)   \n",
      "    \n",
      "    // i0 and i1 represent the coordinates in Matrix C \n",
      "    // We use row-major ordering for the matrices\n",
      "    \n",
      "    size_t i0 = blockIdx.y * blockDim.y + threadIdx.y;\n",
      "    size_t i1 = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "    \n",
      "    // Scratch variable\n",
      "    float temp=0.0f; \n",
      "\n",
      "    // Guard mechanism to make sure we do not go\n",
      "    // outside the boundaries of matrix C \n",
      "    if ((i0<N0_C) && (i1<N1_C)) {\n",
      "        // Get the offset within the memory allocation of C\n",
      "        size_t offset = i0*N1_C+i1;\n",
      "        \n",
      "        // Loop over columns of A and rows of B\n",
      "        for (size_t n=0; n<N1_A; n++) {\n",
      "            \n",
      "            // A is of size (N0_C, N1_A)\n",
      "            // B is of size (N1_A, N1_C)\n",
      "            \n",
      "            // Loop across row i0 of A\n",
      "            // and down column i1 of B\n",
      "            temp+=A[i0*N1_A+n]*B[i1+n*N1_C]; \n",
      "        }\n",
      "        \n",
      "        // Set the value in C at offset\n",
      "        C[offset]=temp;\n",
      "        \n",
      "        // Uncomment this to perform elementwise matrix multiplication instead\n",
      "        // C[offset]=A[offset]*B[offset];\n",
      "    }\n",
      "} \n",
      "\n",
      "int main(int argc, char** argv) {\n",
      "    \n",
      "    //// Step 1. Parse program arguments ////\n",
      "\n",
      "    // Parse command line arguments\n",
      "    int dev_index = h_parse_args(argc, argv);\n",
      "    \n",
      "    // Number of devices discovered\n",
      "    int num_devices=0;\n",
      "    \n",
      "    //// Step 2. Discover resources and choose a compute device ////\n",
      "    \n",
      "    // Helper function to acquire devices\n",
      "    // This sets the default device\n",
      "    h_acquire_devices(&num_devices, dev_index);\n",
      "        \n",
      "    // Report on the device in use\n",
      "    h_report_on_device(dev_index);\n",
      "    \n",
      "    // We are going to do a simple array multiplication for this example, \n",
      "    // using raw binary files for input and output\n",
      "    \n",
      "    // A is of size (N0_C, N1_A)\n",
      "    // B is of size (N1_A, N1_C)    \n",
      "    // C is of size (N0_C, N1_C)\n",
      "\n",
      "    size_t N1_A = NCOLS_A, N0_C = NROWS_C, N1_C = NCOLS_C;\n",
      "\n",
      "    //// Step 3. Construct matrices A_h and B_h on the host \n",
      "    //// and fill them with random numbers ////\n",
      "    \n",
      "    // Number of bytes in each array\n",
      "    size_t nbytes_A = N0_C*N1_A*sizeof(float);\n",
      "    size_t nbytes_B = N1_A*N1_C*sizeof(float);\n",
      "    size_t nbytes_C = N0_C*N1_C*sizeof(float);\n",
      "\n",
      "    // Allocate memory for the host arrays\n",
      "    float* A_h = (float*)h_alloc(nbytes_A);\n",
      "    float* B_h = (float*)h_alloc(nbytes_B);\n",
      "    float* C_h = (float*)h_alloc(nbytes_C);\n",
      "\n",
      "    // Fill the host arrays with random numbers \n",
      "    // using the matrix helper library\n",
      "    m_random(A_h, N0_C, N1_A);\n",
      "    m_random(B_h, N1_A, N1_C);\n",
      "    \n",
      "    //// Step 4. Allocate memory for arrays //// \n",
      "    //// A_d, B_d, and C_d on the compute device ////\n",
      "\n",
      "    float *A_d, *B_d, *C_d;\n",
      "    H_ERRCHK(hipMalloc((void**)&A_d, nbytes_A));\n",
      "    H_ERRCHK(hipMalloc((void**)&B_d, nbytes_B));\n",
      "    H_ERRCHK(hipMalloc((void**)&C_d, nbytes_C));\n",
      "\n",
      "    //// Step 5. 1. Upload matrices A_h and B_h from the host //// \n",
      "    //// to A_d and B_d on the device ////\n",
      "    H_ERRCHK(hipMemcpy(A_d, A_h, nbytes_A, hipMemcpyHostToDevice));\n",
      "    H_ERRCHK(hipMemcpy(B_d, B_h, nbytes_B, hipMemcpyHostToDevice));\n",
      " \n",
      "    //// Step 6. Run the kernel to compute C_d ///\n",
      "    //// from A_d and B_d on the device ////\n",
      "        \n",
      "    // Desired block size\n",
      "    dim3 block_size = { 8, 8, 1 };\n",
      "    dim3 global_size = { (uint32_t)N1_C, (uint32_t)N0_C, 1 };\n",
      "    dim3 grid_nblocks;\n",
      "    \n",
      "    // Choose the number of blocks so that Grid fits within it.\n",
      "    h_fit_blocks(&grid_nblocks, global_size, block_size);\n",
      "\n",
      "    // Amount of shared memory to use in the kernel\n",
      "    size_t sharedMemBytes=0;\n",
      "    \n",
      "    // Launch the kernel using CUDA triple Chevron syntax\n",
      "    // Use 0 when choosing the default (null) stream\n",
      "    hipLaunchKernelGGL(mat_mult, grid_nblocks, block_size, sharedMemBytes, 0, A_d, B_d, C_d, N1_A, N0_C, N1_C);\n",
      "    \n",
      "    // Check the status of the kernel launch\n",
      "    H_ERRCHK(hipGetLastError());\n",
      "    \n",
      "    // Wait for any commands to complete on the compute device\n",
      "    H_ERRCHK(hipDeviceSynchronize());\n",
      "\n",
      "    //// Step 7. Copy the buffer for matrix C_d //// \n",
      "    //// on the device back to C_h on the host ////\n",
      "    H_ERRCHK(hipMemcpy((void*)C_h, (const void*)C_d, nbytes_C, hipMemcpyDeviceToHost));\n",
      "    \n",
      "    //// Step 8. Test the computed matrix **C_h** against a known answer\n",
      "    \n",
      "    // Compute the serial solution using the matrix helper library\n",
      "    float* C_answer_h = (float*)calloc(nbytes_C, 1);\n",
      "    m_mat_mult(A_h, B_h, C_answer_h, N1_A, N0_C, N1_C);\n",
      "    \n",
      "    // Uncomment this to check against elementwise matrix multiplication\n",
      "    // m_hadamard(A_h, B_h, C_answer_h, N0_C, N1_C);\n",
      "\n",
      "    // Print the maximum error between matrices\n",
      "    float max_err = m_max_error(C_h, C_answer_h, N0_C, N1_C);\n",
      "    \n",
      "    //// Step 9. Write the contents of matrices A_h, B_h, and C_h to disk ////\n",
      "\n",
      "    // Write out the host arrays to file\n",
      "    h_write_binary(A_h, \"array_A.dat\", nbytes_A);\n",
      "    h_write_binary(B_h, \"array_B.dat\", nbytes_B);\n",
      "    h_write_binary(C_h, \"array_C.dat\", nbytes_C);\n",
      "    \n",
      "    //// Step 10. Clean up memory alllocations and release resources\n",
      "    \n",
      "    // Free the cuda buffers\n",
      "    H_ERRCHK(hipFree(A_d));\n",
      "    H_ERRCHK(hipFree(B_d));\n",
      "    H_ERRCHK(hipFree(C_d));\n",
      "\n",
      "    // Clean up host memory\n",
      "    free(A_h);\n",
      "    free(B_h);\n",
      "    free(C_h);\n",
      "\n",
      "    // Free the answer matrix\n",
      "    free(C_answer_h);\n",
      "    \n",
      "    // Reset compute devices\n",
      "    h_reset_devices(num_devices);\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cd temp_mat_mult; hipify-perl -hip-kernel-execution-syntax mat_mult.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747905a4-2e43-4269-900f-de3bd4205f5f",
   "metadata": {},
   "source": [
    "If we use the `-inplace` flag, **hipify-perl** copies the file [mat_mult.cpp](temp_mat_mult/mat_mult.cpp) first to [mat_mult.cpp.prehip](temp_mat_mult/mat_mult.cpp.prehip) **if that file doesn't already exist**. Then it performs the conversion from [mat_mult.cpp.prehip](temp_mat_mult/mat_mult.cpp.prehip) to [mat_mult.cpp](temp_mat_mult/mat_mult.cpp). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4704d730-6ab7-4216-89d9-6b13eeb0024e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HIPIFY] info: file 'mat_mult.cpp' statistics:\n",
      "  CONVERTED refs count: 15\n",
      "  TOTAL lines of code: 190\n",
      "  WARNINGS: 0\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  cudaDeviceSynchronize => hipDeviceSynchronize: 1\n",
      "  cudaFree => hipFree: 3\n",
      "  cudaGetLastError => hipGetLastError: 1\n",
      "  cudaMalloc => hipMalloc: 3\n",
      "  cudaMemcpy => hipMemcpy: 3\n",
      "  cudaMemcpyDeviceToHost => hipMemcpyDeviceToHost: 1\n",
      "  cudaMemcpyHostToDevice => hipMemcpyHostToDevice: 2\n"
     ]
    }
   ],
   "source": [
    "!cd temp_mat_mult; hipify-perl -inplace -print-stats -hip-kernel-execution-syntax mat_mult.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498f4116-e38e-4113-a148-844a92d8b83a",
   "metadata": {},
   "source": [
    "Subsequent edits to [mat_mult.cpp.prehip](temp_mat_mult/mat_mult.cpp.prehip) will be propagated across to [mat_mult.cpp](temp_mat_mult/mat_mult.cpp). This allows for an iterative porting process. Use the `--help` flag for more porting options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f4a7d3-8c4d-431b-901a-870916d81a58",
   "metadata": {},
   "source": [
    "### Examine a directory structure for porting potential\n",
    "\n",
    "We use the scripts **hipexamine-perl.sh** or **hipexamine.sh** to recursively search through a directory and examine the potential for porting a code. Note the summary that is produced for each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "12620b08-509c-4d8b-ae80-8534892a134b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HIPIFY] info: file 'cuda_mat_mult/mat_mult.cpp' statistics:\n",
      "  CONVERTED refs count: 14\n",
      "  TOTAL lines of code: 190\n",
      "  WARNINGS: 0\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  cudaDeviceSynchronize => hipDeviceSynchronize: 1\n",
      "  cudaFree => hipFree: 3\n",
      "  cudaGetLastError => hipGetLastError: 1\n",
      "  cudaMalloc => hipMalloc: 3\n",
      "  cudaMemcpy => hipMemcpy: 3\n",
      "  cudaMemcpyDeviceToHost => hipMemcpyDeviceToHost: 1\n",
      "  cudaMemcpyHostToDevice => hipMemcpyHostToDevice: 2\n",
      "  warning: cuda_mat_mult/cuda_helper.hpp:480: removed identifier \"cudaLaunch\" since CUDA 10.1\n",
      "\n",
      "[HIPIFY] info: file 'cuda_mat_mult/cuda_helper.hpp' statistics:\n",
      "  CONVERTED refs count: 55\n",
      "  TOTAL lines of code: 789\n",
      "  WARNINGS: 1\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  CUDA_SUCCESS => hipSuccess: 4\n",
      "  CUresult => hipError_t: 4\n",
      "  cuGetErrorString => hipDrvGetErrorString: 1\n",
      "  cuInit => hipInit: 1\n",
      "  cuda.h => hip/hip_runtime.h: 2\n",
      "  cudaDevAttrManagedMemory => hipDeviceAttributeManagedMemory: 1\n",
      "  cudaDeviceGetAttribute => hipDeviceGetAttribute: 1\n",
      "  cudaDeviceProp => hipDeviceProp_t: 2\n",
      "  cudaDeviceReset => hipDeviceReset: 1\n",
      "  cudaDeviceSynchronize => hipDeviceSynchronize: 1\n",
      "  cudaError_t => hipError_t: 4\n",
      "  cudaEventCreate => hipEventCreate: 2\n",
      "  cudaEventDestroy => hipEventDestroy: 2\n",
      "  cudaEventElapsedTime => hipEventElapsedTime: 1\n",
      "  cudaEventRecord => hipEventRecord: 3\n",
      "  cudaEventSynchronize => hipEventSynchronize: 2\n",
      "  cudaEvent_t => hipEvent_t: 3\n",
      "  cudaGetDevice => hipGetDevice: 1\n",
      "  cudaGetDeviceCount => hipGetDeviceCount: 2\n",
      "  cudaGetDeviceProperties => hipGetDeviceProperties: 2\n",
      "  cudaGetErrorString => hipGetErrorString: 2\n",
      "  cudaLaunchKernel => hipLaunchKernel: 1\n",
      "  cudaSetDevice => hipSetDevice: 3\n",
      "  cudaStreamCreateWithFlags => hipStreamCreateWithFlags: 1\n",
      "  cudaStreamDefault => hipStreamDefault: 1\n",
      "  cudaStreamDestroy => hipStreamDestroy: 1\n",
      "  cudaStreamNonBlocking => hipStreamNonBlocking: 1\n",
      "  cudaStream_t => hipStream_t: 7\n",
      "  cudaSuccess => hipSuccess: 4\n",
      "  cuda_runtime.h => hip/hip_runtime.h: 2\n",
      "\n",
      "[HIPIFY] info: file 'GLOBAL' statistics:\n",
      "  CONVERTED refs count: 69\n",
      "  TOTAL lines of code: 1162\n",
      "  WARNINGS: 1\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  CUDA_SUCCESS => hipSuccess: 4\n",
      "  CUresult => hipError_t: 4\n",
      "  cuGetErrorString => hipDrvGetErrorString: 1\n",
      "  cuInit => hipInit: 1\n",
      "  cuda.h => hip/hip_runtime.h: 2\n",
      "  cudaDevAttrManagedMemory => hipDeviceAttributeManagedMemory: 1\n",
      "  cudaDeviceGetAttribute => hipDeviceGetAttribute: 1\n",
      "  cudaDeviceProp => hipDeviceProp_t: 2\n",
      "  cudaDeviceReset => hipDeviceReset: 1\n",
      "  cudaDeviceSynchronize => hipDeviceSynchronize: 2\n",
      "  cudaError_t => hipError_t: 4\n",
      "  cudaEventCreate => hipEventCreate: 2\n",
      "  cudaEventDestroy => hipEventDestroy: 2\n",
      "  cudaEventElapsedTime => hipEventElapsedTime: 1\n",
      "  cudaEventRecord => hipEventRecord: 3\n",
      "  cudaEventSynchronize => hipEventSynchronize: 2\n",
      "  cudaEvent_t => hipEvent_t: 3\n",
      "  cudaFree => hipFree: 3\n",
      "  cudaGetDevice => hipGetDevice: 1\n",
      "  cudaGetDeviceCount => hipGetDeviceCount: 2\n",
      "  cudaGetDeviceProperties => hipGetDeviceProperties: 2\n",
      "  cudaGetErrorString => hipGetErrorString: 2\n",
      "  cudaGetLastError => hipGetLastError: 1\n",
      "  cudaLaunchKernel => hipLaunchKernel: 1\n",
      "  cudaMalloc => hipMalloc: 3\n",
      "  cudaMemcpy => hipMemcpy: 3\n",
      "  cudaMemcpyDeviceToHost => hipMemcpyDeviceToHost: 1\n",
      "  cudaMemcpyHostToDevice => hipMemcpyHostToDevice: 2\n",
      "  cudaSetDevice => hipSetDevice: 3\n",
      "  cudaStreamCreateWithFlags => hipStreamCreateWithFlags: 1\n",
      "  cudaStreamDefault => hipStreamDefault: 1\n",
      "  cudaStreamDestroy => hipStreamDestroy: 1\n",
      "  cudaStreamNonBlocking => hipStreamNonBlocking: 1\n",
      "  cudaStream_t => hipStream_t: 7\n",
      "  cudaSuccess => hipSuccess: 4\n",
      "  cuda_runtime.h => hip/hip_runtime.h: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hipexamine-perl.sh cuda_mat_mult -exclude-dirs=\".ipynb_checkpoints\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3117e3-f3bf-4c7f-9351-085154143a0e",
   "metadata": {},
   "source": [
    "If we try the hip-clang version we see that it doesn't handle preprocessor directives very well. The following errors with `_aligned_malloc` are due to it not picking up the windows-specific `#define` clauses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4a5ac422-ce12-4499-9aed-93f4db9559bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HIPIFY] error: hipify-clang: Unknown command line argument '-exclude-dirs=.ipynb_checkpoints'.  Try: '/opt/rocm-5.6.1/bin/hipify-clang --help'\n",
      "hipify-clang: Did you mean '--o-dir=.ipynb_checkpoints'?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hipexamine.sh ./cuda_mat_mult -exclude-dirs=\".ipynb_checkpoints\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec657b6f-2a15-4ad1-b8ef-650aedeb25fb",
   "metadata": {},
   "source": [
    "### Porting a directory structure inplace\n",
    "\n",
    "Both the **hipconvertinplace-perl.sh** and **hipconvertinplace.sh** scripts have the ability to convert a code tree inplace. The additional option **-hip-kernel-execution-syntax** replaces CUDA triple chevron kernel calls with the equivalent call to **hipLaunchKernelGGL** macro.\n",
    "\n",
    "#### Porting inplace with hipify-perl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "23846221-b541-45a8-950b-9278b477b207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HIPIFY] info: file 'temp_mat_mult/mat_mult.cpp' statistics:\n",
      "  CONVERTED refs count: 15\n",
      "  TOTAL lines of code: 190\n",
      "  WARNINGS: 0\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  cudaDeviceSynchronize => hipDeviceSynchronize: 1\n",
      "  cudaFree => hipFree: 3\n",
      "  cudaGetLastError => hipGetLastError: 1\n",
      "  cudaMalloc => hipMalloc: 3\n",
      "  cudaMemcpy => hipMemcpy: 3\n",
      "  cudaMemcpyDeviceToHost => hipMemcpyDeviceToHost: 1\n",
      "  cudaMemcpyHostToDevice => hipMemcpyHostToDevice: 2\n",
      "  warning: temp_mat_mult/cuda_helper.hpp:480: removed identifier \"cudaLaunch\" since CUDA 10.1\n",
      "\n",
      "[HIPIFY] info: file 'temp_mat_mult/cuda_helper.hpp' statistics:\n",
      "  CONVERTED refs count: 56\n",
      "  TOTAL lines of code: 789\n",
      "  WARNINGS: 1\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  CUDA_SUCCESS => hipSuccess: 4\n",
      "  CUresult => hipError_t: 4\n",
      "  cuGetErrorString => hipDrvGetErrorString: 1\n",
      "  cuInit => hipInit: 1\n",
      "  cuda.h => hip/hip_runtime.h: 2\n",
      "  cudaDevAttrManagedMemory => hipDeviceAttributeManagedMemory: 1\n",
      "  cudaDeviceGetAttribute => hipDeviceGetAttribute: 1\n",
      "  cudaDeviceProp => hipDeviceProp_t: 2\n",
      "  cudaDeviceReset => hipDeviceReset: 1\n",
      "  cudaDeviceSynchronize => hipDeviceSynchronize: 1\n",
      "  cudaError_t => hipError_t: 4\n",
      "  cudaEventCreate => hipEventCreate: 2\n",
      "  cudaEventDestroy => hipEventDestroy: 2\n",
      "  cudaEventElapsedTime => hipEventElapsedTime: 1\n",
      "  cudaEventRecord => hipEventRecord: 3\n",
      "  cudaEventSynchronize => hipEventSynchronize: 2\n",
      "  cudaEvent_t => hipEvent_t: 3\n",
      "  cudaGetDevice => hipGetDevice: 1\n",
      "  cudaGetDeviceCount => hipGetDeviceCount: 2\n",
      "  cudaGetDeviceProperties => hipGetDeviceProperties: 2\n",
      "  cudaGetErrorString => hipGetErrorString: 2\n",
      "  cudaLaunchKernel => hipLaunchKernel: 1\n",
      "  cudaSetDevice => hipSetDevice: 3\n",
      "  cudaStreamCreateWithFlags => hipStreamCreateWithFlags: 1\n",
      "  cudaStreamDefault => hipStreamDefault: 1\n",
      "  cudaStreamDestroy => hipStreamDestroy: 1\n",
      "  cudaStreamNonBlocking => hipStreamNonBlocking: 1\n",
      "  cudaStream_t => hipStream_t: 7\n",
      "  cudaSuccess => hipSuccess: 4\n",
      "  cuda_runtime.h => hip/hip_runtime.h: 2\n",
      "\n",
      "[HIPIFY] info: file 'GLOBAL' statistics:\n",
      "  CONVERTED refs count: 71\n",
      "  TOTAL lines of code: 1162\n",
      "  WARNINGS: 1\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  CUDA_SUCCESS => hipSuccess: 4\n",
      "  CUresult => hipError_t: 4\n",
      "  cuGetErrorString => hipDrvGetErrorString: 1\n",
      "  cuInit => hipInit: 1\n",
      "  cuda.h => hip/hip_runtime.h: 2\n",
      "  cudaDevAttrManagedMemory => hipDeviceAttributeManagedMemory: 1\n",
      "  cudaDeviceGetAttribute => hipDeviceGetAttribute: 1\n",
      "  cudaDeviceProp => hipDeviceProp_t: 2\n",
      "  cudaDeviceReset => hipDeviceReset: 1\n",
      "  cudaDeviceSynchronize => hipDeviceSynchronize: 2\n",
      "  cudaError_t => hipError_t: 4\n",
      "  cudaEventCreate => hipEventCreate: 2\n",
      "  cudaEventDestroy => hipEventDestroy: 2\n",
      "  cudaEventElapsedTime => hipEventElapsedTime: 1\n",
      "  cudaEventRecord => hipEventRecord: 3\n",
      "  cudaEventSynchronize => hipEventSynchronize: 2\n",
      "  cudaEvent_t => hipEvent_t: 3\n",
      "  cudaFree => hipFree: 3\n",
      "  cudaGetDevice => hipGetDevice: 1\n",
      "  cudaGetDeviceCount => hipGetDeviceCount: 2\n",
      "  cudaGetDeviceProperties => hipGetDeviceProperties: 2\n",
      "  cudaGetErrorString => hipGetErrorString: 2\n",
      "  cudaGetLastError => hipGetLastError: 1\n",
      "  cudaLaunchKernel => hipLaunchKernel: 1\n",
      "  cudaMalloc => hipMalloc: 3\n",
      "  cudaMemcpy => hipMemcpy: 3\n",
      "  cudaMemcpyDeviceToHost => hipMemcpyDeviceToHost: 1\n",
      "  cudaMemcpyHostToDevice => hipMemcpyHostToDevice: 2\n",
      "  cudaSetDevice => hipSetDevice: 3\n",
      "  cudaStreamCreateWithFlags => hipStreamCreateWithFlags: 1\n",
      "  cudaStreamDefault => hipStreamDefault: 1\n",
      "  cudaStreamDestroy => hipStreamDestroy: 1\n",
      "  cudaStreamNonBlocking => hipStreamNonBlocking: 1\n",
      "  cudaStream_t => hipStream_t: 7\n",
      "  cudaSuccess => hipSuccess: 4\n",
      "  cuda_runtime.h => hip/hip_runtime.h: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hipconvertinplace-perl.sh temp_mat_mult -hip-kernel-execution-syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2326375-a616-4704-81c3-9c3c671151bb",
   "metadata": {},
   "source": [
    "#### Porting inplace with hipify-clang\n",
    "\n",
    "Here is the same port with **hipify-clang**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3be9bcbc-4135-41ab-903d-c048cabbf961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: CUDA version is newer than the latest partially supported version 11.8\n",
      "\u001b[0;1;35mwarning: \u001b[0m\u001b[1mCUDA version is newer than the latest partially supported version 11.8 [-Wunknown-cuda-version]\u001b[0m\n",
      "In file included from /tmp/mat_mult.cpp-25db3c.hip:1:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime.h:64:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime.h:28:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime_api.h:8361:\n",
      "\u001b[1m/opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:2667:34: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'cuCtxDetach' is deprecated [-Wdeprecated-declarations]\u001b[0m\n",
      "    return hipCUResultTohipError(cuCtxDetach(ctx));\n",
      "\u001b[0;1;32m                                 ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:6307:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'cuCtxDetach' has been explicitly marked deprecated here\u001b[0m\n",
      "__CUDA_DEPRECATED CUresult CUDAAPI cuCtxDetach(CUcontext ctx);\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:71:42: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__CUDA_DEPRECATED'\u001b[0m\n",
      "#define __CUDA_DEPRECATED __attribute__((deprecated))\n",
      "\u001b[0;1;32m                                         ^\n",
      "\u001b[0mIn file included from /tmp/mat_mult.cpp-25db3c.hip:1:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime.h:64:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime.h:28:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime_api.h:8361:\n",
      "\u001b[1m/opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:2675:34: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'cuDeviceComputeCapability' is deprecated [-Wdeprecated-declarations]\u001b[0m\n",
      "    return hipCUResultTohipError(cuDeviceComputeCapability(major, minor, device));\n",
      "\u001b[0;1;32m                                 ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:4910:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'cuDeviceComputeCapability' has been explicitly marked deprecated here\u001b[0m\n",
      "__CUDA_DEPRECATED CUresult CUDAAPI cuDeviceComputeCapability(int *major, int *minor, CUdevice dev);\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:71:42: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__CUDA_DEPRECATED'\u001b[0m\n",
      "#define __CUDA_DEPRECATED __attribute__((deprecated))\n",
      "\u001b[0;1;32m                                         ^\n",
      "\u001b[0mIn file included from /tmp/mat_mult.cpp-25db3c.hip:1:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime.h:64:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime.h:28:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime_api.h:8361:\n",
      "\u001b[1m/opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:2741:34: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'cuModuleGetTexRef' is deprecated [-Wdeprecated-declarations]\u001b[0m\n",
      "    return hipCUResultTohipError(cuModuleGetTexRef(pTexRef, hmod, name));\n",
      "\u001b[0;1;32m                                 ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:6821:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'cuModuleGetTexRef' has been explicitly marked deprecated here\u001b[0m\n",
      "__CUDA_DEPRECATED CUresult CUDAAPI cuModuleGetTexRef(CUtexref *pTexRef, CUmodule hmod, const char *name);\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:71:42: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__CUDA_DEPRECATED'\u001b[0m\n",
      "#define __CUDA_DEPRECATED __attribute__((deprecated))\n",
      "\u001b[0;1;32m                                         ^\n",
      "\u001b[0mIn file included from /tmp/mat_mult.cpp-25db3c.hip:1:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime.h:64:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime.h:28:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime_api.h:8361:\n",
      "\u001b[1m/opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:2869:35: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'cudaLaunchCooperativeKernelMultiDevice' is deprecated [-Wdeprecated-declarations]\u001b[0m\n",
      "    return hipCUDAErrorTohipError(cudaLaunchCooperativeKernelMultiDevice(launchParamsList, numDevices, flags));\n",
      "\u001b[0;1;32m                                  ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda_runtime_api.h:4377:8: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'cudaLaunchCooperativeKernelMultiDevice' has been explicitly marked deprecated here\u001b[0m\n",
      "extern __CUDA_DEPRECATED __host__ cudaError_t CUDARTAPI cudaLaunchCooperativeKernelMultiDevice(struct cudaLaunchParams *launchParamsList, unsigned int numDevices, unsigned int flags  __dv(0));\n",
      "\u001b[0;1;32m       ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda_runtime_api.h:265:42: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__CUDA_DEPRECATED'\u001b[0m\n",
      "#define __CUDA_DEPRECATED __attribute__((deprecated))\n",
      "\u001b[0;1;32m                                         ^\n",
      "\u001b[0mIn file included from /tmp/mat_mult.cpp-25db3c.hip:1:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime.h:64:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime.h:28:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime_api.h:8361:\n",
      "\u001b[1m/opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:2876:34: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'cuLaunchCooperativeKernelMultiDevice' is deprecated [-Wdeprecated-declarations]\u001b[0m\n",
      "    return hipCUResultTohipError(cuLaunchCooperativeKernelMultiDevice(launchParamsList,\n",
      "\u001b[0;1;32m                                 ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:15803:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'cuLaunchCooperativeKernelMultiDevice' has been explicitly marked deprecated here\u001b[0m\n",
      "__CUDA_DEPRECATED CUresult CUDAAPI cuLaunchCooperativeKernelMultiDevice(CUDA_LAUNCH_PARAMS *launchParamsList, unsigned int numDevices, unsigned int flags);\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:71:42: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__CUDA_DEPRECATED'\u001b[0m\n",
      "#define __CUDA_DEPRECATED __attribute__((deprecated))\n",
      "\u001b[0;1;32m                                         ^\n",
      "\u001b[0mIn file included from /tmp/mat_mult.cpp-25db3c.hip:1:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime.h:64:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime.h:28:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime_api.h:8361:\n",
      "\u001b[1m/opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:3165:34: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'cuTexRefSetAddress_v2' is deprecated [-Wdeprecated-declarations]\u001b[0m\n",
      "    return hipCUResultTohipError(cuTexRefSetAddress(ByteOffset,hTexRef,dptr,bytes));\n",
      "\u001b[0;1;32m                                 ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:128:45: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro 'cuTexRefSetAddress'\u001b[0m\n",
      "#define cuTexRefSetAddress                  cuTexRefSetAddress_v2\n",
      "\u001b[0;1;32m                                            ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:19426:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'cuTexRefSetAddress_v2' has been explicitly marked deprecated here\u001b[0m\n",
      "__CUDA_DEPRECATED CUresult CUDAAPI cuTexRefSetAddress(size_t *ByteOffset, CUtexref hTexRef, CUdeviceptr dptr, size_t bytes);\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:71:42: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__CUDA_DEPRECATED'\u001b[0m\n",
      "#define __CUDA_DEPRECATED __attribute__((deprecated))\n",
      "\u001b[0;1;32m                                         ^\n",
      "\u001b[0mIn file included from /tmp/mat_mult.cpp-25db3c.hip:1:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime.h:64:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime.h:28:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime_api.h:8361:\n",
      "\u001b[1m/opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:3169:34: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'cuTexRefSetAddress2D_v3' is deprecated [-Wdeprecated-declarations]\u001b[0m\n",
      "    return hipCUResultTohipError(cuTexRefSetAddress2D(hTexRef,desc,dptr,Pitch));\n",
      "\u001b[0;1;32m                                 ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:136:45: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro 'cuTexRefSetAddress2D'\u001b[0m\n",
      "#define cuTexRefSetAddress2D                cuTexRefSetAddress2D_v3\n",
      "\u001b[0;1;32m                                            ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:19481:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'cuTexRefSetAddress2D_v3' has been explicitly marked deprecated here\u001b[0m\n",
      "__CUDA_DEPRECATED CUresult CUDAAPI cuTexRefSetAddress2D(CUtexref hTexRef, const CUDA_ARRAY_DESCRIPTOR *desc, CUdeviceptr dptr, size_t Pitch);\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:71:42: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__CUDA_DEPRECATED'\u001b[0m\n",
      "#define __CUDA_DEPRECATED __attribute__((deprecated))\n",
      "\u001b[0;1;32m                                         ^\n",
      "\u001b[0mIn file included from /tmp/mat_mult.cpp-25db3c.hip:18:\n",
      "\u001b[1m/home/toby/Pelagos/Projects/HIP_Course/course_material/L9_Porting_CUDA_to_HIP/temp_mat_mult/cuda_helper.hpp:54:6: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1mredefinition of 'h_errchk'\u001b[0m\n",
      "void h_errchk(hipError_t errcode, const char* message) {\n",
      "\u001b[0;1;32m     ^\n",
      "\u001b[0m\u001b[1m/home/toby/Pelagos/Projects/HIP_Course/course_material/L9_Porting_CUDA_to_HIP/temp_mat_mult/cuda_helper.hpp:39:6: \u001b[0m\u001b[0;1;30mnote: \u001b[0mprevious definition is here\u001b[0m\n",
      "void h_errchk(hipError_t errcode, const char* message) {\n",
      "\u001b[0;1;32m     ^\n",
      "\u001b[0m7 warnings and 1 error generated when compiling for host.\n",
      "Error while processing /tmp/mat_mult.cpp-25db3c.hip.\n",
      "\n",
      "[HIPIFY] info: file 'temp_mat_mult/mat_mult.cpp' statistics:\n",
      "\n",
      "  ERROR: Statistics is invalid due to failed hipification.\n",
      "\n",
      "  CONVERTED refs count: 1\n",
      "  UNCONVERTED refs count: 0\n",
      "  CONVERSION %: 100.0\n",
      "  REPLACED bytes: 106\n",
      "  TOTAL bytes: 5975\n",
      "  CHANGED lines of code: 2\n",
      "  TOTAL lines of code: 191\n",
      "  CODE CHANGED (in bytes) %: 1.8\n",
      "  CODE CHANGED (in lines) %: 1.0\n",
      "  TIME ELAPSED s: 0.68\n",
      "[HIPIFY] info: CONVERTED refs by type:\n",
      "  kernel_launch: 1\n",
      "[HIPIFY] info: CONVERTED refs by API:\n",
      "  CUDA RT API: 1\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  cudaLaunchKernel: 1\n",
      "\u001b[0;1;35mwarning: \u001b[0m\u001b[1mCUDA version is newer than the latest partially supported version 11.8 [-Wunknown-cuda-version]\u001b[0m\n",
      "In file included from /tmp/cuda_helper.hpp-d806ba.hip:1:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime.h:64:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime.h:28:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime_api.h:8361:\n",
      "\u001b[1m/opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:2667:34: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'cuCtxDetach' is deprecated [-Wdeprecated-declarations]\u001b[0m\n",
      "    return hipCUResultTohipError(cuCtxDetach(ctx));\n",
      "\u001b[0;1;32m                                 ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:6307:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'cuCtxDetach' has been explicitly marked deprecated here\u001b[0m\n",
      "__CUDA_DEPRECATED CUresult CUDAAPI cuCtxDetach(CUcontext ctx);\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:71:42: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__CUDA_DEPRECATED'\u001b[0m\n",
      "#define __CUDA_DEPRECATED __attribute__((deprecated))\n",
      "\u001b[0;1;32m                                         ^\n",
      "\u001b[0mIn file included from /tmp/cuda_helper.hpp-d806ba.hip:1:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime.h:64:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime.h:28:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime_api.h:8361:\n",
      "\u001b[1m/opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:2675:34: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'cuDeviceComputeCapability' is deprecated [-Wdeprecated-declarations]\u001b[0m\n",
      "    return hipCUResultTohipError(cuDeviceComputeCapability(major, minor, device));\n",
      "\u001b[0;1;32m                                 ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:4910:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'cuDeviceComputeCapability' has been explicitly marked deprecated here\u001b[0m\n",
      "__CUDA_DEPRECATED CUresult CUDAAPI cuDeviceComputeCapability(int *major, int *minor, CUdevice dev);\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:71:42: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__CUDA_DEPRECATED'\u001b[0m\n",
      "#define __CUDA_DEPRECATED __attribute__((deprecated))\n",
      "\u001b[0;1;32m                                         ^\n",
      "\u001b[0mIn file included from /tmp/cuda_helper.hpp-d806ba.hip:1:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime.h:64:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime.h:28:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime_api.h:8361:\n",
      "\u001b[1m/opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:2741:34: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'cuModuleGetTexRef' is deprecated [-Wdeprecated-declarations]\u001b[0m\n",
      "    return hipCUResultTohipError(cuModuleGetTexRef(pTexRef, hmod, name));\n",
      "\u001b[0;1;32m                                 ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:6821:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'cuModuleGetTexRef' has been explicitly marked deprecated here\u001b[0m\n",
      "__CUDA_DEPRECATED CUresult CUDAAPI cuModuleGetTexRef(CUtexref *pTexRef, CUmodule hmod, const char *name);\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:71:42: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__CUDA_DEPRECATED'\u001b[0m\n",
      "#define __CUDA_DEPRECATED __attribute__((deprecated))\n",
      "\u001b[0;1;32m                                         ^\n",
      "\u001b[0mIn file included from /tmp/cuda_helper.hpp-d806ba.hip:1:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime.h:64:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime.h:28:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime_api.h:8361:\n",
      "\u001b[1m/opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:2869:35: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'cudaLaunchCooperativeKernelMultiDevice' is deprecated [-Wdeprecated-declarations]\u001b[0m\n",
      "    return hipCUDAErrorTohipError(cudaLaunchCooperativeKernelMultiDevice(launchParamsList, numDevices, flags));\n",
      "\u001b[0;1;32m                                  ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda_runtime_api.h:4377:8: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'cudaLaunchCooperativeKernelMultiDevice' has been explicitly marked deprecated here\u001b[0m\n",
      "extern __CUDA_DEPRECATED __host__ cudaError_t CUDARTAPI cudaLaunchCooperativeKernelMultiDevice(struct cudaLaunchParams *launchParamsList, unsigned int numDevices, unsigned int flags  __dv(0));\n",
      "\u001b[0;1;32m       ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda_runtime_api.h:265:42: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__CUDA_DEPRECATED'\u001b[0m\n",
      "#define __CUDA_DEPRECATED __attribute__((deprecated))\n",
      "\u001b[0;1;32m                                         ^\n",
      "\u001b[0mIn file included from /tmp/cuda_helper.hpp-d806ba.hip:1:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime.h:64:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime.h:28:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime_api.h:8361:\n",
      "\u001b[1m/opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:2876:34: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'cuLaunchCooperativeKernelMultiDevice' is deprecated [-Wdeprecated-declarations]\u001b[0m\n",
      "    return hipCUResultTohipError(cuLaunchCooperativeKernelMultiDevice(launchParamsList,\n",
      "\u001b[0;1;32m                                 ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:15803:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'cuLaunchCooperativeKernelMultiDevice' has been explicitly marked deprecated here\u001b[0m\n",
      "__CUDA_DEPRECATED CUresult CUDAAPI cuLaunchCooperativeKernelMultiDevice(CUDA_LAUNCH_PARAMS *launchParamsList, unsigned int numDevices, unsigned int flags);\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:71:42: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__CUDA_DEPRECATED'\u001b[0m\n",
      "#define __CUDA_DEPRECATED __attribute__((deprecated))\n",
      "\u001b[0;1;32m                                         ^\n",
      "\u001b[0mIn file included from /tmp/cuda_helper.hpp-d806ba.hip:1:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime.h:64:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime.h:28:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime_api.h:8361:\n",
      "\u001b[1m/opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:3165:34: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'cuTexRefSetAddress_v2' is deprecated [-Wdeprecated-declarations]\u001b[0m\n",
      "    return hipCUResultTohipError(cuTexRefSetAddress(ByteOffset,hTexRef,dptr,bytes));\n",
      "\u001b[0;1;32m                                 ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:128:45: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro 'cuTexRefSetAddress'\u001b[0m\n",
      "#define cuTexRefSetAddress                  cuTexRefSetAddress_v2\n",
      "\u001b[0;1;32m                                            ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:19426:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'cuTexRefSetAddress_v2' has been explicitly marked deprecated here\u001b[0m\n",
      "__CUDA_DEPRECATED CUresult CUDAAPI cuTexRefSetAddress(size_t *ByteOffset, CUtexref hTexRef, CUdeviceptr dptr, size_t bytes);\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:71:42: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__CUDA_DEPRECATED'\u001b[0m\n",
      "#define __CUDA_DEPRECATED __attribute__((deprecated))\n",
      "\u001b[0;1;32m                                         ^\n",
      "\u001b[0mIn file included from /tmp/cuda_helper.hpp-d806ba.hip:1:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime.h:64:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime.h:28:\n",
      "In file included from /opt/rocm-5.6.1/include/hip/hip_runtime_api.h:8361:\n",
      "\u001b[1m/opt/rocm-5.6.1/include/hip/nvidia_detail/nvidia_hip_runtime_api.h:3169:34: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'cuTexRefSetAddress2D_v3' is deprecated [-Wdeprecated-declarations]\u001b[0m\n",
      "    return hipCUResultTohipError(cuTexRefSetAddress2D(hTexRef,desc,dptr,Pitch));\n",
      "\u001b[0;1;32m                                 ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:136:45: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro 'cuTexRefSetAddress2D'\u001b[0m\n",
      "#define cuTexRefSetAddress2D                cuTexRefSetAddress2D_v3\n",
      "\u001b[0;1;32m                                            ^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:19481:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'cuTexRefSetAddress2D_v3' has been explicitly marked deprecated here\u001b[0m\n",
      "__CUDA_DEPRECATED CUresult CUDAAPI cuTexRefSetAddress2D(CUtexref hTexRef, const CUDA_ARRAY_DESCRIPTOR *desc, CUdeviceptr dptr, size_t Pitch);\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/usr/local/cuda-12.1/include/cuda.h:71:42: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__CUDA_DEPRECATED'\u001b[0m\n",
      "#define __CUDA_DEPRECATED __attribute__((deprecated))\n",
      "\u001b[0;1;32m                                         ^\n",
      "\u001b[0m\u001b[1m/tmp/cuda_helper.hpp-d806ba.hip:54:6: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1mredefinition of 'h_errchk'\u001b[0m\n",
      "void h_errchk(hipError_t errcode, const char* message) {\n",
      "\u001b[0;1;32m     ^\n",
      "\u001b[0m\u001b[1m/tmp/cuda_helper.hpp-d806ba.hip:39:6: \u001b[0m\u001b[0;1;30mnote: \u001b[0mprevious definition is here\u001b[0m\n",
      "void h_errchk(hipError_t errcode, const char* message) {\n",
      "\u001b[0;1;32m     ^\n",
      "\u001b[0m\u001b[1m/tmp/cuda_helper.hpp-d806ba.hip:96:5: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1munknown type name 'SYSTEM_INFO'\u001b[0m\n",
      "    SYSTEM_INFO sys_info;\n",
      "\u001b[0;1;32m    ^\n",
      "\u001b[0m\u001b[1m/tmp/cuda_helper.hpp-d806ba.hip:382:20: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1muse of undeclared identifier '_aligned_malloc'; did you mean 'aligned_alloc'?\u001b[0m\n",
      "    void* buffer = _aligned_malloc(nbytes, alignment);\n",
      "\u001b[0;1;32m                   ^~~~~~~~~~~~~~~\n",
      "\u001b[0m\u001b[0;32m                   aligned_alloc\n",
      "\u001b[0m\u001b[1m/usr/include/stdlib.h:592:14: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'aligned_alloc' declared here\u001b[0m\n",
      "extern void *aligned_alloc (size_t __alignment, size_t __size)\n",
      "\u001b[0;1;32m             ^\n",
      "\u001b[0m\u001b[1m/tmp/cuda_helper.hpp-d806ba.hip:384:11: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1mredefinition of 'buffer'\u001b[0m\n",
      "    void* buffer = aligned_alloc(alignment, nbytes);\n",
      "\u001b[0;1;32m          ^\n",
      "\u001b[0m\u001b[1m/tmp/cuda_helper.hpp-d806ba.hip:382:11: \u001b[0m\u001b[0;1;30mnote: \u001b[0mprevious definition is here\u001b[0m\n",
      "    void* buffer = _aligned_malloc(nbytes, alignment);\n",
      "\u001b[0;1;32m          ^\n",
      "\u001b[0m7 warnings and 4 errors generated when compiling for host.\n",
      "Error while processing /tmp/cuda_helper.hpp-d806ba.hip.\n",
      "\n",
      "[HIPIFY] info: file 'temp_mat_mult/cuda_helper.hpp' statistics:\n",
      "\n",
      "  ERROR: Statistics is invalid due to failed hipification.\n",
      "\n",
      "  CONVERTED refs count: 0\n",
      "  UNCONVERTED refs count: 0\n",
      "  CONVERSION %: 0.0\n",
      "  REPLACED bytes: 0\n",
      "  TOTAL bytes: 24660\n",
      "  CHANGED lines of code: 1\n",
      "  TOTAL lines of code: 790\n",
      "  CODE CHANGED (in bytes) %: 0.0\n",
      "  CODE CHANGED (in lines) %: 0.1\n",
      "  TIME ELAPSED s: 0.53\n",
      "\u001b[0;1;35mwarning: \u001b[0m\u001b[1mCUDA version is newer than the latest partially supported version 11.8 [-Wunknown-cuda-version]\u001b[0m\n",
      "\u001b[1m/tmp/mat_helper.hpp-d998ed.hip:162:5: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1muse of undeclared identifier 'assert'\u001b[0m\n",
      "    assert(len0_src>=K0);\n",
      "\u001b[0;1;32m    ^\n",
      "\u001b[0m\u001b[1m/tmp/mat_helper.hpp-d998ed.hip:163:5: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1muse of undeclared identifier 'assert'\u001b[0m\n",
      "    assert(len1_src>=K1);    \n",
      "\u001b[0;1;32m    ^\n",
      "\u001b[0m2 errors generated when compiling for host.\n",
      "Error while processing /tmp/mat_helper.hpp-d998ed.hip.\n",
      "\n",
      "[HIPIFY] info: file 'temp_mat_mult/mat_helper.hpp' statistics:\n",
      "\n",
      "  ERROR: Statistics is invalid due to failed hipification.\n",
      "\n",
      "  CONVERTED refs count: 0\n",
      "  UNCONVERTED refs count: 0\n",
      "  CONVERSION %: 0.0\n",
      "  REPLACED bytes: 0\n",
      "  TOTAL bytes: 4497\n",
      "  CHANGED lines of code: 1\n",
      "  TOTAL lines of code: 180\n",
      "  CODE CHANGED (in bytes) %: 0.0\n",
      "  CODE CHANGED (in lines) %: 0.6\n",
      "  TIME ELAPSED s: 0.49\n",
      "\u001b[0;1;35mwarning: \u001b[0m\u001b[1mCUDA version is newer than the latest partially supported version 11.8 [-Wunknown-cuda-version]\u001b[0m\n",
      "\n",
      "[HIPIFY] info: file 'temp_mat_mult/mat_size.hpp' statistics:\n",
      "  CONVERTED refs count: 0\n",
      "  UNCONVERTED refs count: 0\n",
      "  CONVERSION %: 0.0\n",
      "  REPLACED bytes: 0\n",
      "  TOTAL bytes: 107\n",
      "  CHANGED lines of code: 1\n",
      "  TOTAL lines of code: 3\n",
      "  CODE CHANGED (in bytes) %: 0.0\n",
      "  CODE CHANGED (in lines) %: 33.3\n",
      "  TIME ELAPSED s: 0.19\n",
      "\n",
      "[HIPIFY] info: file 'GLOBAL' statistics:\n",
      "\n",
      "  ERROR: Statistics is invalid due to failed hipification.\n",
      "\n",
      "  CONVERTED refs count: 1\n",
      "  UNCONVERTED refs count: 0\n",
      "  CONVERSION %: 100.0\n",
      "  REPLACED bytes: 106\n",
      "  TOTAL bytes: 35239\n",
      "  CHANGED lines of code: 5\n",
      "  TOTAL lines of code: 1164\n",
      "  CODE CHANGED (in bytes) %: 0.3\n",
      "  CODE CHANGED (in lines) %: 0.4\n",
      "  TIME ELAPSED s: 1.89\n",
      "[HIPIFY] info: CONVERTED refs by type:\n",
      "  kernel_launch: 1\n",
      "[HIPIFY] info: CONVERTED refs by API:\n",
      "  CUDA RT API: 1\n",
      "[HIPIFY] info: CONVERTED refs by names:\n",
      "  cudaLaunchKernel: 1\n",
      "\n",
      "[HIPIFY] info: TOTAL statistics:\n",
      "  CONVERTED files: 1\n",
      "  PROCESSED files: 4\n"
     ]
    }
   ],
   "source": [
    "!hipconvertinplace.sh temp_mat_mult -hip-kernel-execution-syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc505b5e-1291-4007-86e8-ae027dfca4cd",
   "metadata": {},
   "source": [
    "#### Building the ported code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f3bf73-e76a-49cc-b006-4b90e1eed6b2",
   "metadata": {},
   "source": [
    "If we examine the source tree we see that every source file that has been hipified has been first copied to a file with suffix `*.prehip`. Then the converted code is overwritten in place of the old file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "93c79b1c-e721-4421-9183-a44ed01ccedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2380\n",
      "-rw-rw-r-- 1 toby toby  262144 Sep 26 16:29 array_A.dat\n",
      "-rw-rw-r-- 1 toby toby  262144 Sep 26 16:29 array_B.dat\n",
      "-rw-rw-r-- 1 toby toby  262144 Sep 26 16:29 array_C.dat\n",
      "-rw-rw-r-- 1 toby toby   24660 Sep 26 16:35 cuda_helper.hpp\n",
      "-rw-rw-r-- 1 toby toby   24629 Sep 26 16:34 cuda_helper.hpp.prehip\n",
      "-rw-rw-r-- 1 toby toby     341 Sep 26 16:29 Makefile\n",
      "-rw-rw-r-- 1 toby toby    4497 Sep 26 16:35 mat_helper.hpp\n",
      "-rw-rw-r-- 1 toby toby    4497 Sep 26 16:34 mat_helper.hpp.prehip\n",
      "-rw-rw-r-- 1 toby toby    5975 Sep 26 16:35 mat_mult.cpp\n",
      "-rw-rw-r-- 1 toby toby    5944 Sep 26 16:30 mat_mult.cpp.prehip\n",
      "-rwxrwxr-x 1 toby toby 1545520 Sep 26 16:29 mat_mult.exe\n",
      "-rw-rw-r-- 1 toby toby     137 Sep 26 16:35 mat_size.hpp\n",
      "-rw-rw-r-- 1 toby toby     107 Sep 26 16:34 mat_size.hpp.prehip\n"
     ]
    }
   ],
   "source": [
    "!ls -l temp_mat_mult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2964cedf-53df-49d3-a98a-812a59e34aeb",
   "metadata": {},
   "source": [
    "Try making the ported code with hipcc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b1e07138-163e-4615-93fe-62b653908f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -r *.exe\n",
      "rm: cannot remove '*.exe': No such file or directory\n",
      "make: *** [Makefile:20: clean] Error 1\n",
      "hipcc -g -O2 -x cu mat_mult.cpp -o mat_mult.exe \n",
      "\u001b[01m\u001b[0m\u001b[01mcuda_helper.hpp(54)\u001b[0m: \u001b[01;31merror\u001b[0m: function \u001b[01m\"h_errchk\"\u001b[0m has already been defined\n",
      "  void h_errchk(hipError_t errcode, const char* message) {\n",
      "       ^\n",
      "\n",
      "1 error detected in the compilation of \"mat_mult.cpp\".\n",
      "make: *** [Makefile:16: mat_mult.exe] Error 2\n"
     ]
    }
   ],
   "source": [
    "!cd temp_mat_mult; make clean; make CXX=\"hipcc\" LIBFLAGS=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39501460-ca62-4103-be05-aa00069beaaf",
   "metadata": {},
   "source": [
    "In the original file **cuda_mat_mult/cuda_helper.cpp** we had overloaded the **h_errchk** function to accept errorcodes of both type **CUResult** and **cudaError_t**. Following conversion to HIP the errorcode has been replaced with just **hipError_t**. Therefore we need to manually delete the duplicate **h_errchk** function in **[temp_mat_mult/cuda_helper.hpp.prehip](temp_mat_mult/cuda_helper.hpp.prehip)**. Then rerun the conversion and the make. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9142234b-e1d6-4a1c-b387-c679b25df42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  warning: cuda_helper.hpp:451: removed identifier \"cudaLaunch\" since CUDA 10.1\n",
      "make: Nothing to be done for 'all'.\n",
      "Device id: 0\n",
      "\tname:                                    NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "\tglobal memory size:                      6226 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    49 KB\n",
      "\tmaximum pitch size for memory copies:    2147 MB\n",
      "\tmax block size:                          (1024,1024,64)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,65535,65535)\n",
      "Maximum error (infinity norm) is: 1.52588e-05\n"
     ]
    }
   ],
   "source": [
    "!cd temp_mat_mult; hipify-perl -inplace -hip-kernel-execution-syntax cuda_helper.hpp\n",
    "!cd temp_mat_mult; make CXX=\"hipcc\" LIBFLAGS=\"\"; ./mat_mult.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218d61e3-fdc4-4dd1-8ac1-e537872577fb",
   "metadata": {},
   "source": [
    "Now we should have a successful port of the CUDA code to HIP!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2d5ccc-66a6-4dff-a76e-2b017591f613",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Learnings from the porting process\n",
    "\n",
    "### API differences between CUDA and HIP\n",
    "\n",
    "CUDA has the notion of a driver API and a runtime API. HIP combines the two into one API and then supports a subset of the combined API. Context managment in HIP is deprecated.\n",
    "\n",
    "### Tips for managing large kernels\n",
    "\n",
    "#### Register pressure on ported kernels\n",
    "\n",
    "Due to compiler and runtime maturity, experience with recent hackathons has shown that the NVIDIA software stack is currently better able to handle kernels with large numbers of registers. When porting to AMD hardware there is likely to be fewer registers available per thread before occupancy is affected. See some of the tips in Lesson 7 on <a href=\"../L7_Kernel_Optimisation/Optimisation.ipynb\">optimising kernels</a> to try and reduce register pressure.\n",
    "\n",
    "#### Relocatable device code\n",
    "\n",
    "From [this source](https://docs.amd.com/projects/HIP/en/latest/user_guide/hip_porting_driver_api.html) The linker option `–fgpu-rdc` allows for kernels to call functions that are compiled for different translation units. At the [Pawsey P'Con 23 Hackathon](https://pawsey.org.au/event/pacer-conference-2023-pcon23-registration/) a team found that the use of this flag generated excessively long link times. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
