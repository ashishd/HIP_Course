{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49390830-5c18-41f4-9f1d-18bddc857edd",
   "metadata": {},
   "source": [
    "# Optimising compute with concurrent IO in HIP\n",
    "\n",
    "With many iterative processes there is a need to get information **off** the device at regular intervals. Up to this point we have been transferring data off the compute device **after** kernel execution. Furthermore, the routines to read information from device buffers have thus far been used in a blocking manner, that is the program pauses while the read occurs. Most compute devices have the ability to transfer data **while** kernels are being executed. This means IO transfers can take place during compute and may in some instances **take place entirely** during kernel execution. For the cost of additional programming complexity, significant compute savings can be obtained, as the following diagram illustrates:\n",
    "\n",
    "<figure style=\"margin-bottom 3em; margin-top: 2em; margin-left:auto; margin-right:auto; width:100%\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/optimising_io.svg\"> <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Figure: The difference between sequential and concurrent IO.</figcaption>\n",
    "</figure>\n",
    "\n",
    "## Concurrent IO is enabled with multiple streams\n",
    "\n",
    "A stream is a place where one can perform work, such as the execution of a kernel or an IO operation. Thus far we have been using just the null stream for compute and IO. GPU's have the ability to run streams that do IO **at the same time** as streams devoted to compute, and if a kernel does not occupy all hardware thread on the device, then streams allow for multiple kernels to be run at the same time. In either case **there is a performance avantage to be gained** in using multiple streams. \n",
    "\n",
    "Streams are initialised using the commands **hipStreamCreate** and **hipStreamCreateWithFlags**. The latter command allows one to create streams where the null stream can optionally wait for work in the stream to complete (implicitly synchronise) before running its own work. The command **h_create_streams** from <a href=\"../include/hip_helper.hpp\">hip_helper.hpp</a> is used to create additional streams with the option of implicitly synchronising with the null stream. Below is the code for **h_create_streams**:\n",
    "\n",
    "```C++\n",
    "/// Create a number of streams\n",
    "hipStream_t* h_create_streams(size_t nstreams, int synchronise) {\n",
    "    // Blocking is a boolean, 0==no, \n",
    "    assert(nstreams>0);\n",
    "\n",
    "    unsigned int flag = hipStreamDefault;\n",
    "\n",
    "    // If blocking is 0 then set NonBlocking flag\n",
    "    // meaning we don't synchronise with the null stream\n",
    "    if (synchronise == 0) {\n",
    "        flag = hipStreamNonBlocking;\n",
    "    }\n",
    "\n",
    "    // Make the streams\n",
    "    hipStream_t* streams = (hipStream_t*)calloc(nstreams, sizeof(hipStream_t));\n",
    "\n",
    "    for (int i=0; i<nstreams; i++) {\n",
    "        H_ERRCHK(hipStreamCreateWithFlags(&streams[i], flag));\n",
    "    }\n",
    "\n",
    "    return streams;\n",
    "}\n",
    "```\n",
    "\n",
    "If streams operate at the same time (concurrently), then there must be synchronisation controls. HIP events may be inserted into streams, and the event will register as complete after all previous work in the stream is done. Both event and stream synchronisation commands can help establish dependencies between streams. Non-blocking asynchronous IO calls are also vital for concurrency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec04ce78-df29-42dd-a38a-822a23407cae",
   "metadata": {},
   "source": [
    "## Example with the 2D wave equation\n",
    "\n",
    "The [scalar wave equation](https://en.wikipedia.org/wiki/Wave_equation) adequately describes the propagation of waves. If **U** is a 2D grid storing the amplitude of the wave at every location (the wavefield), **V** is a 2D grid storing velocity, and **t** is time, then 2D waves propagate according to the formula,\n",
    "\n",
    "$$\\frac{\\partial^2 \\textbf{U}}{{\\partial t}^2}=\\textbf{V}^2 \\left (\\frac{\\partial^2 \\textbf{U}}{{\\partial x_{0}}^2}+\\frac{\\partial^2 \\textbf{U}}{{\\partial x_{1}}^2} \\right)+f(t)$$\n",
    "\n",
    "where $x_0$ and $x_1$ are spatial directions and $f(t)$ is a forcing term. If $\\Delta t$ is the time step a second-order finite-difference approximation to the time derivative is given in terms of the amplitude at timesteps $\\textbf{U}_{0}, \\textbf{U}_{1}$ and $\\textbf{U}_{2}.$ \n",
    "\n",
    "$$\\frac{\\partial^2 \\textbf{U}}{{\\partial t}^2} \\approx \\frac{1}{\\Delta t^2} \\left ( \\textbf{U}_{0} -2 \\textbf{U}_{1}+\\textbf{U}_{2} \\right ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78810535-577b-4789-a36c-d5a43d759bd7",
   "metadata": {},
   "source": [
    "Replace $\\frac{\\partial^2 \\textbf{U}}{{\\partial t}^2}$ with $\\frac{1}{\\Delta t^2} \\left( \\textbf{U}_{0} -2 \\textbf{U}_{1}+\\textbf{U}_{2} \\right )$ and solve for $\\textbf{U}_{2}$.\n",
    "\n",
    "$$\\textbf{U}_{2} \\approx 2 \\textbf{U}_{1} - \\textbf{U}_{0} + \\Delta t^2\\textbf{V}^2 \\left (\\frac{\\partial^2 \\textbf{U}_{1}}{{\\partial x_{0}}^2}+\\frac{\\partial^2 \\textbf{U}_{1}}{{\\partial x_{1}}^2} \\right)+f_{1}$$\n",
    "\n",
    "This is an iterative formula to generate the amplitude at the next timestep $\\textbf{U}_2$ if we know the present ampltiude $\\textbf{U}_{1}$ and past amplitude $\\textbf{U}_{0}.$ We also use finite difference approximations for the spatial derivatives, and express the spatial derivatives as a matrix multiplied by $\\textbf{U}_{1}$, but this complexity is unnecessary to show here. All we need to know is that the next timestep is a function ${\\textbf{F}}$ of the present and past timesteps, the velocity, and the forcing term.\n",
    "\n",
    "$$\\textbf{U}_{2}=\\textbf{F}(\\textbf{U}_0, \\textbf{U}_1, \\textbf{V}, f_{1})$$\n",
    "\n",
    "> In geophysics we usually use a [Ricker Wavelet](https://wiki.seg.org/wiki/Dictionary:Ricker_wavelet) for the forcing term $f$, and usually inject that wavelet into one cell within the grid as time progresses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d9870e-642e-4357-a49a-bf220fd42d7c",
   "metadata": {},
   "source": [
    "### Kernel implementation\n",
    "\n",
    "In both [wave2d_sync.cpp](wave2d_sync.cpp) and [wave2d_async.cpp](wave2d_async.cpp) is a kernel called **wave2d_4o** that implements the function **F**. HIP device allocations store $\\textbf{U}_{0}, \\textbf{U}_{1}, \\textbf{U}_{2}$, and $\\textbf{V}$ on the compute device.\n",
    "\n",
    "```C++\n",
    "// Kernel to solve the wave equation with fourth-order accuracy in space\n",
    "__global__ void wave2d_4o (\n",
    "        // Arguments\n",
    "        float_type* U0,\n",
    "        float_type* U1,\n",
    "        float_type* U2,\n",
    "        float_type* V,\n",
    "        size_t N0,\n",
    "        size_t N1,\n",
    "        float dt2,\n",
    "        float inv_dx02,\n",
    "        float inv_dx12,\n",
    "        // Position, frequency, and time for the\n",
    "        // wavelet injection\n",
    "        size_t P0,\n",
    "        size_t P1,\n",
    "        float pi2fm2t2) {    \n",
    "\n",
    "    // U2, U1, U0, V is of size (N0, N1)\n",
    "    size_t i0 = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    size_t i1 = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // Required padding and coefficients for spatial finite difference\n",
    "    const int pad_l=2, pad_r=2, ncoeffs=5;\n",
    "    float coeffs[ncoeffs] = {-0.083333336f, 1.3333334f, -2.5f, 1.3333334f, -0.083333336f};\n",
    "    \n",
    "    // Limit i0 and i1 to the region of U2 within the padding\n",
    "    i0=min(i0, (size_t)(N0-1-pad_r));\n",
    "    i1=min(i1, (size_t)(N1-1-pad_r));\n",
    "    i0=max((size_t)pad_l, i0);\n",
    "    i1=max((size_t)pad_l, i1);\n",
    "    \n",
    "    // Position within the grid as a 1D offset\n",
    "    long offset=i0*N1+i1;\n",
    "    \n",
    "    // Temporary storage\n",
    "    float temp0=0.0f, temp1=0.0f;\n",
    "    float tempV=V[offset];\n",
    "    \n",
    "    // Calculate the Laplacian\n",
    "    for (long n=0; n<ncoeffs; n++) {\n",
    "        // Stride in dim0 is N1        \n",
    "        temp0+=coeffs[n]*U1[offset+(n*(long)N1)-(pad_l*(long)N1)];\n",
    "        // Stride in dim1 is 1\n",
    "        temp1+=coeffs[n]*U1[offset+n-pad_l];\n",
    "    }\n",
    "    \n",
    "    // Calculate the wavefield U2 at the next timestep\n",
    "    U2[offset]=(2.0f*U1[offset])-U0[offset]+((dt2*tempV*tempV)*(temp0*inv_dx02+temp1*inv_dx12));\n",
    "    \n",
    "    // Inject the forcing term at coordinates (P0, P1)\n",
    "    if ((i0==P0) && (i1==P1)) {\n",
    "        U2[offset]+=(1.0f-2.0f*pi2fm2t2)*exp(-pi2fm2t2);\n",
    "    }\n",
    "    \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d266b76-ce52-455e-a9b5-c37b5238307c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Problem setup\n",
    "\n",
    "For this problem we create the 2D grid as a square box of size $(N0,N1)=(256,256)$. The velocity is uniform at 343m/s. This is approximately the speed of sound in air. Then we use a Ricker wavelet as a forcing term to 'let off a firework' in the middle of the box and run a number of timesteps to see how a sound wave propagates in the box. \n",
    "\n",
    "<figure style=\"margin-bottom 3em; margin-top: 2em; margin-left:auto; margin-right:auto; width:80%\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/wave2d_problem.svg\"> <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Figure: Problem setup for the 2D wave equation.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcda1835-1595-438a-9d57-18f6839bc03c",
   "metadata": {},
   "source": [
    "The programs [wave2d_sync.cpp](wave2d_sync.cpp) and [wave2d_async.cpp](wave2d_async.cpp) setup a velocity and wavefield of size (N0, N1). At each timestep the kernel **wave2d_4o** is used to update the solution and a Ricker wavelet is injected into the middle of the box. Enough timesteps are alloted so that the wave propagates through the medium and reflects off the walls. Wavefield arrays that are no longer needed are recycled for efficiency.\n",
    "\n",
    "The Python code below readies the framework for plotting the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcced67c-c103-4ec3-b21e-e83a00f313de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from ipywidgets import widgets\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(\"../include\"))\n",
    "\n",
    "import py_helper\n",
    "\n",
    "float_type = np.float32\n",
    "\n",
    "defines=py_helper.load_defines(\"mat_size.hpp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1637b9-c045-4b17-8492-a8a7c1f2f606",
   "metadata": {},
   "source": [
    "### Sequential (synchronous) IO solution\n",
    "\n",
    "In [wave2d_sync.cpp](wave2d_sync.cpp) we use an array of three HIP device allocations to represent the wavefield at timesteps (0,1,2). The null stream (stream 0) is used for both kernel execution and IO.\n",
    "\n",
    "<figure style=\"margin-bottom 3em; margin-top: 2em; margin-left:auto; margin-right:auto; width:100%\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/sequential_io.svg\"> <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Figure: Sequential IO solution.</figcaption>\n",
    "</figure>\n",
    "\n",
    "#### Make and run the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6e3ecc4-10ea-4f0c-8b9b-b62b61018e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Nothing to be done for 'all'.\n"
     ]
    }
   ],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bac6b827-4ee7-4f24-9cbe-bc856bc18691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device id: 0\n",
      "\tname:                                    \n",
      "\tglobal memory size:                      536 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum pitch size for memory copies:    402 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,2147483647,2147483647)\n",
      "dt=0.001166, Vmax=343.000000\n",
      "dt=0.00116618, fm=34.3, Vmax=343, dt2=1.35998e-06\n",
      "The synchronous calculation took 48.430000 milliseconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/wave2d_sync.exe'], returncode=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run([os.path.join(os.getcwd(),\"wave2d_sync.exe\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa42486-4d6f-4432-9c22-173f9f6c290d",
   "metadata": {},
   "source": [
    "#### Plot the output wavefield\n",
    "\n",
    "At the end of iteration a binary file containing the wavefield at every timestep is written to the file **array_out.dat**. We can read in this wavefield and plot it below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e385ad3-3850-4231-b005-35909c5d1374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f60bcd164034b20894674f9ea319666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='n', max=639), Output()), _dom_classes=('widget-interact'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read the output file back in for display\n",
    "output_sync=np.fromfile(\"array_out.dat\", dtype=float_type)\n",
    "nimages_sync=int(output_sync.size//(defines[\"N0_U\"]*defines[\"N1_U\"]))\n",
    "images_sync=output_sync.reshape(nimages_sync, defines[\"N0_U\"], defines[\"N1_U\"])\n",
    "\n",
    "py_helper.plot_slices(images_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d690e0e-e7ab-4e33-a2aa-2988dfd62958",
   "metadata": {},
   "source": [
    "#### Application trace\n",
    "\n",
    "The script **make_traces.sh** produces traces in the **tau** folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c7d86a4-f911-4837-9917-2e4fcab10124",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RPL: on '230726_151432' from '/opt/rocm-5.6.0' in '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation'\n",
      "RPL: profiling '\"./wave2d_sync.exe\"'\n",
      "RPL: input file ''\n",
      "RPL: output dir '/tmp/rpl_data_230726_151432_34580'\n",
      "RPL: result dir '/tmp/rpl_data_230726_151432_34580/input_results_230726_151432'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_230726_151432_34580/input.xml\"\n",
      "  0 metrics\n",
      "ROCtracer (34603):\n",
      "    HSA-trace(*)\n",
      "    HSA-activity-trace()\n",
      "    HIP-trace(*)\n",
      "Device id: 0\n",
      "\tname:                                    \n",
      "\tglobal memory size:                      536 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum pitch size for memory copies:    402 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,2147483647,2147483647)\n",
      "dt=0.001166, Vmax=343.000000\n",
      "dt=0.00116618, fm=34.3, Vmax=343, dt2=1.35998e-06\n",
      "The synchronous calculation took 51.917000 milliseconds.\n",
      "\n",
      "ROCPRofiler: 643 contexts collected, output directory /tmp/rpl_data_230726_151432_34580/input_results_230726_151432\n",
      "hsa_copy_deps: 1\n",
      "scan hsa API data 18941:18942                                                                                                    hsa_copy_deps: 0\n",
      "scan hip API data 3222:3223                                                                                                    File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_sync.csv' is generating\n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_sync.stats.csv' is generating\n",
      "dump json 642:643                                                                                                    \n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_sync.json' is generating\n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_sync.hsa_stats.csv' is generating\n",
      "dump json 19584:19585                                                                                                    \n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_sync.json' is generating\n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_sync.copy_stats.csv' is generating\n",
      "dump json 637:638                                                                                                    \n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_sync.json' is generating\n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_sync.hip_stats.csv' is generating\n",
      "dump json 3222:3223                                                                                                    \n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_sync.json' is generating\n",
      "RPL: on '230726_151434' from '/opt/rocm-5.6.0' in '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation'\n",
      "RPL: profiling '\"./wave2d_async.exe\"'\n",
      "RPL: input file ''\n",
      "RPL: output dir '/tmp/rpl_data_230726_151434_34659'\n",
      "RPL: result dir '/tmp/rpl_data_230726_151434_34659/input_results_230726_151434'\n",
      "ROCProfiler: input from \"/tmp/rpl_data_230726_151434_34659/input.xml\"\n",
      "  0 metrics\n",
      "ROCtracer (34682):\n",
      "    HSA-trace(*)\n",
      "    HSA-activity-trace()\n",
      "    HIP-trace(*)\n",
      "Device id: 0\n",
      "\tname:                                    \n",
      "\tglobal memory size:                      536 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum pitch size for memory copies:    402 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,2147483647,2147483647)\n",
      "dt=0.001166, Vmax=343.000000\n",
      "dt=0.00116618, fm=34.3, Vmax=343, dt2=1.35998e-06\n",
      "The asynchronous calculation took 32.023000 milliseconds.\n",
      "\n",
      "ROCPRofiler: 1282 contexts collected, output directory /tmp/rpl_data_230726_151434_34659/input_results_230726_151434\n",
      "hsa_copy_deps: 1\n",
      "scan hsa API data 29487:29488                                                                                                    hsa_copy_deps: 0\n",
      "scan hip API data 7722:7723                                                                                                    File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_async.csv' is generating\n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_async.stats.csv' is generating\n",
      "dump json 1281:1282                                                                                                    \n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_async.json' is generating\n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_async.hsa_stats.csv' is generating\n",
      "dump json 30769:30770                                                                                                    \n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_async.json' is generating\n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_async.copy_stats.csv' is generating\n",
      "dump json 0:1                                                                                                    \n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_async.json' is generating\n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_async.hip_stats.csv' is generating\n",
      "dump json 7722:7723                                                                                                    \n",
      "File '/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/rocprof_trace/wave2d_async.json' is generating\n"
     ]
    }
   ],
   "source": [
    "!./make_traces.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3fa9b9-e925-4899-9ef5-9490a04d5e14",
   "metadata": {},
   "source": [
    "Then you can go to the address [https://ui.perfetto.dev](https://ui.perfetto.dev) to open the tracing utility. If you load the file **rocprof_trace/trace_sync.json** you should see something like this. The IO occurs after each kernel execution using the same command queue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589fdcb5-31dd-4a66-b1a4-bef7e039546d",
   "metadata": {},
   "source": [
    "<figure style=\"margin-bottom 3em; margin-top: 2em; margin-left:auto; margin-right:auto; width:100%\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/synchronous_io.png\"> <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Figure: Sequential IO solution.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c9d703-f3e4-4f1f-8434-3b24c32188ca",
   "metadata": {},
   "source": [
    "### Concurrent (asynchronous) IO solutions\n",
    "\n",
    "In [wave2d_async_streams.cpp](wave2d_async_streams.cpp) and [wave2d_async_events.cpp](wave2d_async_events.cpp) are solutions for concurrent IO. The goal is to use **multiple streams** so that while one stream is executing a kernel the others are working on IO.\n",
    "\n",
    "<figure style=\"margin-bottom 3em; margin-top: 2em; margin-left:auto; margin-right:auto; width:100%\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/concurrent_io.svg\"> <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Figure: Concurrent IO solution.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Both solutions use streams, however the difference between the two solutions is that [wave2d_async_streams.cpp](wave2d_async_streams.cpp) uses explicit synchronisation on streams to establish the necessary dependencies between IO and compute, while [wave2d_async_events.cpp](wave2d_async_events.cpp) is focused on using HIP events to achieve the same synchronisation. Both accomplish the same task of moving data while compute is taking place.\n",
    "\n",
    "#### Concurrent access to buffers from the host and device\n",
    "\n",
    "It is a **race condition** to read from a device allocation (from another stream) at the same time as a kernel is writing to the allocation. Furthermore, while it is technically possible to asynchronously read from an array that is also being read by a kernel, it can lead to undefined behaviour as memory can be moved around. Therefore, it is **recommended** to perform IO only on buffers that we **know for sure** are not being used by a kernel. Our kernel needs access to wavefields at timesteps $\\textbf{U}_{0}, \\textbf{U}_{1}, \\textbf{U}_{2}$, therefore they are **not** safe to copy from, but wavefields at earlier timesteps e.g $\\textbf{U}_{-2}, \\textbf{U}_{-1}$ **are** safe to copy.\n",
    "\n",
    "<figure style=\"margin-bottom 3em; margin-top: 2em; margin-left:auto; margin-right:auto; width:30%\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/wavefields.svg\"> <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Figure: Wavefields that are ok to copy.</figcaption>\n",
    "</figure>\n",
    "\n",
    "In this instance, a solution to enable concurrent IO is to have an array of at least four memory allocations on the device to represent the wavefield. We choose an array of **nscratch=5** allocations to allow extra leeway for copies to finish. Associated with the buffers is an array of five streams for IO and one stream for compute. We also create an array of five events to demonstrate the use of events in workflow dependencies.\n",
    "\n",
    "#### Stream-based synchronisation\n",
    "\n",
    "The command **hipStreamSynchronize** \n",
    "\n",
    "# Got to here\n",
    "\n",
    "\n",
    "#### Event-based synchronisation\n",
    "\n",
    " During each iteration **n** of the time loop then:\n",
    "\n",
    "1. We use **hipStreamWaitEvent** to make the compute stream wait for past events arising from IO streams at **(n+2)** and **(n+1)**. \n",
    "1. For teaching purposes use **hipEventSynchronize** to make the host wait on the IO event at **n**.\n",
    "1. Then use **hipStreamSynchronize** on the compute stream to make sure there is no backlog of work.\n",
    "1. Submit the kernel to compute stream to solve for U[(n+2)%nscratch], then record an event (at index **n**) into the compute stream.\n",
    "1. The wavefield at **(n-1)** (which we call the *copy_index*) is safe to copy once the compute stream is done with it. Use **hipStreamWaitEvent** to make the IO stream at **(n-1)** wait on event at **(n-1)**. Then use the IO stream and **hipMemcpy3DAsync** to asynchronously copy the wavefield at **(n-1)** to the stack of output images. \n",
    "1. Use **hipStreamWaitEvent** so that the stream used for the copy waits for the kernel to finish. Record an event to the IO stream after the copy. These events will be then awaited on in the next iteration (step 1).\n",
    "\n",
    "The following diagram shows how the dependencies play out with events and streams during a single iteration.\n",
    "\n",
    "<figure style=\"margin-bottom 3em; margin-top: 2em; margin-left:auto; margin-right:auto; width:100%\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/wavefields_concurrent.svg\"> <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Figure: Compute and IO queues during an iteration.</figcaption>\n",
    "</figure>\n",
    "\n",
    "The code for the iterations in [wave2d_async.cpp](wave2d_async.cpp) is produced here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ce10d2-0688-4a30-892b-afff024c8eed",
   "metadata": {},
   "source": [
    "```C++\n",
    "    for (int n=0; n<NT; n++) {\n",
    "        \n",
    "        // Wait for the event associated with a stream\n",
    "        \n",
    "        // Can make a stream wait on an event\n",
    "        // Make the compute stream wait on previous copies\n",
    "        H_ERRCHK(hipStreamWaitEvent(compute_stream, events[(n+2)%nscratch], 0));\n",
    "        H_ERRCHK(hipStreamWaitEvent(compute_stream, events[(n+1)%nscratch], 0));\n",
    "        \n",
    "        // Or wait on an event directly\n",
    "        H_ERRCHK(hipEventSynchronize(events[n%nscratch]));\n",
    "        \n",
    "        // Can wait on a stream directly also\n",
    "        H_ERRCHK(hipStreamSynchronize(compute_stream));\n",
    "        \n",
    "        // Get the wavefields\n",
    "        U0_d = U_ds[n%nscratch];\n",
    "        U1_d = U_ds[(n+1)%nscratch];\n",
    "        U2_d = U_ds[(n+2)%nscratch];\n",
    "        \n",
    "        // Shifted time\n",
    "        t = n*dt-2.0*td;\n",
    "        pi2fm2t2 = pi*pi*fm*fm*t*t;\n",
    "        \n",
    "        // Launch the kernel using hipLaunchKernelGGL method\n",
    "        // Use 0 when choosing the default (null) stream\n",
    "        hipLaunchKernelGGL(wave2d_4o, \n",
    "            grid_nblocks, block_size, sharedMemBytes, compute_stream,\n",
    "            U0_d, U1_d, U2_d, V_d,\n",
    "            N0, N1, dt2,\n",
    "            inv_dx02, inv_dx12,\n",
    "            P0, P1, pi2fm2t2\n",
    "        );\n",
    "                           \n",
    "        // Check the status of the kernel launch\n",
    "        H_ERRCHK(hipGetLastError());\n",
    "          \n",
    "        // Insert an event into stream at n%nscratch\n",
    "        // It will complete afer the kernel does\n",
    "        H_ERRCHK(hipEventRecord(events[n%nscratch], compute_stream));   \n",
    "        \n",
    "        // Read memory from the buffer to the host in an asynchronous manner\n",
    "        if (n>2) {\n",
    "            size_t copy_index=n-1;\n",
    "            \n",
    "            // Insert a wait for the copy stream on the compute event\n",
    "            H_ERRCHK(\n",
    "                hipStreamWaitEvent(\n",
    "                    streams[copy_index%nscratch], \n",
    "                    events[copy_index%nscratch],\n",
    "                    0\n",
    "                )\n",
    "            );\n",
    "            \n",
    "            // Then asynchronously copy a wavefield back\n",
    "            // Using the copy stream\n",
    "            \n",
    "            // Only change what is necessary in copy_parms\n",
    "            copy_parms.srcPtr.ptr = U_ds[copy_index%nscratch];\n",
    "            \n",
    "            // Z positions of 1 don't seem to work on AMD platforms?!?!\n",
    "            copy_parms.dstPos.z = copy_index;\n",
    "            \n",
    "            // Copy memory asynchronously\n",
    "            H_ERRCHK(\n",
    "                hipMemcpy3DAsync(\n",
    "                    &copy_parms,\n",
    "                    streams[copy_index%nscratch]\n",
    "                )\n",
    "            );\n",
    "            // Record the event to a stream\n",
    "            H_ERRCHK(\n",
    "                hipEventRecord(\n",
    "                    events[copy_index%nscratch],\n",
    "                    streams[copy_index%nscratch]\n",
    "                )\n",
    "            );\n",
    "            \n",
    "            // Copy memory synchronously\n",
    "            //H_ERRCHK(\n",
    "            //    hipMemcpy3D(\n",
    "            //        &copy_parms\n",
    "            //        //streams[copy_index%nscratch]\n",
    "            //    )\n",
    "            //); \n",
    "        }\n",
    "    }\n",
    "```\n",
    "\n",
    "> With this example I used the function **hipMemcpy3DAsync** to copy wavefields as planes back to the host memory allocation. For some bizarre reason, during construction of this example I found that on AMD platforms a value of **copy_parms.dstPos.z=1** resulted in an error for calls to either **hipMemcpy3D** or **hipMemcpy3DAsync**. This strange behaviour was not present with the NVIDIA backend. I have worked around this issue by only copying planes when **n>2**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95733f27-00e3-492b-bea2-8bcbc0fe7881",
   "metadata": {},
   "source": [
    "#### Make and run the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e741744b-3f79-4d55-8046-b829ce17d635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hipcc -g -fopenmp -O2 -I../include wave2d_async.cpp -o wave2d_async.exe \n"
     ]
    }
   ],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "582cea5a-05db-4afe-92ce-075a18bb95c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device id: 0\n",
      "\tname:                                    \n",
      "\tglobal memory size:                      536 MB\n",
      "\tavailable registers per block:           65536 \n",
      "\tmaximum shared memory size per block:    65 KB\n",
      "\tmaximum pitch size for memory copies:    402 MB\n",
      "\tmax block size:                          (1024,1024,1024)\n",
      "\tmax threads in a block:                  1024\n",
      "\tmax Grid size:                           (2147483647,2147483647,2147483647)\n",
      "dt=0.001166, Vmax=343.000000\n",
      "dt=0.00116618, fm=34.3, Vmax=343, dt2=1.35998e-06\n",
      "The asynchronous calculation took 23.229000 milliseconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['/home/toby/Pelagos/Projects/HIP_Course/course_material/L8_IO_Optimisation/wave2d_async_streams.exe'], returncode=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the application\n",
    "subprocess.run([os.path.join(os.getcwd(),\"wave2d_async_streams.exe\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae90139d-e944-414c-a3ab-f95c10dcf5cd",
   "metadata": {},
   "source": [
    "If we check the time elapsed we find that the concurrent IO solution took less time than the sequential IO solution. A trace of the HIP activity shows that IO is taking place during compute.\n",
    "\n",
    "<figure style=\"margin-bottom 3em; margin-top: 2em; margin-left:auto; margin-right:auto; width:100%\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/asynchronous_io.png\"> <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Figure: Concurrent IO solution.</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e868c3-7c11-45bc-8b50-b8d5f8c803b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plot the wavefield and explore results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "459cbfd2-0fdc-4492-9805-f76db5e9a6c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee0af01560084d69a0d53dd256502c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='n', max=639), Output()), _dom_classes=('widget-interact'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum residual between results is 0.0\n"
     ]
    }
   ],
   "source": [
    "# Read the output file back in for display\n",
    "output_async=np.fromfile(\"array_out.dat\", dtype=float_type)\n",
    "nimages_async=int(output_async.size//(defines[\"N0_U\"]*defines[\"N1_U\"]))\n",
    "images_async=output_async.reshape(nimages_async, defines[\"N0_U\"], defines[\"N1_U\"])\n",
    "\n",
    "py_helper.plot_slices(images_async)\n",
    "\n",
    "print(f\"Maximum residual between results is {np.max(images_async-images_sync)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8f9bf8-b44f-4527-8ac3-3ea81fe9a8f3",
   "metadata": {},
   "source": [
    "## Summary of learnings\n",
    "\n",
    "In this module we explored how IO can take place at the same time as a kernel using multiple streams. The concurrent IO solution was faster than the sequential IO solution. With concurrent IO it is a safety measure to avoid accessing memory allocations that are being used by a kernel, unless the allocated is managed. Both stream and event-based synchronisation can establish and enforce dependencies between activity that occurs across multiple streams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb824e5-856c-4b36-ac49-c3801294bc41",
   "metadata": {},
   "source": [
    "<address>\n",
    "Written by Dr. Toby Potter of <a href=\"https://www.pelagos-consulting.com\">Pelagos Consulting and Education</a> for the Pawsey Supercomputing Centre\n",
    "</address>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
